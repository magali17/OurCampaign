---
title: "Instrument Quality Control (QC)"
author: "Magali Blanco"
date: ' `r Sys.time()` '
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    number_sections: true
    toc_float: true
    collapsed: false
    smooth_scroll: false
editor_options: 
  chunk_output_type: console
---
# Overview 

**Purpose**: This script documents the quality control procedures taken to ensure data quality after the ACT TRAP mobile monitoring campaign. 

###--> Approach


### --> Results Summary


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      #fig.height = 5, fig.width = 8
                      fig.height = 6, fig.width = 10
                      )  

# # Clear workspace of all objects and unload all extra (non-base) packages
# rm(list = ls(all = TRUE))
# if (!is.null(sessionInfo()$otherPkgs)) {
#   res <- suppressWarnings(
#     lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
#       detach, character.only=TRUE, unload=TRUE, force=TRUE))
# }

pacman::p_load(knitr, kableExtra, 
               ggpubr, tidyverse,
               ggrepel, #geom_label_repel
               #mapping...adding scales, N arrows
               ggmap, sf, ggspatial, 
               units, #convert between e.g., m to km
               #time series data 
               lubridate,
               # modeling
               broom ##tidy()
               )    
 

set.seed(1)

```


# Upload data

common variables

```{r}
mytz <- "America/Los_Angeles"

start_date <- "2019-02-22"
end_date <- "2020-03-17" #or: "2020-03-18" to capture all of 3/17 ?

pt_pollutants <- c("PM", "BC")

#air pollutants of immediate interest
ap <- c(
  #CO2
  "co2_umol_mol",
  #aethalometer
  "ma200_ir_bc1",
  #neph
  "neph_bscat", #"neph_ccoef",
  #no2
  "no2",
  #nanoscan
  "ns_total_conc",
  # discmini
  'pmdisc_number',  
  #ptraks
  "pnc_noscreen", "pnc_screen"
  )

unique_routes <- paste0("R0", c(1:9))

#drop instruments w/ few readings (only want a primary and backup instrument if possbile)
drop_instruments <-  c(paste0("PMPT_", c(1,2,4)),
                               paste0("PMPTSCREEN_", c(1,4))
)
```

### --> add lab duplicate collocation data    

## helper tables

```{r}
#imports, fieldnotes, and location tables
load(file.path("Data", "Original", "imports_fieldnotes_location_tables.rda"))

calibrations <- readRDS(file.path("Data", "Original", "lab_calibrations_in_fixed_rooms.rda")) %>%
  mutate(date = ymd(substr(time, 1, 10))) %>%
  filter(
    #not sure why tim measured these  
    !analyte %in% c("no, nox"),
    # only keep instrument readings we're interested in
    variable %in% ap,
    #drop instruments w/ few readings (only want a primary and backup instrument)
    !instrument_id %in% drop_instruments
  ) %>%
  
  #drop analyte since this is sometimes wrong (Amanda); other variables make duplicate rows - due to merging issue earlier?
  select(-c(analyte, id, import_id, level)) %>%
  distinct() %>%
  drop_na(instrument_id, variable, value)

```

## stop data

```{r}
#all stop data
dt0 <- readRDS(file.path("Data", "Original", "stop_data_2019-02-22_2020-03-17.rda")) %>%
  mutate(date = ymd(substr(time, 1, 10))) %>%
  filter(
    # only keep instrument readings we're interested in
    variable %in% ap,
    #drop instruments w/ few readings (only want a primary and backup instrument)
    !instrument_id %in% drop_instruments,
    #drop Roosevelt Garage
    #location != "MS0000"
  ) %>%
  #drop empty data
  drop_na(value) %>%
  # drop ~ 700 duplicate rows - why are these here?? MS000?
  distinct()
  

#ID primary instruments
primary_instruments <- dt0 %>%
  group_by(variable, instrument_id) %>%
  summarize(
    n = n()
  ) %>% 
  group_by(variable) %>%
  filter(n == max(n)) %>%
  ungroup() %>%
  pull(instrument_id)
 
dt0 <- dt0 %>%
  mutate(primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup" ),
         primary_instrument = factor(primary_instrument, levels = c("Primary", "Backup"))
         )


# DUPLICATE ROWS??
# There ~ 15k rows w/ duplicate information other than the value column. Looks like co_14 (the primary CO2 instrument) was primarily responsible for this - it reported duplicate readings at any given time. Duplicate readings appear to be very similar. Was this related to an instrument logging issues?? We'll average over these.

#dt0 %>% select_at(vars(-value)) %>% distinct() %>% nrow() #6283650
  
# dt0 %>% 
#   select(location, time, instrument_id, value) %>% 
#   
#   slice(2974420, 2974421, 4056517, 4056518, 4056523, 4056524, 4056529, 4056530, 4056535, 4056536, 4056541, 4056542, 4056549, 4056550 , 4056555, 4056556, 4056561, 4056562, 4056567, 4056568, 4056573, 4056574, 4056579, 4056580, 4056585, 4056586, 4056591, 4056592, 4056597, 4056598, 4056603, 4056604, 4056611, 4056612, 4056617, 4056618, 4056623, 4056624, 4056629, 4056630, 4056635, 4056636
# , 4056641, 4056642
# , 4056647, 4056648
# , 4056653, 4056654
# , 4056659, 4056660
# , 4056665, 4056666
# , 4056673, 4056674
# , 4056679, 4056680
# , 4056685, 4056686
# , 4056691, 4056692
# , 4056697, 4056698
# , 4056703, 4056704
# , 4056709, 4056710
# , 4056715, 4056716
# , 4056721, 4056722
# , 4056727, 4056728
# , 4056735, 4056736
# , 4056741, 4056742
# , 4056747, 4056748
# , 4056753, 4056754
# , 4056759, 4056760
# , 4056765, 4056766
# , 4056771, 4056772
# , 4056777, 4056778
# , 4056783, 4056784
# , 4056789, 4056790
# , 4056797, #4056
#         ) %>% View()
  
   
dt0 <- dt0 %>% 
  #take avg of duplicate CO2_14 rows w/ slightly different values
  group_by_at(vars(-value)) %>% 
  summarize(value = mean(value)) %>% 
  ungroup()

dt0_w <- dt0 %>%
  select(-c(variable, primary_instrument)) %>%
  spread(instrument_id, value) %>%
  arrange(time)
  
```

```{r}
#types of instruments
instruments <- unique(dt0$instrument_id) %>%
  str_extract(., "[^_]+") %>%
  unique() %>%
  sort()

```

 
```{r, eval=F}
# don't add stop_route_no since results in sites w/ missing visited number?

#sequence of stop order based on a run. Using runs after the campaign had run for a while, since some changes happened early on. The resulting numbering should be mostly accurate for the rest of the campaign.

first_run <- dt0 %>%
  #don't consider initial runs
  filter(date > ymd("2019-04-01"),
         location != "MS0000"
         ) %>%  
  
  group_by(site) %>%
  filter(time == min(time)) %>% 
  ungroup() %>%
  distinct(runname) %>% pull()
  
run_stop_order <- dt0 %>%
  #only keep first run of each route
  filter(runname %in% first_run,
         location != "MS0000"
         ) %>%
  
  group_by(runname, location) %>%
  mutate(arrival_time = min(time)) %>% 
  ungroup() %>%
  distinct(runname, location, arrival_time) %>% 
  group_by(runname) %>%
  #number stop order within each day
  mutate(run_stop_no = row_number(),
         run_stop_no = str_pad(run_stop_no, width = 2, side = "left", pad = "0")
         ) %>%
  ungroup() %>%
  select(location, run_stop_no)

#unique(dt0$run_stop_no)

#dim(left_join(dt0, run_stop_order))

# add stop order within a route
dt0 <- left_join(dt0, run_stop_order)

# # how many sites have NAs (location not visited within that runname)
# dt0 %>%
#   filter(is.na(run_stop_no)) %>%
#   distinct(location) %>%
#   nrow()

```

## DOE 

```{r, eval=F}
# #tell R time is actually 8 hrs behind GMT (always in PST), then convert to include PDT. Otherwise, R thinks the times are all in PDT & PST. DO THIS FIRST - before calculating other temporal variables.

daylight_start <- ymd("2019-03-10")
daylight_end <- ymd("2019-11-03")

#LA time
summer <- ymd_hm("2019-03-11 15:00", tz = mytz)
fall <-ymd_hm("2019-11-04 15:00", tz = mytz)

# reset time to actually be in PST, then adjust to LA time
with_tz(ymd_hms(summer, tz = "Etc/GMT+8"),
        tzone = mytz
        )

with_tz(ymd_hms(fall, tz = "Etc/GMT+8"),
        tzone = mytz
        )

```

* **note** DOE data is fixed so that it is read as PST (correct) and then converted to LA time (includes daylight savings) like the rest of our mobile monitoring data. If Dave S fixes this in the dataset, this initial setting should be removed/adjusted.

```{r}
doe0 <- readRDS(file.path("Data", "Original", "doe_dt.rda"))

drop_doe_vars <- c(
  #don't need/know what theese are 
  "doe_uv_633", "doe_ref_mc", "doe_base_mc", "doe_trace_co",
  
  # # have no2 at the same sites 
  "doe_trace_noy", "doe_trace_no",     "doe_trace_noy-no", "doe_nox", "doe_no"
  )

doe <- doe0 %>%
  rename(location_doe=site) %>%
  
  filter(
    #don't need Tacoma site - no collocation here
    location_doe != "AQSTAC",
    #don't need these variables
    !variable %in% drop_doe_vars,
         ) %>%
  mutate(
    ## #tell R time is actually 8 hrs behind GMT (always in PST, no dayllight savings), then convert to America/Los_Angeles to include PDT like the rest of our data. Otherwise, R thinks the times are all in UTC OR PDT & PST. DO THIS FIRST - before calculating other temporal variables.
    time = with_tz(ymd_hms(time, tz = "Etc/GMT+8"), tzone = mytz),
    
    date = ymd(substr(time, 1, 10)),
    #crosswalk for our location IDs
    location = recode_factor(factor(location_doe),
                             #10th & Weller
                             "AQS10W" = "MC0120",
                             #Beacon Hill
                             "AQSBH" = "MC0003",
                             #Kent-Central & James
                             "AQSK" = "MC0406",
                             #Duwamish
                             "AQSD" = "MC0126",
                             #Tukwila Allentown
                             "AQSTUK" = "MC0002"
                             ),
    #sampling frequency
    freq = recode_factor(factor(freq),
                          "DOE_H" = "hour",
                          "DOE_M" = "minute",
                          "DOE_D" = "day",
                          ),
    freq = factor(freq, levels = c("minute", "hour", "day")),
    
    pollutant = ifelse(grepl("_no", variable), "NO2", 
                            ifelse(grepl("pm25", variable), "PM2.5",
                                   ifelse(grepl("neph", variable), "Neph",
                                          ifelse(grepl("_bc_", variable), "BC", NA)
                                          )
                            )
                            ),
    pollutant = factor(pollutant, levels = c("BC", "NO2", "Neph", "PM2.5"))
    
  )  %>%
  
  filter(
    #only keep data for study period
    date >= ymd(start_date) & date <= ymd(end_date)
  )

```

* note, hourly and daily readings should be "validated" by the DOE since the final files were requested > 3 months after the data were collected (how long it takes DOE to validate data.)

```{r}

doe %>%
  # make code run faster
  group_by(location_doe, freq, variable, pollutant) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  
  #filter(!variable %in% drop_doe_vars) %>%
  mutate(location_doe = substr(location_doe, 4, nchar(location_doe))) %>%
  
  ggplot(aes(y=variable, x=location_doe)) + 
  facet_grid(pollutant~freq, scales="free", space = "free_y") + 
  geom_bin2d(aes(fill=count)) + 
  
  labs(title =  "Available regulatory data")

```

* check that minute and hour data are similar since only hourly data are validated by DOE

```{r}
#compare minute and hour data
df <- doe %>%
  select(-import_id) %>%
  filter(freq %in% c("minute", "hour")) %>%
  #remove minute time stamp from minute data
  mutate(time = ymd_h(format(time, format = "%Y-%m-%d %H")))  
  
df1 <- df %>%
  filter(freq=="hour") %>%
  rename(hour_val = value) 

df2 <- df %>%
  filter(freq=="minute") %>%
  select(location_doe, time, variable, value) #%>% View()

df <- left_join(df1, df2) %>%
  select(-freq) %>%
  drop_na()
  
```

```{r}
df %>%
  mutate(diff = value - hour_val) %>%
  group_by(pollutant, variable) %>%
  summarize(
    min = min(diff),
    median = median(diff),
    max = max(diff)
  ) %>%
  kable(caption = "differnece between minute and hourly readings (minute-hour value)") %>%
  kable_styling()
  
  
```

* there is a lot more variability in the minute data compared to the hourly data. This could be real, or it could be noise.    
* [not shown in plot] smooth line is directly over the 1-1 line, indicating that minute data are in agreement with hourly data    

```{r}
## #takes forever to run
# df %>%
#   #takes forever to run
#   filter(pollutant %in% c("BC", "NO2", "Neph")) %>%
#   
#   ggplot(aes(x=hour_val, y=value, col=location_doe)) + 
#   facet_wrap(~pollutant, scales="free") + 
#   geom_point(alpha=0.05) +
#   geom_smooth() +
#   geom_abline(slope = 1, intercept = 0) + 
#   #coord_fixed() +
#   
#   labs(x="Validated hourly value",
#        y = "Unvalidated minute value"
#        )

df %>%
  #pollutants with true minute values
  filter(pollutant %in% c("BC", "NO2", "Neph")) %>%
  
  ggplot(aes(x=hour_val, y=value)) + 
  facet_grid(location_doe~pollutant, scales="free") + 
  geom_bin2d() +
  geom_abline(slope = 1, intercept = 0, linetype="dashed", aes(col="1-1")) + 
  #geom_smooth(aes(col="loess")) +

  labs(x="Validated hourly value",
       y = "Unvalidated minute value"
       )

```


observations    
* 10W and BH had PM2.5 from different sources over the course of the year 

```{r}
#check that there is sufficient data in the minute data 
doe %>%
  filter(freq == "minute") %>%
  distinct(location_doe, variable, pollutant, date) %>%
  
  filter(!variable %in% drop_doe_vars) %>%
  
  ggplot(aes(x=date, y=variable)) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) + 
  
  labs(title = "Available minute data during the study period")

  
```

* note, most are FEM methods (not nephs; also, not all bam methods are FEM - e.g., doe_bam_pm25?)    
* there is some overlap in instrument readings. This is probably done on purpose by the PSCAA in order to calibrate readings from different instruments. We will drop instrument reading as soon as another FEM method starts sampling.    
* BC readings at 10W are missing for ~4 months from around May-Aug 2019



```{r, eval=F}
# don't need to show this. next fig is similar but updated 

doe %>%
  filter(freq == "minute") %>%
  distinct(location_doe, variable, pollutant, date) %>%
   
  filter(
    #these sites already have full neph data 
         !(location_doe %in% c('AQSD', 'AQSK', 'AQSTUK') & variable %in% c('doe_bam_pm25', 'doe_pm25_fem', 'doe_pm25_fem_bam', 'doe_pm25_fem_teo') )
         
    
  ) %>%
  
  ggplot(aes(x=date, y=variable)) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) + 
  
  labs(title = "Minute data to be used to calculate anual averages")

```

```{r, eval=F}
# when does sampling start/stop or sites with multiple instruments measuring PM2.5?
doe %>%
  filter(freq == "minute",
         location_doe %in% c('AQS10W', 'AQSBH'),
         pollutant == "PM2.5"
         ) %>%
  group_by(location_doe, variable) %>%
  summarize(
    start_date = min(date),
    end_date = max(date)
  ) %>%
  arrange(location_doe, start_date)
 
```

```{r}
doe2 <- doe %>%
  filter(
    # hour/day estimates may be largely aggregates of minute data 
    freq == "minute",
    
    #these sites already have full neph data
    !(location_doe %in% c('AQSD', 'AQSK', 'AQSTUK') & variable %in% c('doe_bam_pm25', 'doe_pm25_fem', 'doe_pm25_fem_bam', 'doe_pm25_fem_teo') ),
    
    # a diff FEM method starts sampling at both 10W and BH at this time
    !(variable == "doe_pm25_fem" & date > ymd("2019-04-01") ),
    # a diff ?FEM method starts sampling at BH 
    !(variable == "doe_pm25_fem_teo" & date > ymd("2019-11-25") & location_doe == 'AQSBH'),
    
  ) 

```

* dropped overlapping data at 10W and BH: 

```{r}
#check that things look right # looks right

## plot
doe2 %>%
  distinct(location_doe, variable, pollutant, date) %>%
  
  ggplot(aes(x=date, y=variable
             #y=pollutant
             )) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) +
  
  labs(title = "Data used to estimate annual averages at AQS sites")


# # no need to show this
# ## table
# doe2 %>%
#   filter(freq == "minute",
#          location_doe %in% c('AQS10W', 'AQSBH'),
#          pollutant == "PM2.5"
#          ) %>%
#   group_by(location_doe, variable) %>%
#   summarize(
#     start_date = min(date),
#     end_date = max(date)
#   ) %>%
#   arrange(location_doe, start_date) %>%
#   kable(caption = "PM2.5 readings used to calculate an annual average at 10W and BH since multiple instruments were used over the course of the ACT TRAP study period") %>%
#   kable_styling()

```

```{r}
doe2 <- doe2 %>%
  #rename each pollutant regardless of exact measurement instrument used
  group_by(location_doe, location, date, time, pollutant) %>%
  summarize(value = mean(value, na.rm = T)) %>% 
  ungroup()

```

observations per instrument 

* looks good. We have ~1 reading/minute at each site

```{r}
doe2 %>%
  group_by(pollutant, location_doe) %>%
  summarize(
    total_samples = n(),
    samples_per_hour = total_samples/as.numeric(difftime(max(date), min(date), units="hour"))
  ) %>%
  kable(caption = "Number of samples per site and pollutant used to estimate annual averages", digits = 0) %>%
  kable_styling()
  
```


```{r}
# #see unique sampling locations. can use these to download data faster from the results table
# imports %>% mutate(loc = substr(runname, 12, nchar(runname))) %>% distinct(loc)

#imports %>% mutate(loc = substr(runname, 12, 14)) %>% distinct(loc)

```

```{r}


```


# Functions

* colocation plot fn

```{r, colo.plot fn}

# fn returns colocation plot w/ best fit info, RMSE and MSE-based R2

colo.plot <- function(data.wide=mm.wide, 
                      x.variable, x.label = "",
                      y.variable, y.label = "",
                      col.by = "",
                      shape.var="",
                      alpha_value = 0.3,
                      mytitle = "", title_width = 60,
                      mysubtitle = NULL,
                      mycaption = NULL,
                      int_digits = 0,
                      slope_digits = 2,
                      r2.digits = 2, 
                      rmse.digits = 0,
                      convert_rmse_r2_to_native_scale=F
                      ) {
  
  #if label is left blank, use variable name
  if(x.label == ""){x.label <- x.variable}
  if(y.label == ""){y.label <- y.variable}
  
  data.wide <- data.wide %>% drop_na(x.variable, y.variable)  
  
  lm1 <- lm(formula(paste(y.variable, "~", x.variable)), data = data.wide)
  #summary(lm1)
  
  
  ################################################
  ## ?? need this fns inside this fn???
  #returns MSE
  mse <- function(obs, pred){
    mean((obs - pred)^2)
  }
  
  rmse <- function(obs, pred){
    sqrt(mean((obs - pred)^2))
  }
  
  #returns MSE-based R2
  r2_mse_based <- function(obs, pred) {
    mse.est <- mse(obs, pred)
    r2 <- 1- mse.est/mean((obs - mean(obs))^2)
    max(0, r2)
  }  
  
  ################################################ 
  
  
  #rmse
  if (convert_rmse_r2_to_native_scale==T) {
    rmse <- rmse(obs = exp(data.wide[[x.variable]]), pred = exp(data.wide[[y.variable]])) %>% 
      round(digits = rmse.digits)
    
    r2 <- r2_mse_based(obs = exp(data.wide[[x.variable]]), pred = exp(data.wide[[y.variable]])) %>%
      round(r2.digits)
    } 
  
  else {
    rmse <- rmse(obs = data.wide[[x.variable]], pred = data.wide[[y.variable]]) %>% 
    round(digits = rmse.digits)
    
    r2 <- r2_mse_based(obs = data.wide[[x.variable]], pred = data.wide[[y.variable]]) %>%
      round(r2.digits)
  }
  
  
  fit.info <- paste0("y = ", round(coef(lm1)[1], int_digits), " + ", round(coef(lm1)[2], slope_digits), 
                     "x \nMSE-R2 = ", r2,  
                     "\nRMSE = ", rmse,
                     "\nNo. Pairs = ", nrow(data.wide))
  
  max_plot <- max(max(data.wide[[x.variable]]), max(data.wide[[y.variable]]) )
  min_plot <- min(min(data.wide[[x.variable]]), min(data.wide[[y.variable]]) )
    
  #compare  
  p <- data.wide %>%
    ggplot(aes(x= data.wide[[x.variable]], y= data.wide[[y.variable]])) + 
    geom_point(alpha=alpha_value, aes(col = data.wide[[col.by]],
                                      shape = data.wide[[shape.var]]
    )) + 
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1) +
    #geom_smooth(aes(fill="loess")) +
    geom_smooth(method = "lm", aes(fill="LS")) +
    xlim(min_plot, max_plot) +
    ylim(min_plot, max_plot) +
    labs(title = #wrapper(
      mytitle, width = title_width
      #)
      ,
         subtitle = mysubtitle,
         caption = mycaption,
         x = x.label,
         y = y.label,
         col = col.by, 
      shape = shape.var,
         fill = "fit") +
    annotate("text", -Inf, Inf, label = fit.info, hjust = 0, vjust = 1)

  return(p)
  
}
```

* fn adds season indicator based on the date

```{r}
# adds sesason to a given dataset with a date variable. Uses typical equinox/solstice dates

add_season <- function(dt, .date_var) {
  
  pacman::p_load(lubridate)
  
  # dt <- aqs_daily
  # .date_var <- "Date.Local"
  
  winter <- "-12-21" #usually winter starts on 21st, sometimes on 22nd 
  spring <- "-03-20"
  summer <- "-06-21" #usually summer starts on 21st, sometimes on 22nd 
  fall <- "-09-23" #usually fall starts on 22nd, sometimes on 23nd. Using 23rd for 2019 mobile monitoring campaign 
  
  dt <- dt %>%
    rename(date_var = .date_var) %>%
    #make sure variable is in date format
    mutate(date_var = as.Date(date_var),
           season = factor(ifelse((date_var >= ymd(paste0((year(date_var)-1), winter)) & date_var < ymd(paste0(year(date_var), spring))) |
                                    date_var >= ymd(paste0(year(date_var), winter)), "winter",
                                  ifelse(date_var >= ymd(paste0(year(date_var), spring)) &
                                           date_var < ymd(paste0(year(date_var), summer)), "spring",
                                         ifelse(date_var >= ymd(paste0(year(date_var), summer)) &
                                                  date_var < ymd(paste0(year(date_var), fall)), "summer", 
                                                ifelse( date_var >= ymd(paste0(year(date_var), fall)) &
                                                          date_var < ymd(paste0(year(date_var), winter)), "fall", 
                                                        NA)))), 
                           levels = c("spring", "summer", "fall", "winter"))
    )
  
  #change time variable back to what it was originally
  names(dt)[names(dt) %in% "date_var"] <- .date_var
  
  return(dt)
  
}

```


* function for a standard boxplot with different whisker definitions to avoid plotting extreme/outlier points

```{r, alt_boxplot fn, echo=T}
#function takes df and returns summary statistics for plotting alternative boxplots with quantiles: 10, 25, 50, 75 and 90. this reduces the plotting of outliers, which are typically problematic when dealign with large datasets. 

alt_boxplot <- function(df, var, min_q=0.025, max_q=0.975){
  df <- df %>%
    rename(var = var) %>%
    
    #calculate quantiles
    summarize(
      Qmin = quantile(var, min_q),
      Q25 = quantile(var, 0.25),
      Q50 = quantile(var, 0.50),
      Q75 = quantile(var, 0.75),
      Qmax = quantile(var, max_q)
      )
  
  names(df)[names(df)==var] <- var
  
  return(df) 
    
}

```

```{r, mse r2, rmse}
#returns MSE
mse <- function(obs, pred){
  mean((obs - pred)^2)
  }

rmse <- function(obs, pred){
  sqrt(mean((obs - pred)^2))
  }


#returns MSE-based R2
r2_mse_based <- function(obs, pred) {
  mse.est <- mse(obs, pred)
  r2 <- 1- mse.est/mean((obs - mean(obs))^2)
  max(0, r2)
  }  

```


# Instrumentation

Instrument notes

* extreme instrument readings outside the range. Individual spikes may indicate poor instrument performance, but sustained spikes may be true though they may have reduced accuracy. Taking median stop concentrations and dropping median concentrations outside the range of the instrument readings should address these ideas?

  - from Tim G: "Not only the magnitude of the reading matters, but also the duration of the extreme value readings.  Instrument readings that are outside the manufacturers' rated range are best omitted from the data set, especially if they are one-time spikes.   A sustained extreme reading suggests that a high level of the pollutant has been detected but the quantity may not be very reliable.  You might consider replacing the numeric levels that exceed the manufacturers' rated maximum with that maximum number for sustained exceedances, on the premise that the device isn't made to measure higher than the stated level."

* The legacy NO2 CAPS monitor used at the start of the study does not have automatic baseline adjustment so would be reset on a weekly basis instead of occurring a few times during the monitoring day as with the newer fast-response unit.

* There were no clock issues with either NanoScan.  The clock issues were primarily with the P-Traks, but the resetting of internal clock before every drive kept the clock drift to a minimum.  After ~6 hrs, the clock for the instrument might be off by 2 to 3 sec. 

```{r}
instrument_range <- dt0 %>%
  distinct(variable) %>%
  mutate(
    Min = c(0, 0, 0, 10^3, 0, 10^2, 0, 0),
    #no max for neph
    Max = c(5e3, 2e3, 5e5, 10^6,NA, 10^6, 10^6, 5e5),
    Units = c("ppm", "ppb", "pt/cm3", "pt/cm3", "bscat/m", "pt/cm3", "ng/m3", "pt/cm3")
    
  )
  
```

Expected instrument values

```{r}
instrument_range %>%
  kable(caption = "Instrumet measurement ranges. Note, nephs don't have a max.") %>%
  kable_styling()

```

```{r}
"from Tim Gould, based on the highway-oriented measurements in the ACT-TRAP study. Some higher concentration levels from vehicle sources or other elevated pollution events are possible.  The ranges are a good indicator but any outlier should not automatically be discarded.

Expected range of measurements by instrument
BC:   0 - 15,000 ng/m^3
CO2:  410 - 600 ppm
NO2:  2 - 100 ppb
neph.:  0.1 e-5 to 7.5 e-5 /m
NanoScan particle total:  1500 to 80,000 pt/cm^3
P-Trak particle count:  500 to 70,000 pt/cm^3"

```

data availability plot

```{r}
#plot 
dt0 %>%
  distinct(date, variable, instrument_id) %>%  
  
  ggplot(aes(x=date, y=instrument_id, col=variable)) + 
  geom_point()

```

```{r}
#tables
dt0 %>%
  distinct(date, variable, #instrument_id
           ) %>%
  group_by(variable, #instrument_id
           ) %>%
  summarize(
      sampling_days = n(),
      start_date = min(date),
      end_date = max(date)
    ) %>%
  add_row(variable="OVERALL MEAN", sampling_days=mean(.$sampling_days)) %>%
  kable(caption = "Total instrument sampling", 
        digits = 0
        ) %>%
  kable_styling()

```

# Quality Assurance 

## Calibrations

* I think the "level" column is what the instrument read, while the conc column is the true concentration during a calibration procedure. 
* pm25 is nephelometer readings 

* **NOTE**: don't use the "analyte" column, which is wrong for some instruments (Amanda confirmed this). Use instrument_id column.

```{r}
# NOTE: don't use "analyte" column, which is wrong for some instruments??

calibrations %>%
  #one record per calibration attempt
  distinct(date, instrument_id) %>%  
  #number of times each instrument was calibrated
  group_by(instrument_id) %>%
  summarize(
    number_of_calibrations = n(),
    dates = paste(date, collapse = ", ")
  ) %>%
  kable(caption = "Number of gas instrument calibrations (incluing nephelometers [PM25]) and particle instrument zero checks during the study period") %>%
  kable_styling()
  
```


## Zeroing for particle intruments
notes   
* notes field does not clarify exact time when the particle filter was placed/removed from the instruments. You can typically see instrument readigns dip for a period of time before going back up though (e.g., 2019-10-22).

observations      

### --> see 5/6 notes 

* The filter may not have been placed correctly/on time on 2019-05-06?

```{r}

# NOTE: don't use "analyte" column, which is wrong for some instruments??

calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|"))),
                    !str_detect(instrument_id, "PM25")
                    ) %>% 
  
  ggplot(aes(x=time, y=value, col=instrument_id, shape=instrument_id)) + 
  scale_shape_manual(values = c(1:length(unique(calibrations$instrument_id)))) +
  facet_wrap(~date, scales="free") + 
  geom_point(alpha=0.5) + 
  geom_hline(yintercept = 0, linetype="dashed")

  
  
# dates
calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|"))),
                    !str_detect(instrument_id, "PM25")
                    ) %>%  
  group_by(date, #instrument_id
           ) %>%
  summarize(
    instruments = paste(unique(instrument_id), collapse = ","),
    #notes = paste(unique(notes), collapse = "," )
  ) %>%
  kable(caption = "zero check dates and instruments. The notes field does not indicate the exact filter placement/removal time from instruments (when instruments should have read '0')") %>%
  kable_styling()
  
  
  
    # #some instruments had duplicate files in one day
  # distinct(instrument_id, date) %>% 
  # group_by(instrument_id) %>%
  # summarize(
  #   number_of_zeros = n(),
  #   dates = paste0(unique(date), collapse = ", ")
  #   
  # ) %>%
  # kable(caption = "when zero checks for particle instruments occurred") %>%
  # kable_styling()

```








```{r}
# Instrument Time Trends w/ All Data

```

```{r}
### --> delete here and only do at  2-min level?
### --> see manuals?
```


notes   
* see how values change as a sampling day goes on (e.g., see NO2 isues?)
* plotting hours as a continous variable   
* note: this is only using stop data 

observations       
* see some instrument sensitivity over time (e.g., CO2 sensitiviy early on)?
* changes over time could occur b/c: a) different sites; b) changing AP levels; c) instrument issues   
* backup instruments typically have a lot less data (so more variable)


```{r, fig.height=12, eval=F}

p <- list()

#give each facet its own instrument_id legend. make plot in loop b/c various things are done to df before plotting. 

for(i in seq_along(ap)) {
  #i=1
  
  p[[i]] <- dt0 %>%
    
    filter(variable == ap[i],
           #location != "MS0000"
           ) %>%
    
    group_by(instrument_id, runname) %>%
    #calculate time since start of run
    mutate(time = as.numeric(difftime(time, min(time), units = "hours"))) %>% #View()
    
    #drop roosevelt after use it ot calculate start time
    filter(location != "MS0000") %>%
    
    ggplot(aes(x=time, y=value, col=instrument_id, 
               linetype=primary_instrument
               )) +
    geom_smooth() +
    facet_wrap(~variable, scales="free") +
    
    # theme(legend.justification=c(0,0), legend.position=c(0,0),
    #       #legend.key = element_blank(), #legend.key.size = 1
    #       ) +
     
    labs(x= "hours since departing Roosevelt garage",
         col="",
         linetype = ""
         )  
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)

```

same as above but by stop order number (note, some routes may take a while to get to first stop, e.g., route 8)



```{r, fig.height=12, eval=F}
### --> error: plots r empty 

 
#plot with all the data
p0 <- dt0 %>%
    ggplot(aes(x=location, y=value, col=instrument_id, 
               linetype=primary_instrument
               )) +
   geom_boxplot() + 
  #geom_smooth() +
    facet_wrap(~variable, scales="free") +
    
    #theme(legend.justification=c(0,0), legend.position=c(0,0)) +
    
    labs(x= "stop order number within a run",
         col="",
         linetype = ""
         )

p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1
  
  #only plot a subset of the data
  p[[i]] <- p0 %+% subset(dt0, variable == ap[i])

  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)


```


 


# Fieldnotes and instrument warnings 

### --> see if instrument errors are associated w/ poor instrument collocations

### -> have Kaya code these and ID things that need attention? how to do this efficiently?

notes    
* see QAQC.html doc that Amanda and Kaya have worked on 
  * most instrument error codes don't have "use" indicators to clarify whether that data should be used or not though (as of 5/11/21)     

*e.g., alcohol wick issues 

```{r}
fieldnotes2 <- fieldnotes %>%
  #only look at stop data w/ notes
  filter(
    str_detect (site_id, regex("^MS|MC", ignore_case = T)),
    !is.na(notes),  
    !notes %in% c("", ".", "\\")
  )


```



# Summary of all raw data 

* note, some instruments have very high, abnormal readings
  - e.g., neph 176

```{r, fig.height=12}
#density plots 

p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  df <- dt0 %>%
    #drop Roosevelt Garage
    filter(location != "MS0000",
           variable== ap[i]) #%>% 
  ##example - look at a subset of data. for initial plotting
    #filter(sample(c(TRUE, FALSE), size = nrow(.), prob = c(0.1, 0.9), replace = T))
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    #guides(linetype = FALSE) + 
    labs(title = "raw instrument readings with min/max annotated",
         linetype = "Instrument",
         col = "ID"
         )
   
    
  
  #p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)

```

```{r, eval=F}

dt0 %>%
  #drop Roosevelt Garage
  filter(location != "MS0000") %>%
  
  group_by(#instrument_id,
           variable
           ) %>%
  summarize(
    no_readings = n(),
    Min = min(value),
    # q01 = quantile(value, 0.01),
    #Q02.5 = quantile(value, 0.025),
    #Q25 = quantile(value, 0.25),
    #Q50 = quantile(value, 0.50),
    #Q75 = quantile(value, 0.75),
    #Q97.5 = quantile(value, 0.975),
    # q99 = quantile(value, 0.99),
    Max = max(value),
  ) %>%
  # # scientific notation w/ few sigfigs 
  # mutate_at( vars(-no_readings), ~format(., scientific=T, digits = 3) ) %>%
  
  kable(caption="raw instrument readings",
        ) %>%
  kable_styling()

```



```{r}
# # Drop bad data 
# 
# ### --> do this at 2-mi level? 
# 
# ## extreme values (e.g., low)
# 
# -very high vs low (e.g., pt<300; MOVUP dropped readings >27k ng/m3 - in top 1% data)
# -- compare against other types of instruments as well? 
# - BC: check that attenuation < 50% (_atn1). Instruments are most sensitive at detecting changes below this threshold
# 
# 
# * dropping UFP readings < 300 pt/cm3, which are very  unlikely

```


### --> drop instrument error messages? 

```{r}

```


```{r}
# note: doing this at the stop level. otherwise PMPTSCREEN_94 has some low values that don't correlate wel w/ PMPTSCREEN_3

low_pt_val <- 300

dt <- dt0 #%>%
  # filter(
  #   #drop particle readings of 0 since these are often associated w/ alcohol wick issues (not inlcuding neph)
  #   !(grepl("PM", instrument_id) & !grepl("PM25", instrument_id)  & value <= low_pt_val)
  #  )  
  
```

```{r}
# proportion of data dropped

```

```{r, eval=F}
(nrow(dt0) - nrow(dt))/nrow(dt0) # ~0.04%

```

 
```{r,}
#table 

dt %>%
  #drop Roosevelt Garage
  filter(location != "MS0000") %>%
  
  group_by(instrument_id) %>%
  summarize(
    no_readings = n(),
    Min = min(value),
    # q01 = quantile(value, 0.01),
    Q02.5 = quantile(value, 0.025),
    Q25 = quantile(value, 0.25),
    Q50 = quantile(value, 0.50),
    Q75 = quantile(value, 0.75),
    Q97.5 = quantile(value, 0.975),
    # q99 = quantile(value, 0.99),
    Max = max(value),
  ) %>%
  # scientific notation w/ few sigfigs 
  mutate_at( vars(-no_readings), ~format(., scientific=T, digits = 3) ) %>%
  
  kable(caption="raw instrument readings",
        ) %>%
  kable_styling()

```




# Calculate Site medians (for each instrument)

 
- don't include lab data here? 

```{r}
#note: on rare instantces when duplicate instruments were on board, all the data are used to calculate a median
dt_stops <- dt %>%
  filter(location != "MS0000") %>%
  #calculate arrival time at any stop
  group_by(runname, date, location) %>%
  mutate(time = min(time)) %>%  
  
   group_by(runname, date, time, location, site, instrument_id, variable, primary_instrument) %>%
  summarize(
    #mean_value = mean(value),
    value = median(value)
  ) %>%
  ungroup()

```


# Summary of data 

notes    
* boxplots are not traditinal. they show specific quantiles 

 
```{r, fig.height=12, eval=F}
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  p[[i]] <- dt_stops %>%
  
  filter(
    ##drop Roosevelt Garage
    #location != "MS0000",
         variable== ap[i]
         ) %>%
    
    #calc stats for alternative boxplots
    group_by(instrument_id, variable, primary_instrument) %>%
    alt_boxplot(var = "value") %>%  
    
    ggplot(aes(x=instrument_id, #col=instrument_id, 
               linetype=primary_instrument),) + 
    geom_boxplot(aes(ymin=Qmin, lower=Q25, middle=Q50, upper=Q75, ymax=Qmax), stat = "identity") + 
    
    facet_wrap(~variable, scales="free") +
    
    labs(linetype="Instrument", col="ID")
  
  #p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)


```


* labels are for the min/max reading of each instrument 

```{r, fig.height=12, eval=T}
 
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  df <- dt_stops %>%
            ##drop Roosevelt Garage
    filter(#location != "MS0000",
           variable== ap[i])  
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    guides(linetype = FALSE)
   
    # don't do since we have values <=0 
    #scale_x_log10()
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)


```



amount of data outside of expectd instrument ranges
   
* out of range date is more likely to be below the minimum instrument range. Removing all values outside the instrument range could thus bias overall median slightly up (this was more true when we ran this at the raw second level, where ~16% of BC data was below the min)

```{r}
dt_stops %>%
  #observations per pollutant
  group_by(variable, #instrument_id
           ) %>%
  mutate(n_original = n()) %>%
  ungroup() %>%
  
  left_join(instrument_range) %>%
  #filter(value >= Min, value <= Max) %>% #View()
  mutate(below_min = value < Min,
         above_max = value > Max
         ) %>%
  
  group_by(variable, #instrument_id
           ) %>%
  summarize(
    n_original = unique(n_original),
    n_below_min = sum(below_min),
    n_above_max = sum(above_max),
    
    prop_below_min = mean(below_min),
    prop_above_max = mean(above_max),
    total_prop_out_of_range = sum(prop_below_min, prop_above_max, na.rm = T)
  ) %>%
  kable(caption = "proportion of data outside the instrument range. N = number of stops (~309 stops x ~29 visits/stop)", digits = 2) %>%
  kable_styling()


```

stop estimates when values outside the range of data are dropped

```{r, fig.height=12}
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=3

  #only plot a subset of the data
  df <- dt_stops %>%
    left_join(instrument_range) %>% 
    
    #neph has no max. need a number to avoid an error
    mutate(Max = ifelse(is.na(Max), 9999999, Max)) %>% #View()
    
    filter( 
           variable== ap[i],
           #drop values outside the expected instrument range
           value >= Min, value <= Max 
           ) 
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    guides(linetype = FALSE)
   
    # don't do since we have values <=0 
    #scale_x_log10()
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4) %>%
  annotate_figure(top = "Stop medians when stops with values outside an instrument range are dropped")

```

## Update data

* **drop median estimates outside the range of each instrument's reporting range**    
* drop NanoScan and non-screened PTRAK values < 300 pt/cm3   

```{r}
min_tot_ufp <- 300

dt_stops2 <- dt_stops %>%
    left_join(instrument_range) %>% 
    #neph has no max. need a number to avoid an error
    mutate(Max = ifelse(is.na(Max), 9999999, Max)) %>% #View()
    filter(
      #drop values outside the expected instrument range
      value >= Min, value <= Max 
           )  %>%
  #drops 3 UFP values < 300
  filter(!(variable %in% c("ns_total_conc", "pnc_noscreen") &  value < min_tot_ufp ))
  
 


#keep approved stop data, but see raw values
dt2 <- dt_stops2 %>%
  distinct(date, location, instrument_id) %>%
  left_join(dt)

```


# Time series of when the lowest and highest values were observed

* as an example, we're just looking at the lowest and highest readings here

* pulling out the second data used to calculate site medians to better inspect the instrument patterns for possible errors

```{r}

#create standardized values & relabel UFP instruments
dt2_temp <- dt2 %>%  
    #scale values using all the data 
    group_by(variable) %>%
    mutate(value = scale(value)) %>%
  ungroup() %>%
  mutate(
    #simplify plots
    variable = ifelse(variable %in% c("ns_total_conc", "pmdisc_number", "pnc_noscreen", "pnc_screen"), "UFP", variable)
    )

```

**highest (max) readings**

```{r}
# pull out stops when we saw max values
extreme_high <- dt_stops2 %>%
  group_by(variable) %>%
  mutate(median_overall_value = median(value)) %>%
  filter(value == max(value)) %>% 
  #ungroup() %>%
  distinct(date, location, instrument_id, value, median_overall_value) %>%
  group_by(date, location) %>%
  summarize(instrument_id = paste0(unique(instrument_id), collapse = "|"),
            median_stop_value = paste0(unique(value), collapse = "|"),
            median_overall_stop_value = paste0(unique(median_overall_value), collapse = "|")
            )
  
extreme_high %>%
  kable(caption = "stops where the max instrument value has been observed") %>%
  kable_styling()
```

* high instrument observations tend to occur when other particle readings are also high, particularly for particle instruments

* CO2_19 (backup) has much higher readings than co2_14 (primary) on 6/13 and higher readings than other CO2 readings.     
  * **drop all co2_19 readings?** co2_19 also recorded the highest co2 readings in the dataset on the same day at different stops. CO2_19 was never used on its own.

* BC on 2/10 stands out here, but other particle instruments are also increasing though less so
* **keep NO2_2 on 5/11 since otehr instruments are also high?**


```{r, fig.height=10}
df <- data.frame()

for (i in 1:nrow(extreme_high)) {
  #i=1
  
  temp <-  dt2_temp %>%
    filter(date == extreme_high$date[i] & location == extreme_high$location[i]) %>%
    mutate(
      #pt_size = ifelse(grepl(extreme_high$instrument_id[i], instrument_id), 1, 1),
      extreme_instrument = ifelse(grepl(extreme_high$instrument_id[i], instrument_id), instrument_id, NA),
           ) 
    df <- bind_rows(df, temp)
  
}
  
temp_labels <- df %>%
  drop_na() %>%
  
  group_by(date, location) %>%

  mutate(time = min(time),
         value = max(value)
         ) %>%
  ungroup() %>%
  
  group_by(date, location, time, value) %>%
  summarize(
    extreme_instrument = paste0(unique(extreme_instrument), collapse = ", ")
  )
  
  df %>%  {
    
    ggplot(., aes(x=time, y=value)) + 
    
      #black points make it easier to ID questionnable data
      geom_point(data=subset(., instrument_id==extreme_instrument), size=2) +
      
      geom_point(aes(shape=instrument_id, col=variable)) +
      geom_text(data = temp_labels, aes(label= extreme_instrument), 
               hjust=0, show.legend = F) +
    
      facet_wrap(date~location, scales = "free") +
      scale_shape_manual(values = c(1:length(unique(dt_stops$instrument_id)))) +
      
      #don't show size in legend 
      guides(size = F) + 
      
      labs(y = "Standardized Value: (X-mean)/SD",
           title = "Plots of when high instrument readings occur"
           )
  }
  
```

**lowest (min) readings**

* there are more instances with min readings than max readings

```{r}
# pull out stops when we saw max values
extreme_low <- dt_stops2 %>%
  group_by(variable) %>%
  filter(value == min(value) #| value == min(value),
         ) %>% 
  distinct(date, location, instrument_id) %>%
  group_by(date, location) %>%
  summarize(instrument_id = paste0(unique(instrument_id), collapse = "|"))
  
extreme_low %>%
  kable(caption = "stops where the min instrument value has been observed") %>%
  kable_styling()
```

* nothing stands out as being unordinaryly low? 

* BC tends to have low readings 

```{r, fig.height=20}

df <- data.frame()

for (i in 1:nrow(extreme_low)) {
  #i=1
  
  temp <-  dt2_temp %>%
    filter(date == extreme_low$date[i] & location == extreme_low$location[i]) %>%
    mutate(
      #pt_size = ifelse(grepl(extreme_low$instrument_id[i], instrument_id), 1, 1),
      extreme_instrument = ifelse(grepl(extreme_low$instrument_id[i], instrument_id), instrument_id, NA),
           ) 
    df <- bind_rows(df, temp)
  
}
  
temp_labels <- df %>%
  drop_na() %>%
  ungroup() %>%
  group_by(date, location) %>%

  mutate(time = min(time),
         value = max(value)
         ) %>%
  
  group_by(date, location, time, value) %>%
  summarize(extreme_instrument = paste0(unique(extreme_instrument), collapse = ", "))
  
  df %>%  {
    
    ggplot(., aes(x=time, y=value)) + 
    
      #black points make it easier to ID questionnable data
      geom_point(data=subset(., instrument_id==extreme_instrument), size=2) +
      
      geom_point(aes(shape=instrument_id, col=variable)) +
      geom_text(data = temp_labels, aes(label= extreme_instrument), 
               hjust=0, show.legend = F) +
    
      facet_wrap(date~location, scales = "free") +
      scale_shape_manual(values = c(1:length(unique(dt_stops$instrument_id)))) +
      
      #don't show size in legend 
      guides(size = F) + 
      
      labs(y = "Standardized Value: (X-mean)/SD",
           title = "Plots of when low instrument readings occur"
           )
        
      }
```


## Update data

* drop co2_19 (backup instrument) readings for a day when these readings were much higher than other collocated instrument and other days 

```{r}

dt_stops3 <- dt_stops2 %>%
  filter(
    #drop co2_19 (backup instrument) readings for a day when these readings were much higher than other collocated instrument and other days 
    !(date ==extreme_high$date[3] & location== extreme_high$location[3] & instrument_id==extreme_high$instrument_id[3])
  )

```


# Compare collocated instruments 

### --> pull lab/garage collocation data from results table (see locations: LABC, RM114, WILCOXAMB that are not necessarily collocation times)

```{r}
dt_stops_wide <- dt_stops3 %>%
  select(-primary_instrument) %>%
  spread(instrument_id, value)

# dt_wide <- dt %>%
#   spread(instrument_id, value) 
  
```

### --> note, have to modify code if have > 2 collocated duplicate instruments (e.g. PMPTSCREEN)

```{r}
df <- data.frame()

#only keep stop data w/ collocations
colo3 <- dt_stops3 %>%
  group_by(time, location, variable) %>%
  filter(length(unique(instrument_id)) >1 ) %>% 
  ungroup() %>%
  #make sure this is arranged by time
  arrange(time)
  #select(-primary_instrument)

for(i in seq_along(ap)) {
  #i=8
  
  temp <- colo3 %>%
    filter(variable == ap[i]) %>%
    group_by(variable) %>% #View()
    summarize(
      n_pairs = n()/2,
      
      R2_reg = cor(
        .$value[.$instrument_id == unique(.$instrument_id)[1] ],
        .$value[.$instrument_id == unique(.$instrument_id)[2] ]
                   )^2,
      R2_mse = r2_mse_based(
        .$value[.$instrument_id == unique(.$instrument_id)[1] ],
        .$value[.$instrument_id == unique(.$instrument_id)[2] ]
        ),
      RMSE =rmse(
        .$value[.$instrument_id == unique(.$instrument_id)[1] ],
        .$value[.$instrument_id == unique(.$instrument_id)[2] ]
        )
      ) %>%
    mutate_at(vars(contains("R2")), ~round(., 2)) %>%
    #note: neph gets 0 b/c values are small
    mutate(RMSE = round(RMSE, 0)
        
    )  
  
  df <- rbind(df, temp)
  
}

df %>%
  kable(caption = "Agreement between collocated duplicate instruments. Based on site medians", 
        #digits = 2
          ) %>%
  kable_styling()

```



* BC looks good 
* **CO2 has a poor fit w/ the backup instrument**, which tends to report high readings

  * drop co2_19, especially since it is never used on its own?
* NO2 looks good 
* nephs look good
* discminis look good

* PTRAKS look good
* screened P-TRAKS look good    
  * only these 2 screened PTRAKS were collocated.

* **PMSCAN 3 has lower readings in 2020**

```{r, fig.height=18}

p <- list()

p[[1]] <- dt_stops_wide %>%
  colo.plot(x.variable = "BC_0066", y.variable = "BC_0063", col.by = "site", 
            slope_digits = 2, alpha_value = 0.7) + 
  labs(col="Route")


p[[2]] <- dt_stops_wide %>%
  colo.plot(x.variable = "CO2_14", y.variable = "CO2_19", col.by = "site", alpha_value = 0.7
            ) + 
  labs(col="Route")

p[[3]] <- dt_stops_wide %>%
  colo.plot(x.variable = "NO2_2", y.variable = "NO2_1", col.by = "site", 
            slope_digits = 0, int_digits = 2, alpha_value = 0.7) + 
  labs(col="Route")

p[[4]] <- dt_stops_wide %>%
  colo.plot(x.variable = "PM25_176", y.variable = "PM25_205", col.by = "site", int_digits = 6, rmse.digits = 6, alpha_value = 0.7) + 
  labs(col="Route")

p[[5]] <- dt_stops_wide %>%
  colo.plot(x.variable = "PMDISC_3", y.variable = "PMDISC_8", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

p[[6]] <- dt_stops_wide %>%
  colo.plot(x.variable = "PMPT_94", y.variable = "PMPT_93", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

p[[7]] <- dt_stops_wide %>%
  colo.plot(x.variable = "PMPTSCREEN_94", y.variable = "PMPTSCREEN_3", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

p[[8]] <- dt_stops_wide %>%
  colo.plot(x.variable = "PMSCAN_5", y.variable = "PMSCAN_3", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

ggarrange(plotlist = p, ncol = 2, nrow = 4) %>%
  annotate_figure(top = "Instrument Collocations")

```


* PMSCAN_3 looks fine before 2020. We'll only keep the data collected during 2019, which will allow us to use data collected with this instrument a few days before PMSCAN_5

```{r}
dt_stops_wide %>%
  filter(year(date) < "2020"
         #date < "2019-04-01"
         ) %>%
  colo.plot(x.variable = "PMSCAN_5", y.variable = "PMSCAN_3", col.by = "runname", alpha_value = 0.7, mytitle = "Collocated NanoScan data during 2019 (excludes 2020)") 
   
```

## Collocation instrument time trends

*see if duplicate instruments behave differenlty over time when in exact same place 

* still see some trends related to how long instruments are run (is this real?)   
 
```{r}

colo <- data.frame()

for (i in seq_along(instruments)) {
  #i=1
  
  colo0 <- dt_stops_wide %>%
    #these were never collocated w/ the others
    select(-PMPTSCREEN_2, -PMPTSCREEN_93) %>% 
    
    select(date, time, location, site, variable, contains(paste0(instruments[i], "_" ))) %>%
    drop_na() %>%
    gather("instrument_id", "value", contains(instruments[i])) 
  
  if(i==1) {
    colo <- colo0
    } else
      colo <- rbind(colo, colo0)  
  
}
  
colo <- colo %>%
    mutate(
      primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup"),
      primary_instrument = factor(primary_instrument, levels = c("Primary", "Backup"))
    )

#unique(colo$instrument_id)

```

* **things look good? would expect concentrations to increase as the day goes on if you depart early on, and the opposite to happen if you start in the afternoon**


```{r, fig.height=12}
p <- list()

#give each facet its own instrument_id legend. make plot in loop b/c various things are done to df before plotting. 

for(i in seq_along(ap)) {
  #i=2
  
  p[[i]] <- colo %>%
    
    filter(variable == ap[i]) %>% #View()
    
    group_by(variable, #instrument_id, 
             date) %>%
    #calculate time since start of run
    mutate(first_stop = min(time),
           time = as.numeric(difftime(time, first_stop, units = "hours")),
           
           ) %>%  # View()
    
    ggplot(aes(x=time, y=value, 
               #col=factor(date),
               #col=instrument_id, 
               linetype=primary_instrument)) +
    facet_wrap(~variable, scales="free") +
    geom_smooth(aes(col=factor(first_stop),), se=F,
                alpha=0.8
                ) +
    geom_smooth(col="black") +
    
    # geom_point(aes(col=factor(date)), alpha=0.8) + geom_line(aes(col=factor(date)), alpha=0.8) +
    # geom_point() + geom_line() +
    
    labs(#title = "collocated instrument readings",
         x= "hours since first stop",
         col="1st Stop Time",
         linetype=""
         )  
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4) %>%
  annotate_figure(top = "collocated instrument readings. black = overall trend")

```


# Update Datasets

**Stop level data**

* dropped stop medians outside each instrument's reporting range b/c these readings are less trustworthy/accurate (kept readings at the second level since we will mainly be dealing w/ stop medians)

* Dropped all co2_19 readings b/c    
  * this instrument tends to report high values
  * its values were higher than CO2_14 when they were collocated
  * it was a backup instrument that was never used on its own

* dropped SCAN 3 after 2019 when collocation readings looked different than SCAN 5
  * we collected a few extra days of data with this instrument at the begining of the campiagn   
  * SCAN 3 (Edmund's) was an older instrument    

* if duplicate instruments were on the platform, calculate their median value

**Second-level data**

* see above for inclusion criteria   
* note: extremes are still included in these data if stop medians were within an instrument's range

```{r}
dt_stops4 <- dt_stops3 %>%
  #don't need instrument ranges anymore
  select(-c(Min, Max, Units)) %>%
  filter(
    #drop all CO2_19 data
    !instrument_id %in% c("CO2_19"),
    #drop 2020 PMSCAN_3 data after 2019
    !(instrument_id == "PMSCAN_3" & year(date) > "2019"),
         ) %>%
  # #if multiple instruments on platform, use the median of the two medians
   group_by_at(vars(-c(instrument_id, primary_instrument, value))) %>%
   summarize(value = median(value)) %>%
  ungroup()
  

# note, this dataset could still have some extreme values (this hasn't been vetted as much as stop data)
dt3 <- dt2 %>%
  filter(
    #drop all CO2_19 data
    !instrument_id %in% c("CO2_19"),
    #drop 2020 PMSCAN_3 data after 2019
    !(instrument_id == "PMSCAN_3" & year(date) > "2019"),
         )
  
```


# Instrument time trends (all data)

* things look fine?    
  * e.g., don't see extreme NO2 trends      
* using log10 y scale since there are some extreme values 


```{r, fig.height=12}

dt_stops4 %>%
  #have to group by runname (date and route) b/c some runs ended at midnight and the grouping gets messed up otherwise
  group_by(variable,  runname) %>%
    #calculate time since start of run
    mutate(first_stop = min(time),
           time = as.numeric(difftime(time, first_stop, units = "hours")),
           ) %>%  
  {
    ggplot(., aes(x=time, y=value, 
               #col=instrument_id, 
               #linetype=primary_instrument
               )) +
      facet_wrap(~variable, scales="free") +
      geom_smooth(se=F, alpha=0.8) +
      
                        #dont plot extreme points - easier visualization
      geom_point(#data=filter(., value < quantile(value, 0.99)),
                 alpha=0.05) +
      #easier to visualize overall trend w/ extreme values
      scale_y_log10() +
    
    labs(title = "instrument readings over time (all remaining data)",
         subtitle = "trend lines use all data",
         x= "hours since first stop",
         linetype="Instrument"
         )  
}


```



# AQS sites 

Looking at the ACT TRAP field protocol, it looks like BC, NO2 and neph (also neph PM2.5) values are collected every minute, though these are not officially validated like the hourly data are. We will use these instead of hourly data since we want to compare these readings to our 2-minute collocations.

Note that BH and 10W dont have minute data. BH has rolling 1-hour averages every 6 minutes for PM2.5 (teoms), while 10W seems to report the same thing every minute for about an hour (~58 min). 

We will have to use a mix of semi-hourly PM2.5 data and hourly data for BH and 10W since data comes from various non-minute-level sources.


-see if our campaign measurements were similar to those from AQS sites. Compare AQS:
  - 2-min medians
  - 2-min annual avg estimate
  - true annual avg w/ full data


```{r}
 
#stop collocations
dt_stops3_colo <- dt_stops4 %>%
  filter(
    #pollutants also measured at AQS sites
    variable %in% c("ma200_ir_bc1", "neph_bscat", "no2"),
    #only keep collocation sites
    grepl("MC", location)
    ) %>% 

  #drop seconds from time stamp (for DOE merging). change default tz of UTC to LA
  mutate(time = ymd_hm(format(time, format = "%Y-%m-%d %H:%M"), tz=mytz),  #tz=mytz
         #time = force_tz(time, tzone = mytz),
         ) %>%  
  #if duplicate instruments, take mean value
  group_by(date, time, location, variable) %>%
  summarize(value = mean(value)) %>% 
  ungroup() %>%
  
  spread(variable, value) %>%
  arrange(time)  %>%
  
  #rename pollutants
  rename(BC = ma200_ir_bc1,
         Neph = neph_bscat,
         NO2 = no2
         )  %>%
  mutate(
    # put in same units as DOE
    ##change from ng/m3 to ug/m3
    BC = BC/10^3,
    ##change from bscat/m to (bscatx10^-4)/m
    Neph = Neph*10^4
  )

# data from DOE
doe_colo <- dt_stops3_colo %>%
  #also pull 2nd minute from DOE data
  mutate(time = time+60) %>%
  #add start time (1 min earlier)
  rbind(dt_stops3_colo,.) %>%
  #collocation times & places
  select(time, location) %>%
  arrange(time) %>% 
  # doe data 
  left_join(doe2) %>%  
  #MC0406 not visited on 2020-03-13 06:41:00	
  drop_na() %>%# View()
  #calculate 2-min median - same as avg for 2 numbers.
  group_by(date, location, location_doe, pollutant) %>%
  summarize(value=median(value)) %>% 
  #wide format
  pivot_wider(names_from = pollutant, values_from = value, names_prefix = "doe_") #%>%
  #don't have mobile monitoring equivalent
  #select(-doe_PM2.5) 
  
#merge our estimates and doe estimates
dt_doe <- left_join(dt_stops3_colo, doe_colo) 
  

```

## Calibrate neph readings

* e.g., PM2.5 will be used for Kaya's disparities paper 

- if our neph readings are similar to the readings from AQS sites (near 1-1 line), we'll use Cooper's calibration curve to estimate PM2.5 mass concentration from bscat. Alternatively, we can create a new calibration based on the PM2.5 readings at the AQS sites we visited during our campaign (?).

* see what how the DOE calibrates nephelometer readings to estimate PM2.5. Note that they do this at the site level.

```{r}
# #doe2 #could use this, but have to filter specific sites that used npm25 for PM2.5 estimates
# #
# doe %>%
#   select(-c(import_id, pollutant)) %>%
#   filter(variable %in% c("doe_neph", "doe_npm25"),
#          freq=="minute"
#          ) %>% 
#   spread(variable, value) %>% 
#   
#   colo.plot(x.variable = "doe_neph", y.variable = "doe_npm25", col.by="location_doe")

```

### --> OK to use doe_npm25 as dependent variable in this model (vs. hourly PM2.5 - what DOE uses in their models)? SHouldn't make a big difference? 

```{r}
#lm fit
doe_lm <- doe %>%
  select(-c(import_id, pollutant)) %>%
  filter(variable %in% c("doe_neph", "doe_npm25"),
         freq=="minute"
         ) %>% 
  spread(variable, value) %>%
  
  lm(doe_npm25~doe_neph, data=.) 

doe_lm %>% summary()

doe_int <- tidy(doe_lm)$estimate[tidy(doe_lm)$term=="(Intercept)"]
doe_slope <- tidy(doe_lm)$estimate[tidy(doe_lm)$term=="doe_neph"]
   
```


### --> use doe calibration curve instead of ours since this is at the minute level? otherwise need to update calibration curve when Dave updates this.

* using doe neph-pm2.5 calibration at three AQS sites since    
  * these are at the minute level
  * there are a lot of pairs?

```{r}
#convert bscat ___/m to PM2.5 ug/m3 

#estimates from Cooper's work. this will be updated based on what Dave Slager finds.
# int <- 1.060539
# slope <- 2.509793 #*10

# estimates from DOE data (see above)  
int <- doe_int
slope <- doe_slope

```

```{r}
dt_doe <- dt_doe %>%
  #estimate PM2.5 from neph readings
  mutate(PM2.5 = Neph*slope + int)

```


## 2-min collocations (medians)

* There are various discrepancies between DOE and mobile monitoring observations    
  * locations - these are not exaclty the same. Also, the MM vehicle does not always park in the exact same location    
  * instrument differences    
    * all of our PM2.5 estimates come from neph estimates. DOE PM2.5 estimates compared here are both from nephelometer and other measurement methods.     
    * DOE uses site-specific neph-PM2.5 calibration curves. We will use a region calibraiton curve based on multiple AQS sites in the Spatiotemporal modeling region     
      * our MM calibration curve from the ST model excludes Duwamish, even though we use it to predict PM2.5 at Duwamish in this analysis   
    * DOE PM2.5 "minute" readings are actually for longer time periods. For example, rolling 1-hr estimates that are updated every 6 min.
    * the "2-min" median estimates have slightly different start and end times for the MM and DOE estimates. DOE estimates area alwyays exactly 2-min, starting at 00 seconds. MM median stops could have started after 00 seconds into the minute and could have been sligthly longer than 2 min (e.g., 3 min).


###--> use same legends for facets 

```{r, fig.height=10}
p <- list()

#BC
p[[1]] <- dt_doe %>%
  colo.plot(x.variable = "doe_BC", y.variable = "BC", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "BC (ug/m3)",
       y = "Mobile Monitoring",
       x = "DOE"
       )

#Neph
p[[2]] <- dt_doe %>%
  colo.plot(x.variable = "doe_Neph", y.variable = "Neph", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "Nephelometer (bscat [x 10^-4]/m)",
       y = "Mobile Monitoring",
       x = "DOE"
       )

#PM2.5
p[[3]] <- dt_doe %>%
  colo.plot(x.variable = "doe_PM2.5", y.variable = "PM2.5", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "PM2.5 (ug/m3)",
       y = "Mobile Monitoring (neph estimate)",
       x = "DOE"
       )

#NO2
p[[4]] <- dt_doe %>%
  colo.plot(x.variable = "doe_NO2", y.variable = "NO2", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "NO2 (ppb)",
       y = "Mobile Monitoring",
       x = "DOE"
       )
  
ggarrange(plotlist = p) %>%
  annotate_figure(top = "2-min collocations (medians)")

```

## Annual Avg comparisons

```{r}
#calculate annual averages
true_annual <- doe2 %>%
  group_by(location_doe, location, pollutant) %>%
  summarize(
    true_avg = mean(value)
  )

short_term_annual <- dt_doe %>%
  #make long format
  gather("pollutant", "value", matches(c("bc", "neph", "no2", "PM2.5"))) %>%   
  mutate(
    source = ifelse(grepl("doe", pollutant), "DOE", "Mobile Monitoring"),
    pollutant = gsub("doe_", "", pollutant)
  )  %>%
  #dorp places w/o data
  drop_na() %>% 
  group_by(location, location_doe, pollutant, source) %>%
  summarize(
    value = mean(value)
  )

#note that DOE PM2.5 has no MM equivalent (yet)
true_annual <- left_join(true_annual, short_term_annual) %>%
  drop_na(value) %>%
  ungroup()

```

* annual average neph estimates are not great    
  * though we only had 3 sites with neph readings, and the range was narrow (?)       
* neph-based PM2.5    
  * looks OK except for duwamish for MM campaign    
  * looks better for DOE since they fit site-specific calibration curves   

```{r, fig.height=10}
p <- list()

u_pollutant <- unique(true_annual$pollutant)

for (i in seq_along(u_pollutant)) {
  #i=1
  df <- true_annual %>%
    filter(pollutant == u_pollutant[i])
    
  range <-  df %>%
    summarize(
      min = min(c(true_avg, value)),
      max = max(c(true_avg, value))
    )  
  
  p[[i]] <- df %>%
    ggplot(aes(x=true_avg, y=value, col=source)) + 
    geom_point(aes(shape=location_doe)) + 
    geom_smooth(method = "lm" ,se=F, aes()) + 
    geom_abline(slope = 1, intercept = 0) + 
    lims(x=c(range$min, range$max), y=c(range$min, range$max)) + 
    coord_fixed() +
    labs(title = u_pollutant[i],
         col = "2-min Est",
         shape = "Location",         
         x="True Annual Average",
         y = "Estimate"
         )
  
    # colo.plot(x.variable = "true_avg", y.variable = "value", col.by = "location", shape.var = "source", alpha_value = 0.8, mytitle = u_pollutant[i]) +
  
}

ggarrange(plotlist = p) %>%
  annotate_figure(top = "Annual average estimates from 2-min readings")

```

 
```{r}
true_annual %>%
  group_by(pollutant, source) %>%
  summarize(
    R2_reg = cor(true_avg, value)^2,
    R2_mse = r2_mse_based(true_avg, value),
    RMSE = rmse(true_avg, value)
  ) %>%
  kable(caption = "site annual average estimates from 2-min readings relative to the true annual average", 
        digits = 2
        ) %>%
  kable_styling()
  
  
```


Do the observed annual avearge neph readings have a narrow range?

* the distribution of minute-level neph data is much wider than the range of neph estimates we see for the annual averages above. This suggests that the "poor" performing neph readings are all close in range to one another and hence why the plot may be making these look bad even though they are not that different. 
  * neph estimates for these sites (Duwamish, Kent, Tukwilla) are also on the high end relative to 

```{r}
doe2 %>% 
  filter(pollutant == "Neph",
         #sites where we're using neph-based PM2.5 (vs e.g., FEM method)
         #location_doe %in% c("AQSD", "AQSK", "AQSTUK")
         ) %>%
  summarize(
    min = min(value),
    Q05 = quantile(value, 0.05),
    Q25 = quantile(value, 0.25),
    Q50 = median(value),
    Q75 = quantile(value, 0.75),
    #IQR = IQR(value),
    Q95 = quantile(value, 0.95),
    max= max(value)
  )
   
```



When did we visit neph DOE sites? Checking that we conducted balanced sampling at these collocation sites.

```{r}
neph_colos <- dt_doe %>%
  #sites with neph data
  filter(location_doe %in% c("AQSD", "AQSK", "AQSTUK")) %>%
  
  select(location_doe, date, time, Neph) %>%
  drop_na() %>%
  mutate(
    hour = factor(hour(time)),
    day = wday(time, label = T, week_start = 1),
  ) %>% 
  add_season(.date_var = "date") 

```

* things look OK?  
  * visits by season-day of week look okay. visits didn't occur on every combination, but weekday and weekend samples occurred    
  * there are more gaps in season-hour combinations, though these are also intermittent for the most part   

```{r}
#hour
neph_colos %>%
  ggplot(aes(x=hour, y=location_doe)) + 
  geom_bin2d() +
  facet_wrap(~season) + 
  #geom_bar(stat = "count", position = "dodge") 
  labs(title = "number of visits to DOE collocation sites with nephs")

#day
neph_colos %>%
  ggplot(aes(x=day, y=location_doe)) + #fill = location_doe
  geom_bin2d() +
  facet_wrap(~season) +
  #geom_bar(stat = "count", position = "dodge")
  labs(title = "number of visits to DOE collocation sites with nephs")

# #season
# neph_colos %>%
#   ggplot(aes(x=season, y=location_doe)) + #fill = location_doe
#     geom_bin2d()


```



# Appendix

## Calibrate to the mean of instrument readings

* we will not do this since there was generally good agreement between instruments and some backup instruments were less trustworthy (e.g., NO2_1, PMSCAN_3, PMDISC_8 was dropped at one point before being replaced).

### --> update point below if have room collocation for all PTRAKS screened

* note that the screened PTRAKS were not all collocated, though we know that 93 and 94 were similar when they were unscreened, and that 94 and 3 were similar, thus we will assume that at least 3/4 of the instruments were in good agrement. We don't have any collocation information for PTRAK 2, which collected a substantial amount of data.



# Save final datasets & variables

```{r, eval=F}
# median stop data 
saveRDS(dt_stops4, file.path("Data", "Output", "stop_data.rda"))

#second data
saveRDS(dt3, file.path("Data", "Output", "second_data.rda"))

# variables
save(ap,       file = file.path("Data", "Output", "common_vars.rda"))

```

