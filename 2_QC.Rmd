---
title: "Instrument Quality Control (QC)"
author: "Magali Blanco"
date: ' `r Sys.time()` '
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    number_sections: true
    toc_float: true
    collapsed: false
    smooth_scroll: false
editor_options: 
  chunk_output_type: console
---

```{r, eval=F, echo=F}
#update credentials to use token instead of old passowrd
gitcreds::gitcreds_set() # select '2' and enter new token

```


# Overview 

**Purpose**:   

* This script documents the quality control (QC) procedures taken to ensure data quality after the ACT TRAP mobile monitoring campaign. This code is on [GitHub](github.com/magali17/OurCampaign.git).
 

**QC Steps** 

* downloaded data sources:    
  * mobile monitoring data for all pollutants (9): CO, CO2, BC, Nephelometers, NO2, UFPs (4 measures)       
  * DOE minute (non-validated) and hourly (validated) data for various pollutants: BC, NO2, Neph, PM2.5, CO
  
* gas calibrations   
  * instruments readings were adjusted after a gas calibration until the next calibration was done (i.e., calibrations were for specific time periods)   
  * all calibrations used the reference concentration as the independent variable, and "calibrated" measurements were estimated after solving for this covariate      
  * calibrations were based on the median instrument reading of a reference concentration (i.e., a few minutes worth of data) using steady instrument readings (e.g., after applying time delays) 
  * nothing was done for these insruments, which were reset after calibration:    
    * CO, Nephs  
  * normal calibrations were done for   
    * CO2, NO2_1 
  * NO2_2 instrument was calibrated using a no intercept model since instruments were resent after zero readings and thus, these should have been failry close to the true concentration    

* zero concentrations   
  * checked that particle instruments reported near zero readings during zero air checks   
  * checked that these zeros were not drifting (i.e., becoming less accurate) over time 

* checked that aethalometer attenuation as < 50% at all times. This is when the instrument is most sensitive to detecting concentration changes.  

* dropped instrument readings with bad status codes:   
  * MA200: flow error or optical saturation codes     
  * NO2: automatic in-field, baseline measurements   

* calculated site concentration medians. All subsequent analyses were done at the stop-level. 

* dropped untrustworthy site medians:   
  * readings outside the range of each instrument's reporting range   
  * NanoScans and non-screened P-TRAKS readings < 300 pt/cm3 
  * screened P-TRAKS readings < 100 pt/cm3
  
* inspected time series data during times when extreme instrument concentrations (min & max conc) were observed. Readings were compared against other instrument readings (i.e., were other instruments also reporting elevated levels?)   
  * __[delete note here since this instrument is fully dropped later?]__ dropped one instance of CO2_19 (backup instrument) readings when readings were dissimilar from other instruments   

* compared & calibrated collocated duplicate instruments 
  * since the two screened P-TRAKS that collected most of the data were not collocated, we plotted density curves to show that their distribtuions were similar 
  * we calibrated some duplicate instruments a second time:    
    * instruments were not recalibrated to eachother if:      
      * there was a primary instrument used in the campaign the majority of the time and backup instruments produce similar readings during collocations    
        * unscreened P-TRAKs   
        * nephs   
        * BC
      * other reasons for not recalibrating instruments:   
        * the screened PTRAKs were not calibrated since only two backup instruments were collocated (3 and 94), and both were in good agreement    
        * CO instruments since none were in agreement with eachother (unclear which is the better/approariate reading)       
        * CO2 since the backup instrument was dropped         
    * if there was no primary instrument, however, readings were calibrated to the average of both instruments based on their collocations    
      * discminis   
    * calibrate backup instruments when they deviate from the primary instrument using recent collocation data
      * calibrate NanoScan (ns) 3 to ns 5 using data collected before __2019-04-01__ since 3 tends to read low and ns 3 values after this were not used anyways    
      * calibrate NO2_1 to NO2_2 using data collected before __2019-07-01__.     
        * Note that some NO2_1 values become negative    
    
* calibrated neph readings to estimate PM2.5 using Copper's calibration curve

* checked that there were no instrument trends occurring over the course of a samplign day (e.g., increasing instrument readings imply as a funciton of time)   

* AQS data comparisons   
  * checked that Copper's calibration curve was similar to the one fit using DOE data for 2016. Estimates would thus be similar regardless of what curve was used.
  
  * see AQS section for details on the discrepancies between these two curves 
  
  * compared 2-min mobile monitoring estimates to 2-min AQS site estimates 

* dropped all CO data because it they did not correlate well across duplicate instruments or with DOE data.

**Observations** 

* data sources
  * DOE seemed to be genrally complete (e.g., ~60 minute readings/hr). Minute data were associated with hourly readings, though they were more variable.    
  * Most pollutants used a "primary" instrument to measure pollutants. An exception waws the discminis and the screened ptraks.        
* Pollutants were measured for similar durations of time throughout the study period (~264 days avg)

* gas calibrations    
  * gas calibrations were calculated using lags - the data collected between t0+5 min and t1-1min, where t0=reference conc start time, and t1= reference conc end time.
  * CO2 and NO2 instruments were genrally in good agreement with reference concentrations. CO instruments showed variable performance. 

* zero concentrations       
  * particle instruments generally reported readings near 0 during zero checks, though there was some variability. The NanoScans tended to report readings in the hundreds of pt/cm3.         

* aethalometer attenuation    
  * all aethalometer readings were < 50% 

* extreme instrument readings   
  * most readings looked fine - similar concentration patterns were observed across multiple instruments   
  * CO2_19, however, tended to report high readings that were different than other instruments    
  * BC and CO tended to report low concentration readings  
  
* instrument collocations   
  * most looked good, except for CO2_19 and CO instruments  
    * CO2_19 (backup instrument) reported some readings much higher than CO2_14
    * CO instruments were in poor agreement with eachother 
  * NO2_1  (backup instrument) tended to read higher than NO2_2
  * PMSCAN_3  (backup instrument) tended to read lower than PMSCAN_5 
  
* there were no large trends in instrument readings as a function of how long the instrument had been collecting data for  

* dropped duplicate pollutant readings from duplicate instruments being placed on the platform. If multiple instruments were on the platform...   
  * and a primary instrument was used, used readings from the primary instrument
  * and both instruments were backups, used:
    * PTRAK 94 (instead if PTRAK 3), since this was used unscreend & has good agreement with other instruments
    * CO_190134 (instead of CO_3), since it had better agrement (R2) with CO_1 
 
* AQS data comparisons   

    * Copper's calibration curve was similar to the one fit using DOE data for 2016   

  * 2-min mobile monitoring and DOE estimates were similar for BC, Neph bscat, NO2 and PM2.5. CO estimates had poor correlation. 
  
  * mobile monitoring estimates of site annual averages were decent (?) for BC and NO2. There was limited variation in nephelometer readings, making it difficult to assess correlation.  Mobile monitoring estimates of PM2.5 were generally lower than true concentrations. Mobile monitoring estimates of CO were much higher than true concentrations. 

* not including CO, which as excluded from this analysis as a whole, about 9% of the stop data were dropped based on the QC procedures outlined above. 

# Upload data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      #fig.height = 5, fig.width = 8
                      fig.height = 6, fig.width = 10
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(knitr, kableExtra, 
               ggpubr, tidyverse,
               ggrepel, #geom_label_repel
               ggpmisc, #stat_poly_eq() # adds lm text to ggplots
               #mapping...adding scales, N arrows
               ggmap, sf, ggspatial, 
               units, #convert between e.g., m to km
               #time series data 
               lubridate,
               # modeling
               broom ##tidy()
               )    
 

set.seed(1)

```



common variables & functions

```{r}
#functions
source("0_Functions.R")
source("format_db_flags.R")

mytz <- "America/Los_Angeles"

start_date <- "2019-02-22"
end_date <- "2020-03-17" #or: "2020-03-18" to capture all of 3/17 ?

max_stop_duration <- 180 #3 min

pt_pollutants <- c("PM", "BC")

#air pollutants of immediate interest
ap <- c(
  #CO
  "co_ppm",
  #CO2
  "co2_umol_mol",
  #aethalometer
  "ma200_ir_bc1",
  #neph
  "neph_bscat", #"neph_ccoef",
  #no2
  "no2",
  #nanoscan
  "ns_total_conc",
  # discmini
  'pmdisc_number',  
  #ptraks
  "pnc_noscreen", "pnc_screen"
  )

unique_routes <- paste0("R0", c(1:9))

#drop instruments w/ few readings (only want a primary and backup instrument if possbile)
drop_instruments <-  c(paste0("PMPT_", c(1,2,3, 4)),
                               paste0("PMPTSCREEN_", c(1,4)),
                       "PMSCAN_2",
                       "PMDISC_7" #, "BC_0263"
                       
)

gas_calibrated <- c("co_ppm", "co2_umol_mol", "neph_bscat", "no2")

```


## helper tables

 

```{r}
#imports, fieldnotes, and location tables
load(file.path("Data", "Original", "imports_table.rda"))
load(file.path("Data", "Original", "fieldnotes_table.rda"))
load(file.path("Data", "Original", "locations_table.rda"))
 
```
 

```{r}
#will mege these later after adding a lag period to tables
calibrations_table0 <- readRDS(file.path("Data", "Original", "calibrations_table.rda"))  
         
calibration_data <- readRDS(file.path("Data", "Original", "calibration_data.rda")) %>%
  rename(time = timeint)  

#2 min medians (for collocation comparisons later)
calibration_data_2min <- calibration_data %>%
  select(site, instrument_id, analyte, level, conc, unit, duration, runname, time, #variable, 
         value) %>%
  #select(-contains("flags"), -status) %>%
  mutate(time = floor_date(time, "2 mins")) %>%
  group_by_at(vars(-value)) %>%
  summarize(value = median(value)) %>%
  ungroup()
  
  
```


## stop data
 

```{r}
#all stop data
dt0 <- readRDS(file.path("Data", "Original", "stop_data.rda")) %>%
  rename(time = timeint) %>%
  mutate(date = ymd(substr(time, 1, 10)),
         # this must have the wrong label - see original data
         instrument_id = ifelse(instrument_id == "BC_0263", "BC_0063", instrument_id)
         ) %>%
  filter(
    #drop instruments w/ few readings (only want a primary and backup instrument)
    !instrument_id %in% drop_instruments,
  )  

#ID primary instruments
primary_instruments <- dt0 %>%
  group_by(variable, instrument_id) %>%
  summarize(
    n = n()
  ) %>% 
  group_by(variable) %>%
  filter(n == max(n)) %>%
  ungroup() %>%
  pull(instrument_id)
 
dt0 <- dt0 %>%
  mutate(primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup" ),
         primary_instrument = factor(primary_instrument, levels = c("Primary", "Backup"))
         )


# DUPLICATE ROWS??
# There ~ 15k rows w/ duplicate information other than the value column. Looks like co_14 (the primary CO2 instrument) was primarily responsible for this - it reported duplicate readings at any given time. Duplicate readings appear to be very similar. Was this related to an instrument logging issues?? We'll average over these
  
dt0 <- dt0 %>% 
  #take avg of duplicate CO2_14 rows (same time stamp w/ slightly different values)
  group_by_at(vars(-value)) %>% 
  summarize(value = mean(value)) %>% 
  ungroup()

  
```

```{r}
#types of instruments
instruments <- unique(dt0$instrument_id) %>%
  str_extract(., "[^_]+") %>%
  unique() %>%
  sort()

```

MA200 data for QC

```{r}
ma200 <- readRDS(file.path("Data", "Original", "ma200_data.rda")) %>%
  rename(time = timeint) %>%
  mutate(date = ymd(substr(time, 1, 10))) %>%
  distinct()

```

 

```{r}
### --> DELETE??

# ## Additional room collocation data 
# 
# * calculate 2-min medians so instrument time stamps always line up

 
# room_colo <- readRDS(file.path("Data", "Original", "room_colo.rda")) %>%
#   ungroup() %>%
#   filter(
#     # only keep instrument readings we're interested in
#     variable %in% ap,
#     # don't look at these instruments
#     !instrument_id %in% c(
#       #we did not conduct MM with this instrment 
#       "PMSCAN_2",
#       #these were not used in the campaign as non-screened
#       paste0("PMPT_", c(2:4))
#       ),
#     # none of the screened ptraks were collocated. #c(2,94))
#     !grepl("PMPTSCREEN", instrument_id),
#     
#     # paper field notebook says that PMPT_94 had a wick issue around this time. the colloation data also shows that its readings were higher than PMPT93 for 6 readings (readings occurred every 2 sec)
#     !(instrument_id == "PMPT_94" & time >= "2019-04-18 11:48:00" & time < "2019-04-18 12:12:00"),
#     
#         ) %>%
#   #calculate 2-min medians
#   mutate(time = floor_date(time, "2 mins")) %>% 
#   group_by_at(vars(-value)) %>%
#   summarize(value = median(value)) %>%
#   ungroup()

```


## DOE 

* **note** DOE data is fixed so that it is read as PST (correct) and then converted to LA time (includes daylight savings) like the rest of our mobile monitoring data. If Dave S fixes this in the dataset, this initial setting should be removed/adjusted.

```{r}
doe0 <- readRDS(file.path("Data", "Original", "doe_data.rda"))

unique(doe0$site)

drop_doe_vars <- c(
  # # have no2 at the same sites 
  "doe_trace_noy", "doe_trace_no",     "doe_trace_noy-no", "doe_nox", "doe_no"
  )

doe <- doe0 %>%
  rename(location_doe=site,
         time = timeint
         ) %>%
  filter(
    #don't need these variables
    !variable %in% drop_doe_vars,) %>%
  mutate(
    
    date = ymd(substr(time, 1, 10)),
    #crosswalk for our location calib_instrument_ids
    location = recode_factor(factor(location_doe),
                             #10th & Weller
                             "AQS10W" = "MC0120",
                             #Beacon Hill
                             "AQSBH" = "MC0003",
                             #Kent-Central & James
                             "AQSK" = "MC0406",
                             #Duwamish
                             "AQSD" = "MC0126",
                             #Tukwila Allentown
                             "AQSTUK" = "MC0002"
                             ),
    #sampling frequency
    freq = recode_factor(factor(freq),
                          "DOE_H" = "hour",
                          "DOE_M" = "minute",
                          ),
    freq = factor(freq, levels = c("minute", "hour")),
    
    pollutant = ifelse(grepl("_no", variable), "NO2", 
                            ifelse(grepl("pm25", variable), "PM2.5",
                                   ifelse(grepl("neph", variable), "Neph",
                                          ifelse(grepl("_bc_", variable), "BC", 
                                                 ifelse(grepl("trace_co", variable), "CO", 
                                                 NA)
                                          )
                            )
                            )
                       ),
    pollutant = factor(pollutant, levels = c("BC", "NO2", "Neph", "PM2.5", "CO"))
    
  )   

```

* note, hourly and daily readings should be "validated" by the DOE since the final files were requested > 3 months after the data were collected (how long it takes DOE to validate data.)

```{r}

doe %>%
  # make code run faster
  group_by(location_doe, freq, variable, pollutant) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  mutate(location_doe = substr(location_doe, 4, nchar(location_doe))) %>%
  
  ggplot(aes(y=variable, x=location_doe)) + 
  facet_grid(pollutant~freq, scales="free", space = "free_y") + 
  geom_bin2d(aes(fill=count)) + 
  labs(title =  "Available regulatory data")

```

* check that minute and hour data are similar since only hourly data are validated by DOE

```{r}
#compare minute and hour data
df <- doe %>%
  select(-import_id) %>%
  #filter(freq %in% c("minute", "hour")) %>%
  #remove minute time stamp from minute data
  mutate(time = ymd_h(format(time, format = "%Y-%m-%d %H")))  
  
df1 <- df %>%
  filter(freq=="hour") %>%
  rename(hour_val = value) 

df2 <- df %>%
  filter(freq=="minute") %>%
  select(location_doe, time, variable, value) #%>% View()

df <- left_join(df1, df2) %>%
  select(-freq) %>%
  drop_na()
  
```

```{r}
df %>%
  mutate(diff = value - hour_val) %>%
  group_by(pollutant, variable) %>%
  summarize(
    min = min(diff),
    median = median(diff),
    max = max(diff)
  ) %>%
  kable(caption = "differnece between minute and hourly readings (minute-hour value)") %>%
  kable_styling()
  
  
```

* there is a lot more variability in the minute data compared to the hourly data. This could be real, or it could be noise.    
* [not shown in plot] smooth line is directly over the 1-1 line, indicating that minute data are in agreement with hourly data    

```{r}

df %>%
  #pollutants with true minute values
  filter(pollutant %in% c("BC", "NO2", "Neph", "CO")) %>%
  
  ggplot(aes(x=hour_val, y=value)) + 
  facet_wrap_equal(pollutant~location_doe, scales="free") + 
  geom_bin2d() +
  geom_abline(slope = 1, intercept = 0, linetype="dashed", aes(col="1-1")) + 
  #geom_smooth(aes(col="loess")) +
 
  labs(x="Validated hourly value",
       y = "Unvalidated minute value"
       ) 

```


observations    
* 10W and BH had PM2.5 from different sources over the course of the year 

```{r}
#check that there is sufficient data in the minute data 
doe %>%
  filter(freq == "minute") %>%
  distinct(location_doe, variable, pollutant, date) %>%
  filter(!variable %in% drop_doe_vars) %>%
  
  ggplot(aes(x=date, y=variable)) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) + 
  
  labs(title = "Available minute data during the study period")

  
```

* note, most are FEM methods (not nephs; also, not all bam methods are FEM - e.g., doe_bam_pm25?)    
* there is some overlap in instrument readings. This is probably done on purpose by the PSCAA in order to calibrate readings from different instruments. We will drop instrument reading as soon as another FEM method starts sampling.    
* BC readings at 10W are missing for ~4 months from around May-Aug 2019

```{r}
doe2 <- doe %>%
  filter(
    # hour/day estimates may be largely aggregates of minute data 
    freq == "minute",
    #these sites already have full neph data
    !(location_doe %in% c('AQSD', 'AQSK', 'AQSTUK') & variable %in% c('doe_bam_pm25', 'doe_pm25_fem', 'doe_pm25_fem_bam', 'doe_pm25_fem_teo') ),
    
    # a diff FEM method starts sampling at both 10W and BH at this time
    !(variable == "doe_pm25_fem" & date > ymd("2019-04-01") ),
    # a diff ?FEM method starts sampling at BH 
    !(variable == "doe_pm25_fem_teo" & date > ymd("2019-11-25") & location_doe == 'AQSBH'),
    
  ) 

```

* dropped overlapping data at 10W and BH: 

```{r}
#check that things look right # looks right

## plot
doe2 %>%
  distinct(location_doe, variable, pollutant, date) %>%
  
  ggplot(aes(x=date, y=variable
             #y=pollutant
             )) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) +
  
  labs(title = "Data used to estimate annual averages at AQS sites")

```

```{r}
doe2 <- doe2 %>%
  #rename each pollutant regardless of exact measurement instrument used
  group_by(location_doe, location, date, time, pollutant) %>%
  summarize(value = mean(value, na.rm = T)) %>% 
  ungroup()

```

observations per instrument 

* looks good. We have ~1 reading/minute at each site

```{r}
doe2 %>%
  group_by(pollutant, location_doe) %>%
  summarize(
    total_samples = n(),
    samples_per_hour = total_samples/as.numeric(difftime(max(date), min(date), units="hour"))
  ) %>%
  kable(caption = "Number of samples per site and pollutant used to estimate annual averages", digits = 0) %>%
  kable_styling()
  
```





# Instrumentation

* extreme instrument readings outside the range. Individual spikes may indicate poor instrument performance, but sustained spikes may be true though they may have reduced accuracy. Taking median stop concentrations and dropping median concentrations outside the range of the instrument readings should address these ideas?

  - from Tim G: "Not only the magnitude of the reading matters, but also the duration of the extreme value readings.  Instrument readings that are outside the manufacturers' rated range are best omitted from the data set, especially if they are one-time spikes.   A sustained extreme reading suggests that a high level of the pollutant has been detected but the quantity may not be very reliable.  You might consider replacing the numeric levels that exceed the manufacturers' rated maximum with that maximum number for sustained exceedances, on the premise that the device isn't made to measure higher than the stated level."

* The legacy NO2 CAPS monitor used at the start of the study does not have automatic baseline adjustment so would be reset on a weekly basis instead of occurring a few times during the monitoring day as with the newer fast-response unit.

* There were no clock issues with either NanoScan.  The clock issues were primarily with the P-Traks, but the resetting of internal clock before every drive kept the clock drift to a minimum.  After ~6 hrs, the clock for the instrument might be off by 2 to 3 sec. 

* Tim L: "The Nanoscan measures optical diameter after alcohol condensation to grow the particles, whereas the disk-mini measures electrical mobility diameter based on the current produced when the particles are captured on a oppositely charged surface. The electrostatic method has lower detection efficiencies for these very small particles than does the condensation/optical method."

```{r}
instrument_range <- data.frame(variable = character(), Min = numeric(), Max = numeric(), Units = character()) %>% 
  add_row(variable ="pnc_screen", Min = 0, Max = 5e5, Units = "pt/cm3") %>%
  add_row(variable ="co2_umol_mol", Min = 0, Max = 5e3, Units = "ppm") %>%
  add_row(variable ="co_ppm", Min = 0, Max = 200, Units = "ppm") %>%
  add_row(variable ="no2", Min = 0, Max = 2000, Units = "ppb") %>%
  add_row(variable ="pmdisc_number", Min = 1e3, Max = 1e6, Units = "ppb") %>%
  add_row(variable ="ma200_ir_bc1", Min = 0, Max = 1e6, Units = "ng/m3") %>%
  add_row(variable ="neph_bscat", Min = 0, Max = NA, Units = "bscat/m") %>%
  add_row(variable ="ns_total_conc", Min = 1e2, Max = 1e6, Units = "bscat/m") %>%
  add_row(variable ="pnc_noscreen", Min = 0, Max = 5e5, Units = "bscat/m") 


```

Expected instrument values

```{r}
instrument_range %>%
  kable(caption = "Instrumet measurement ranges. Note, nephs don't have a max.") %>%
  kable_styling()

```

data availability plot

```{r}
#plot 
dt0 %>%
  distinct(date, variable, instrument_id) %>%  
  ggplot(aes(x=date, y=instrument_id, col=variable)) + 
  geom_point()

```

```{r}
#tables
dt0 %>%
  distinct(date, variable, #instrument_id
           ) %>%
  group_by(variable, #instrument_id
           ) %>%
  summarize(
      sampling_days = n(),
      start_date = min(date),
      end_date = max(date)
    ) %>%
  ungroup() %>%
  add_row(variable="OVERALL MEAN", sampling_days=mean(.$sampling_days)) %>%
  kable(caption = "Total instrument sampling", 
        digits = 0
        ) %>%
  kable_styling()

```

same as above, by instrument_id

```{r}
dt0 %>%
  distinct(date, variable, instrument_id
           ) %>%
  group_by(variable, instrument_id
           ) %>%
  summarize(
      sampling_days = n(),
      start_date = min(date),
      end_date = max(date)
    ) %>%
  ungroup() %>%
  kable(caption = "Total instrument sampling", 
        digits = 0
        ) %>%
  kable_styling()

```


# Quality Assurance 


## Status codes

### Aethalometers 

* older sources that are slightly different 
  * the [mm_trap_qaqc repository](https://github.com/kaufman-lab/mm_trap_qaqc/blob/master/format_db_flags.R)   
  * see Amanda's draft of [QAQC.Rmd](https://github.com/kaufman-lab/mm_trap_qaqc/blob/master/QAQC.Rmd) documentation. This is also under trap\QAQC\qaqc_report_code\QAQC.Rmd. 

* will drop the following flags

```{r}
drop_bc_flags <- c("unstable_flow", 
                   #Also drop these? There are no observations  with these in this dataset either way
                   "flow_range", "optical_sat" 
                   )
```


```{r}
# see documentation on server: trap\QAQC\qaqc_report_code\QAQC.Rmd

#The way the error code works for the MA200 is based on using a base-2 number string, which includes a bunch of binary indicators (so something like 011010), which appears as base 10 in the database (something like 512).  Some of these numbers will represent combinations of multiple codes

library(data.table)

bc_errors <- dt0 %>% 
  filter(grepl("ma200", variable)) %>% 
  setDT() %>% 
  ma200.status.all() %>%
  as.tibble()

detach(package:data.table)

# table summarizing flags found
bc_errors %>%
  select(instrument_id, time, ma200.flags) %>%
  gather(error, value, ma200.flags) %>%
  group_by(error) %>%
  summarize(
    N_total = n(),
    N_with_flags = sum(value==1),
    Prop = mean(value),
  ) %>%
  kable(caption = "MA200 status flags. N = number of 10-sec measurements.",
        digits = 4) %>%
  kable_styling()


# status 640. could drop this, but not sure if this can occurr elsewhere as well.
drop_bc <- bc_errors %>%
  select(instrument_id, time, ma200.flags) %>%
  gather(error, value, ma200.flags) %>%
  filter(error %in% drop_bc_flags & value==1
         ) %>%
  select(instrument_id, time)
  
```

### NO2

* drop NO2 in field, baseline adjustments

```{r}
#these are the only 'measurement' (vs zero) flgs that are good
keep_no2_flags <- c("10004", "10104" ) 

dt0 %>%
  filter(variable == "no2") %>% 
  group_by(instrument_id) %>%
  mutate(n_total = n()) %>%
  mutate(Reading = ifelse(status %in% keep_no2_flags, "measurement", "zero")) %>%
  group_by(Instrument = instrument_id, Reading) %>%
  summarize(
    N = n(),
    'Instrument Proportion' = N/unique(n_total)
  ) %>%
  kable(caption = "proportion of NO2 data with usable readings (i.e., true measurements vs zero readings)",
        digits = 3
        ) %>%
  kable_styling()


```


### Update Data

**Drop bad status codes**

* bad flags that will be dropped

```{r}
paste("Bad BC flags:", paste(drop_bc_flags, collapse = ", "))

paste("Good NO2 flags indicating true measurements (vs e.g., baseline readings):", paste(keep_no2_flags, collapse = ", "))

```


```{r}
dt <- dt0 %>%
  filter(
    #drop NO2_2 field zeros 
    !(variable == "no2" & !status %in% keep_no2_flags),
  ) %>%
  # drop few MA200 readings w/ errors
  anti_join(drop_bc)


```


## Aethalometer Filter attenuation 

* checking the filter attenouation (uv_atn1 is what triggers the tape). If the attenuation is > 50%, the filter is less sensitive and could account for the zero readings. If this is true, we would want to drop these data. 

* everything looks fine. Most of the UV wavelength readings are far from 50% attenuation.

```{r}
# table
ma200 %>% 
  filter(variable == "ma200_uv_atn1") %>% 
  group_by(instrument_id) %>%
  summarize(
    N = n(),
    Min = min(value),
    Q25 = quantile(value, 0.25),
    Median = median(value),
    Q75 = quantile(value, 0.75),
    Max = max(value)
  ) %>%
  kable(caption = "Aethalometer UV attenuation readings. N = number of 10-sec readings.") %>%
  kable_styling()
  
```


* there doesn't seem to be an association between low BC readings and attenuation (UV is what makes the tape advance). 

### --> instruments read slightly higher as attenuation increases? is this is the opposite of what we would expect?

```{r}
ma200 %>%
  select(time, instrument_id, variable, value) %>%
  spread(variable, value) %>%
  gather("variable", "value", contains("atn1")) %>% 
  ggplot(aes(y=ma200_ir_bc1, x=value, col=variable)) + 
  facet_grid(variable~instrument_id) +
  geom_point(alpha=0.2) +
  geom_vline(xintercept = 50, linetype=2, alpha=0.5) +
  geom_hline(yintercept = 0, linetype=2, alpha=0.5) +
  geom_smooth(se=F, col="black") + 
  labs(
    title = "BC readings vs filter attenuation",
    y = "BC (ng/m3)",
    x = "Attenuation (%)"
  )


```


## Gas Calibrations

* The "level" column is what the instrument read, while the conc column is the true concentration during a calibration procedure. 
* pm25 is nephelometer readings 

* **NOTE**: don't use the "analyte" column, which is wrong for some instruments (Amanda confirmed this). Use instrument_id column.

* CO: Tim G: tried resetting the instrument but had issues…so team just noted the response to the span gas. Both langans didn’t respond well. 

* see a lag that occurs in instrument response to a true reference concentration when looking at time series plots. need to add a lag to start and end of each refernce reading.

* nephs were reset based on the zero and span gas inputs if the readings were found to be out of spec (defined as more than +-0.05 e-5 away from the reference B-scat values). These were based on manual entries (not in the database as of 6/11/21). These manually recorded  calibration values could be just as valuable as the other calibration event results (logged and in the database on a 10-sec resolution).    
  * **Whether or not you may choose to calibrate earlier readings may depend on how much the nephs drifted away from the "true" concentration since the previous time it was calibrated and rest.** "As found" neph readings are pre-calibration and could be used to adjust neph readings during earlier times; whie "as set" neph readings are post-calibration after the instrument has been resest such that we trust that the instrument is within spec (+-0.05e-5).    
  * Amanda and I decided not to recalibrate neph readings backwards since these calibration data were collected in a different fashion as the rest of the data in the database.  We are also not doing this with other instruments. Amanda will summarize these calibration data in the QC documenet she is working on.

```{r}
# time series 
instruments_calibrated <- unique(calibration_data$instrument_id)

# gas instruments & neph
calib_instrument_ids <- calibration_data %>%
  filter(grepl("co_|co2|no2", variable, ignore.case = T)) %>%
  distinct(instrument_id) %>%
  pull() %>% sort()

#particle instruments with just zero readings
pt_instrument_ids <- setdiff(instruments_calibrated, calib_instrument_ids)

```

* **all readings, before adding a lag to when instruments start reading**


* for the nanosscans, the initial reading after a filter is placed on the inlet will be omitted from the data set and consider the data after a minute or so to ensure the sampling line has been cleaned of particles from the previous interval

```{r, fig.height=8}
# no lag
start_lag <- 0
end_lag <- 0

calibrations <- calibrations_table0 %>%
    #one time gets dropped. there is an error/instrument restart note.
    filter(starttime < endtime) %>%
  
  #is this causing merging issues, import id's are different for 2 diff types of files
  select(-c(import_id, )) %>%
  distinct() %>%
  
  #make calibration table long format
    rowwise() %>%
    mutate(time = list(seq(starttime, endtime, 1))) %>%
    ungroup() %>%
    select(-starttime, -endtime) %>%
    unnest(cols=c(time)) %>%
    arrange(time) %>% #View()
    
#join actual instrument readings
  left_join(calibration_data) %>%
  drop_na(value)


```

```{r, eval=F}
#no need to show this - takes up a lot of space

#gases
for(i in seq_along(calib_instrument_ids)) {
  #i=6
  #if there are data for this instrument..
  obs <- calibrations %>%
    filter(instrument_id == calib_instrument_ids[i]
           #instrument_id == instruments_calibrated[i]
           ) %>%
    nrow()
  
  if(obs > 0) {
    p <- calibrations %>%
      filter(instrument_id == calib_instrument_ids[i]
             #instrument_id == instruments_calibrated[i]
             ) %>%
      gather(kind, value, conc, value) %>%
      mutate(
        kind = recode_factor(factor(kind),
                             "conc" = "Reference",
                             "value" = "Instrument"
                             )
        ) %>%
      
      ggplot(aes(x=time, y=value, col=kind,  
                 )) + 
      facet_wrap(~runname+instrument_id, scales="free") + 
      geom_point(alpha=0.1) +
      
      guides(colour = guide_legend(override.aes = list(alpha = 1))) +
      
      labs(title = "calibration readings"
           )
    
    print(p)
  }
  }

```

* **after adding a lag** 

  * Note that this drops  a lot of data but the instruments took a while to stabilize. Questions about their reliability for 2-min stops...? 
  
* also dropped additional initial readings for NO2_1 that took a while to stabilize 

* We still see a very unstable reading for NO2_2 on 2019-04-15 ~ 13:00. There are no notes explaining this in the calibration notebook. It looks like the refernce concentration jump from 0 ppb to 80 ppb may have been too large for the NO2_2 instrument, particularly if it was not warmed up for long prior to doing so. Note, this was also the first in lab NO2 calibratiobn for our campaign, so we may have still been learning how to properly calibrate the instrument. The instrument starts to stabilize ~ 45 min into the 80 ppb session, but then falls again. Readings are more in line with other reference concentrations afterwards. **We will drop the 80 ppb reference concentration b/c the jump in concentration may have been too much for the instrument, particulalry if the instrument was not warmed up.** There's an argument to be made for also dropping the first 0 reading, but it shouldn't make a difference since the our calibration curves won't take that into account.

* dropped the edges of some of the particle instruments since some readings looked unstable

```{r, echo=T}
# add a lag - different for different instruments
gas_start_lag <- 300 #5 min
pt_start_lag <- 180 # 3 min
end_lag <- 60 # 1 min
## see spike in readings discminis near the end of each session - due to the filter being taken off?
end_lag_disc <- end_lag + 60 # 2 min

```

```{r, fig.height=10}

calibrations <- calibrations_table0 %>%
  #drop these data so not confused w/ no2 when merging
    #some instruments (esp NO2) have a lag response period. wait a few seconds before using the data
    mutate(starttime =  if_else(instrument_id %in% calib_instrument_ids, 
                             starttime + gas_start_lag,
                             starttime + pt_start_lag),
           endtime = if_else(grepl("PMDISC_", instrument_id),  endtime-end_lag_disc, endtime-end_lag)
           ) %>%  
    #one time gets dropped. there is an error/instrument restart note.
    filter(starttime < endtime) %>%
  #is this causing merging issues b/c they're different??
  select(-c(import_id)) %>%
  distinct() %>%  
  #make calibration table long format
    rowwise() %>%
    mutate(time = list(seq(starttime, endtime, 1))) %>%
    ungroup() %>%
    select(-starttime, -endtime) %>%
    unnest(cols=c(time)) %>%
    arrange(time) %>% 
  
  #join actual instrument readings
  left_join(calibration_data) %>%
  
  #make sure timezone is preserved
  mutate(time = ymd_hms(time, tz = mytz)) %>%
  filter(
    #drop these problematic/unstable No2_2 readings (see note above)
    !(instrument_id == "NO2_2" & runname == "2019-04-15_LABC" & conc == 80),
     
    # intrument takes long time to stabilize here
    #drop these problematic/unstable No2_2 readings (see note above)
    !(instrument_id == "NO2_2" & runname == "2019-04-15_LABC" & conc == 80),
    !(instrument_id == "NO2_2" & time > "2019-05-06 15:07:00" & time < "2019-05-06 15:15:00"),
    !(instrument_id == "NO2_2" & time > "2019-05-22 11:40:00" & time < "2019-05-22 12:00:00"),
    !(instrument_id == "NO2_2" & time > "2019-05-31 12:14:00" & time < "2019-05-31 12:17:00"),
    !(instrument_id == "NO2_2" & time > "2019-05-31 12:44:00" & time < "2019-05-31 12:50:00"),
    !(instrument_id == "NO2_2" & time > "2019-08-07 10:35:00" & time < "2019-08-07 10:46:00"),
    
    !(instrument_id == "NO2_1" & time > "2019-03-06 19:15:00" & time < "2019-03-06 19:24:00"),
    !(instrument_id == "NO2_1" & time > "2020-01-14 14:14:00" & time < "2020-01-14 14:20:00"),

    #neph filter was removed after this time
    !(instrument_id == "PM25_176" & grepl("2019-10-02", runname) & time >= ymd_hms("2019-10-02 14:12:00", tz = mytz)),
    
     # these filter on/off times are wrong in the databaes, from looking at the calibration notebook notes
    !(instrument_id == "PMPT_94" & time > "2019-10-02 14:04" & time < "2019-10-02 15:00" ),
    !(instrument_id == "PMSCAN_5" & time > "2019-10-02 14:04" & time < "2019-10-02 14:10:10"),
   
    # the filter on/off times seem at these times. will do some trimming
        ### --> also remove large 13:09 spike??
    !(instrument_id %in% c("PMPT_94", "PM25_176", "PMSCAN_5") & runname == "2020-01-13_LABC" & 
      (time < "2020-01-13 13:08:00" | time > "2020-01-13 13:13:00")),
      ## these few readigns look unstable and diff from previous readings - filter was not on long enough?
    !(instrument_id =="PMDISC_3" & runname == "2020-01-13_LABC" &  time > "2020-01-13 13:05:00"),
    !(instrument_id =="PMPTSCREEN_93" & runname == "2020-01-13_LABC" &  time > "2020-01-13 13:07:00"),
    !(instrument_id =="PMPTSCREEN_93" & runname == "2019-11-21_LABC" &  time > "2019-11-21 11:16:00")
    ) %>%   
  drop_na(value)

```

 

```{r}
#gases 

for(i in seq_along(calib_instrument_ids)) {
  #i=6  
  
  #if there are data for this instrument..
  obs <- calibrations %>%
    filter(instrument_id == calib_instrument_ids[i]) %>%
    nrow()
  
  if(obs > 0) {
    p <- calibrations %>%
      filter(instrument_id == calib_instrument_ids[i],
             ) %>%
      gather(kind, value, conc, value) %>%
      mutate(
        kind = recode_factor(factor(kind),
                             "conc" = "Reference",
                             "value" = "Instrument"
                             )
        ) %>%
      
      ggplot(aes(x=time, y=value, col=kind,  
                 )) + 
      facet_wrap(~instrument_id + runname, scales="free") + 
      geom_point(alpha=0.1) +
      
      guides(colour = guide_legend(override.aes = list(alpha = 1))) +
    
    labs(title = "calibration readings"
         )
    
    print(p)
  }
  }


```

* scatterplots of date-specific instrument responses to reference gases 

* we will calculate median instrument responses for any given refernce concentration (there are multiple reference concentrations per calibration date) to use stable instrument responses in the calibration curves. Medians are typically calculated from a few minutes of instrument response data (similar to the stop data).  

* note: some neph data missing after 2019-10-22 (the calibration table thus doesn't have all the entries for PM25_205) because it was documented manually (i.e., one stable reading was recorded for any given refefence concentration) rather than having the instrument produce an output file of readings. It  was thus not included in the database.

```{r}
calibration_medians <- calibrations %>%
  mutate(date = factor(substring(runname, 1, 10))) %>%
  #calculate median instrument response when steady instrument values are considered
  group_by(variable, instrument_id, runname, date, conc, duration) %>%
  summarize(value = median(value)) %>%
  #only include times w/ at least 2 refence concentrations
  group_by(variable, instrument_id, runname, date) %>%
  filter(n() >= 2) %>%  
  ungroup() 
  
```

* we still see a slope different from the rest for NO2_2 on 2019-04-15 after dropping one bad set of readings (see note above). The instrument underreported readings for a bit. Calibration adjustments will thus impact ~ 3wks worth of data until the next calibration was completed in 2019-05-06. 

* on 2019-09-25, NO2_1 consistenly undereported known reference concentrations (you see a deviation from the 1-1 line for this day in the plots below). This underreporting was consistent for all of the reference concentrations, including 0 ppb, where it reported concentrations of ~ -24 ppb. This drastic adjustment is thus likely appropriate and not a user-end error (e.g., could NO2 have been recorded in the calibration notebook even though the instrument was actually reporting NO?).

* note that in the plots below, CO2 was automatically reset after each calibration, so no adjustments are necesssary 

```{r, fig.height=10}

calibration_medians %>%
  #only include gas calibrated instruments
  filter(instrument_id %in% calib_instrument_ids) %>%
  
  ggplot(aes(x=conc, y=value, col=date, group=date)) + 
  facet_wrap_equal(~instrument_id, scales="free") +
  theme(aspect.ratio = 1,
        legend.position = "none"
        ) +
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  geom_line(stat="smooth", method = "lm", size = 0.5, alpha = 0.8) +
  geom_point(alpha=0.2) + 
  
   labs( 
     title = "Comparison of median instrument responses to reference concentrations",
     subtitle= "Each line is a calibration date",
     x = "Reference Conc",
     y = "Instrument Response Conc"
       )

```

* calibrate CO and NO2 gas instruments since these were not reset otherwise 
   * A no intercept model for new NO2_2 is probably best b/c NO2 is constantly resetting to 0...so the intercept should be correct.     
    * the database shows 0, then span gas, then another 0. 
* no need to do for CO2 instruments b/c these were reset after calibrations based on the gas level
* ? probably no need to do this for nephs b/c they were reset if values were different from what was expected based on using 99.99% CO2 gas (~2.14e-5 at Temp = 293K)
 
* the calibration curve will use our sensor as the dependent variable and the true concentration as the independent variable so that the error term is estimated for our sensor. The MOVUP team has seen differences when things are done the other way (how you may traditionally dot his). Thus, we want to model:

$$
Conc_{instrument} = Conc_{reference}*\beta_1 + \beta_0 + \epsilon
$$

  * We can then solve for true estimate. If it’s a linear model, we can do this algebraically. 
  * Since these are high-end instruments they “should” respond linearly on the concs ranges we’re dealing with, and so a simple linear regression should be OK.

$$
(Conc_{instrument} - \hat\beta_0)/\hat \beta_1  = \hat Conc_{reference} 
$$


* Fit calibration curves for time period until the next calibration. 
  * note, neph calibration data are not in the database b/c they were collected manually in a very different way than the rest of the data. Nephs were reset after a calibration anyways, so this should not be a problem unless we were concerend that nephs significantly drifted between calibrations. 
    * Amanda will be summarizing calibration results in the QC report she is working on.

```{r}
# a no-intercept model for NO2_2
# an intercept model for NO2_1 and all CO instruments

#NO2_2
no2_2_calib_curves <- calibration_medians %>%
  filter(instrument_id == "NO2_2") %>%
  group_by(variable, instrument_id, runname, date) %>%
  nest() %>%     
  mutate(
                                  #don't estimate intercept
    lm = map(data, ~lm(value~conc -1, data = .x)),
    tidied = map(lm, tidy)
  ) %>%
  unnest(tidied)


### --> add nephs if get more data 

no2_1_and_co_calib_curves <- calibration_medians %>%
  filter(instrument_id == "NO2_1" | 
           grepl("CO_", instrument_id)
         ) %>%  
  group_by(variable, instrument_id, runname, date) %>% 
  
  nest() %>%     
  mutate(
    lm = map(data, ~lm(value~conc, data = .x)),
    tidied = map(lm, tidy)
  ) %>%
  unnest(tidied)
   
calibration_curves <- rbind(no2_2_calib_curves, no2_1_and_co_calib_curves) %>%
  ungroup() %>%
  select(variable, instrument_id, runname, date, term, estimate) %>%  
  spread(term, estimate) %>%
  rename(intercept = "(Intercept)") %>%
  mutate(
    # no2_2 has intercept of 0, though it's not estimated in lm
    intercept = ifelse(is.na(intercept), 0, intercept),
    date = as.Date(date)     
         ) %>%
  # #only include times when 'conc' can be estimated b/c > 1 concentrations used
  drop_na(conc)

```

```{r}
# summarize calibation curves
calibration_curves %>%
  gather("term", "estimate", intercept, conc) %>%
  group_by(variable, instrument_id, term) %>%
  summarize(
    N = n(),
    Min = min(estimate),
    Median = median(estimate),
    Max = max(estimate)
  ) %>%
  #arrange(term, instrument_id) %>%
  kable(caption = "Distribution of calibration curve coefficient estimates for conc_instrument ~ conc_reference*B1 + B0. N = number of calibration days. 'conc' term is the slope. A no intercept model was fit to instrument NO2_2, which reset after each zero reading.", digits = 2) %>%
  kable_styling()
  
```


```{r}
# calibration function calibrates raw data ('value' variable) using the latest calibration curves for any given dataset (e.g., for each instrument_id if it is looped over)  

calib_fn <- function(dt, calibration_curves_df) {
  
  calibration_dys <- nrow(calibration_curves_df)
  
  # loop over every calibration date
  for(i in 1:calibration_dys) {
    #i=6
    
    #if there are subsequent calibration curves, only calibrate the data until the next calibration curve is fit
    if(i != calibration_dys) {
      
      dt <- dt %>%
        mutate(
          value = ifelse(date >= calibration_curves_df$date[i] & date < calibration_curves_df$date[i+1],
                         (value - calibration_curves_df$intercept[i])/calibration_curves_df$conc[i], 
                         value 
                         )
          )
      
      #if it's the last calibration curve, use it all data collected afterwards
    } else {
      dt <- dt %>%
        mutate(
          value = ifelse(date >= calibration_curves_df$date[i],
                         (value - calibration_curves_df$intercept[i])/calibration_curves_df$conc[i], 
                         value 
                         )
          )
      
    }
  }
  
  return(dt)
  
}

```


```{r}
calib_instruments2 <- unique(calibration_curves$instrument_id)

# calibrated data
dt0_calibrated <- data.frame()

#loop over the calibration function for each instrument_id
for (i in seq_along(calib_instruments2) ) {
  #i=2
  
  curves_temp <- calibration_curves %>%
    filter(instrument_id == calib_instruments2[i])
  
  dt_temp <- dt %>%
    filter(instrument_id == calib_instruments2[i]) %>%
    calib_fn(dt = ., calibration_curves_df = curves_temp)
  
  dt0_calibrated <- rbind(dt0_calibrated, dt_temp)
  
}


```

### Update data

```{r}
dt <- dt %>%
  rename(value_raw = value) %>%
  left_join(dt0_calibrated) %>%  
  #if no calibrations were done, use the uncalibrated values
  mutate(value = ifelse(is.na(value), value_raw, value))
   
```


## Zero checks

* **If instruments report readings near 0 and the noise is small (compared to a low background level?), we may not need to do anything to adjust particle readings** 


* this is mainly for particle instruments, but may include some gas instruments that were checked with zero air

* field notesdo not always clarify exact time when the particle filter was placed/removed from the instruments. You can typically see instrument readigns dip for a period of time before going back up though (e.g., 2019-10-22).   

* no notes on why particle readings may have been elevated on 2019-03-13 
* no notes on why PMDISC_8 on 2019-07-10 reported some very high readings    

observations      

* PMSCAN_5 tends to read high on some days


```{r}
#particles
for(i in seq_along(pt_instrument_ids)) {
  
  #i=12
    p <- calibrations %>%
      filter(instrument_id == pt_instrument_ids[i]) %>%
      gather(kind, value, conc, value) %>%
      mutate(
        kind = recode_factor(factor(kind),
                             "conc" = "Reference",
                             "value" = "Instrument"
                             )
        ) %>%
      
      ggplot(aes(x=time, y=value, col=kind,  )) + 
      facet_wrap(~runname+instrument_id, scales="free") + 
      geom_point(alpha=0.4) +
      
      guides(colour = guide_legend(override.aes = list(alpha = 1))) +
      
      labs(title = "calibration readings"
           )
    
    print(p)
}

```

**boxplots of 2-min medians from instrument readings during zero checks** These are comparable to 2-min stops.

* we ideally want "low stop conc" values for each instrument to be much higher than that instrument's "median overall conc" during zeroing. We see that for most instruments except:    
  * neph values (PM25_176 are close)      
  * BC_0063 where a low stop value is actually below the median zero concentration value. This coud be because    
    - eathalometers are very noisy   
    - is it possible that some places have BC readings near 0 ng/m3?   

  * **boxplots are not traditional**, the whiskers indicate the 2.5th and 97.5th quantiles (range for 95% of the data)
  * 'Low value' represents a typically low value for that instrument while on route, based on the 5th quantile of second-level data

```{r}

#what is a low reading?
low_q <- 0.05

low_value <- readRDS(file.path("Data", "Output", "20210622 before adding new data", "stops_distribution_table.rda")) %>%
  select(instrument_id, low = Q05)
 
```

* Tim's thoughts 
  * BC:    
    - makes low concentration readings less trustworthy since they are near the instrument's lower detection abilities (?)   
  * nepsh:     
    - would expect a narrower band in the middle of the distribution -- tighter range between 25th and 75th percentile.    
    - median value is somewhat high? Thought at least it is below a "low" ambient concentration
  * nanoscans   
    - the initial reading after a filter is placed on the inlet should be omitted from the data set and consider the data after a minute or so to ensure the sampling line has been cleaned of particles from the previous interval
    - The relatively small difference between the zero reading and your typical "low concentration" stop is what I find of most concern. This suggests that NS #3 would not be able to accurately characterize low concentrations since it is not that different from zero...Or these differences could be due to the fact that nanoscan 3 has a lot less data. 
    
* BC_0066 is only based on one instance

```{r}

calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|")))) %>%  
  #calculate 2-min medians 
  mutate(time = floor_date(time, unit = "2 min")) %>%
  group_by_at(vars(-value)) %>%
  summarize(value = median(value)) %>%  #group_by(instrument_id) %>% summarize(n())
  # stats for boxplots
  group_by(variable, instrument_id) %>%
  alt_boxplot(var = "value") %>%  
  left_join(low_value) %>%
  #add if primary instrument
  mutate(Primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup"),
         Primary_instrument = factor(Primary_instrument, levels = c("Primary", "Backup"))
         ) %>%
  
  ggplot(aes(x=instrument_id, linetype=Primary_instrument)) + 
  facet_wrap(~variable, scales="free") +
  geom_hline(yintercept = 0, linetype=2, alpha=0.5) +
  geom_boxplot(stat = "identity", aes(ymin=Qmin, lower=Q25, middle=Q50, upper=Q75, ymax=Qmax)) +
  
  # a low on-route value (for comparison)
  geom_point(aes(y = low, col=paste0("Low Stop Conc\nQ", low_q)), size=3) +
  labs(title="2-min median instrument readings during zero checks",
       y = "level",
       col=""
       )

```


* don't see temporal trends to suggest shifting instrument readings over time?

```{r}

calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|"))),
         #Discmini 8 has 1 instance whre conc is missing but a diff filter was used
         #conc==0
         ) %>% 
  #calculate 2-min medians 
  mutate(time = floor_date(time, unit = "2 min")) %>%
  group_by_at(vars(-value)) %>%
  summarize(value = median(value)) %>%  
  # stats for boxplots
  group_by(variable, instrument_id, runname) %>%
  alt_boxplot(var = "value") %>%
  left_join(low_value) %>% 
  #add if primary instrument
  mutate(Primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup"),
         Primary_instrument = factor(Primary_instrument, levels = c("Primary", "Backup")),
         date = substr(runname, 1,10) 
         ) %>%
  
  ggplot(aes(x=instrument_id, fill=date, linetype=Primary_instrument)) + 
  facet_wrap(~variable, scales="free") +
  geom_hline(yintercept = 0, linetype=2, alpha=0.5) +
  geom_boxplot(stat = "identity", aes(ymin=Qmin, lower=Q25, middle=Q50, upper=Q75, ymax=Qmax)) +
  scale_fill_brewer(palette = "Purples") +
  # a low on-route value (for comparison)
  geom_point(aes(y = low, col=paste0("Low Stop Conc\nQ", low_q)), size=3) +
  
  labs(title="2-min median instrument readings during zero checks",
       y = "level",
       col=""
       )

```

* by filter placement (on the manifold vs directly on the instrument inlet)   
  - there is no clear discrepancy in instrument response related to the filter placement. Good. 

```{r}

calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|")))) %>% 
  mutate(filter_to = ifelse(grepl("manifold", notes), "manifold", "instrument"),
         date = substr(runname, 1,10),
         Primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup"),
         Primary_instrument = factor(Primary_instrument, levels = c("Primary", "Backup")),
         #calculate 2-min medians 
         time = floor_date(time, unit = "2 min"),
         ) %>%
  group_by_at(vars(-value)) %>%
  summarize(value = median(value)) %>%  
  # stats for boxplots
  group_by(variable, instrument_id, #date, 
           filter_to, Primary_instrument) %>%
  alt_boxplot(var = "value") %>%
  left_join(low_value) %>% 
  
  ggplot(aes(x= instrument_id, fill=filter_to, linetype=Primary_instrument)) + 
  facet_wrap(~variable, scales="free") +
  geom_hline(yintercept = 0, linetype=2, alpha=0.5) +
  geom_boxplot(stat = "identity", aes(ymin=Qmin, lower=Q25, middle=Q50, upper=Q75, ymax=Qmax)) +

  scale_fill_brewer(palette = "Purples") +
  labs(title="2-min median instrument readings during zero checks",
       col=""
       )

```



* table shows that:  
  - most of the readings are genreally near 0   
  - median concentrations are low relative to a "low" ambient concentration ("Q50/low val"; where a low value is defined as quantile `r low_q` during a stop). This suggests that instruments are responding to the filter placement   
    * exceptions are BC (ambient BC readings near 0 could be real?) and nephs   
  - the IQR of instrument readings during zeroing is genrally small realtive to a low ambient concentration excetp for   
    nephs, BC and nanoscan 5 
    
```{r}
# table 
calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|"))),
                    #!str_detect(instrument_id, "PM25"),
         #conc==0
         ) %>% 
   #calculate 2-min median readings for each instrument
  mutate(time = floor_date(time, unit = "2 min")) %>%
  group_by_at(vars(-value)) %>%
  summarize(value = median(value)) %>%  
  ungroup() %>%
  mutate(date = substr(runname, 1, 10)) %>%
  left_join(low_value) %>%  
  group_by(instrument_id) %>%  
  summarize(
    N = n(),
    Days = length(unique(date)),
    Q02.5 = quantile(value, 0.025),
    Q25 = quantile(value, 0.25),
    Q50 = median(value),
    Q75 = quantile(value, 0.75),
    IQR = IQR(value),
    Q97.5 = quantile(value, 0.975),
    
    #compare to a "low value"
    low_ambient_conc = unique(low),
    'Q50/low_ambient_conc' = Q50/low_ambient_conc,
    'IQR/low_ambient_conc' = IQR/low_ambient_conc
    
    ) %>%
    kable(caption = paste0("Raw instrument readings during zeroing. Quantiles are the error for zero concentration (e.g., Q50 = median error). N = number of 2-min readings. Low ambient concentration values are esimated from stop data based on quantile ", low_q, "."), 
          digits = 2
          ) %>%
    kable_styling()
     
```


### --> If readings are not near 0 or the noise is large, ask Edmund Elena what we should do about specific instrument. adjust backwards since these are not adjusted like the gas instruments are otherwise? 

```{r}


```





# Summary of all raw data 
 
 

```{r}
print("total raw, second-level obsevations")

nrow(dt)

```

 
```{r,}
#table 

dt %>%
  #drop Roosevelt Garage
  #filter(location != "MS0000") %>%
  
  group_by(instrument_id) %>%
  summarize(
    no_readings = n(),
    Min = min(value),
    # q01 = quantile(value, 0.01),
    Q02.5 = quantile(value, 0.025),
    Q25 = quantile(value, 0.25),
    Q50 = quantile(value, 0.50),
    Q75 = quantile(value, 0.75),
    Q97.5 = quantile(value, 0.975),
    # q99 = quantile(value, 0.99),
    Max = max(value),
  ) %>%
  # scientific notation w/ few sigfigs 
  mutate_at( vars(-no_readings), ~format(., scientific=T, digits = 3) ) %>%
  
  kable(caption="raw instrument readings",
        ) %>%
  kable_styling()

```


# Calculate Site medians (for each instrument)


```{r}

dt %>%
  #filter(location != "MS0000") %>%
  distinct(runname, location, time, stop_id ) %>%
  group_by(runname, location, stop_id) %>%
  #only keep first 3 min of data collected. some stops were ~ 20-30 min long (e.g., driver breaks)
  filter(time- min(time) <= max_stop_duration) %>% 
  summarize(stop_duration = difftime(max(time), min(time), units = "mins")) %>%
  ungroup() %>%  
  summarize(
    N = n(),
    # N_above3min = sum(stop_duration>3),
    # Prop_above3min = N_above3min/N,
    Min = min(stop_duration),
    Q05 = quantile(stop_duration, 0.05),
    Median = median(stop_duration),
    Q95 = quantile(stop_duration, 0.95),
    Max = max(stop_duration)
  ) %>%
  kable(caption = "Stop duration when using first 3 minutes. N = number of stops.", digits = 2) %>%
  kable_styling()

```


```{r}
#note: on rare instantces when duplicate instruments were on board, all the data are used to calculate a median

dt_stops <- dt %>%
  #filter(location != "MS0000") %>%
  #calculate arrival time at any stop. also ensure that duplicate instruments are aligned correctly
  group_by(runname, date, location, stop_id) %>%
  #only keep first 3 min of data at each stop
  filter(time- min(time) <= max_stop_duration) %>%
  mutate(time = min(time)) %>%  
   group_by(runname, date, time, location, stop_id, instrument_id, variable, primary_instrument) %>%
  summarize(
    value = median(value),
    value_raw = median(value_raw)
  ) %>%
  ungroup()

```

 
summary of stop-level data 

```{r, fig.height=12, eval=F}
#* labels are for the min/max reading of each instrument 
 
 
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  df <- dt_stops %>%
            ##drop Roosevelt Garage
    filter(#location != "MS0000",
           variable== ap[i])  
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    guides(linetype = FALSE)
   
    # don't do since we have values <=0 
    #scale_x_log10()
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 5)


```


amount of data outside of expected instrument ranges
   
* out of range date is more likely to be below the minimum instrument range. Removing all values outside the instrument range could thus bias overall median slightly up (this was more true when we ran this at the raw second level, where ~16% of BC data was below the min)


```{r}
dt_stops %>%
  #observations per pollutant
  group_by(variable, #instrument_id
           ) %>%
  mutate(n_original = n()) %>%
  ungroup() %>%
  
  left_join(instrument_range) %>%  
  mutate(below_min = value < Min,
         above_max = value > Max
         ) %>%
  
  group_by(variable, #instrument_id
           ) %>%
  summarize(
    n_original = unique(n_original),
    n_below_min = sum(below_min),
    n_above_max = sum(above_max),
    
    prop_below_min = mean(below_min),
    prop_above_max = mean(above_max),
    total_prop_out_of_range = sum(prop_below_min, prop_above_max, na.rm = T)
  ) %>%
  kable(caption = "proportion of data outside the instrument range. N = number of stops (~309 stops x ~29 visits/stop)", digits = 2) %>%
  kable_styling()


```


## Update data

* **drop median estimates outside the range of each instrument's reporting range**    
* drop NanoScan and non-screened PTRAK values < 300 pt/cm3   
* drop screened PTRAK values < 100 pt/cm3.    
* **note that there are few readings being dropped here, and that these additional cut offs are much lower than the ambient levels we normally see.**

```{r}
min_tot_ufp <- 300
min_screen_ufp <- 100

dt_stops2 <- dt_stops %>%
    left_join(instrument_range) %>% 
    #neph has no max. need a number to avoid an error
    mutate(Max = ifelse(is.na(Max), 9999999, Max)) %>%  
    #drop values outside the expected instrument range
  filter(value >= Min, value <= Max)  %>%
  #drops 3 UFP values < 300
  filter(
    !(variable %in% c("ns_total_conc", "pnc_noscreen") & value < min_tot_ufp ),
    !(variable %in% c("pnc_screen") & value < min_screen_ufp ),
    )
  

### --> need this dataset later? 

#keep approved stop data, but see raw values
dt2 <- dt_stops2 %>%
  distinct(date, location, instrument_id) %>%
  left_join(dt)

```

```{r}
# stop estimates when values outside the range of data are dropped
# 
# * CO still has extreme values. So do some of the UFP instruments.

```



# Time series of when the lowest and highest values were observed

* as an example, we're just looking at the lowest and highest readings here

* pulling out the second data used to calculate site medians to better inspect the instrument patterns for possible errors

```{r}

#create standardized values & relabel UFP instruments
dt2_temp <- dt2 %>%  
    #scale values using all the data 
    group_by(variable) %>%
    mutate(value = scale(value)) %>%
  ungroup() %>%
  mutate(
    #simplify plots
    variable = ifelse(variable %in% c("ns_total_conc", "pmdisc_number", "pnc_noscreen", "pnc_screen"), "UFP", variable)
    )

```

**highest (max) readings**

```{r}
# pull out stops when we saw max values
extreme_high <- dt_stops2 %>%
  group_by(variable) %>%
  mutate(median_overall_value = median(value)) %>%
  filter(value == max(value)) %>% 
  distinct(date, location, instrument_id, value, median_overall_value) %>%
  group_by(date, location) %>%
  summarize(instrument_id = paste0(unique(instrument_id), collapse = "|"),
            median_stop_value = paste0(unique(value), collapse = "|"),
            median_overall_stop_value = paste0(unique(median_overall_value), collapse = "|")
            )
  
# extreme_high %>%
#   kable(caption = "stops where the max instrument value has been observed") %>%
#   kable_styling()

```

* high instrument observations tend to occur when other particle readings are also high, particularly for particle instruments

* CO 190134 looks unstable

* CO2_19 (backup) has much higher readings than co2_14 (primary) on 6/13 and higher readings than other CO2 readings.     
  * **will drop all co2_19 readings** since co2_19 also recorded the highest co2 readings in the dataset on the same day at different stops, and CO2_19 was never used on its own.

* on 2/10, BC and other particle instruments show elevatd concentrations, so okay?

* on 2019-09-03, NO2_1 looks pretty high. This could be real? Instrument readigns look stable

```{r, fig.height=10}
df <- data.frame()

for (i in 1:nrow(extreme_high)) {
  #i=1
  
  temp <-  dt2_temp %>%
    filter(date == extreme_high$date[i] & location == extreme_high$location[i]) %>%
    mutate(
      #pt_size = ifelse(grepl(extreme_high$instrument_id[i], instrument_id), 1, 1),
      extreme_instrument = ifelse(grepl(extreme_high$instrument_id[i], instrument_id), instrument_id, NA),
           ) 
    df <- bind_rows(df, temp)
  
}
  
temp_labels <- df %>% 
  group_by(date, location) %>%
  mutate(time = min(time),
         value = max(value)) %>%
  group_by(date, location, time, value) %>%
  drop_na(extreme_instrument) %>%
  summarize(extreme_instrument = paste0(unique(extreme_instrument), collapse = ", "))
  
  df %>%  {
    ggplot(., aes(x=time, y=value)) + 
      geom_point(aes(shape=instrument_id, col=variable)) +
      #black points make it easier to ID questionnable data
      geom_point(data=subset(., instrument_id==extreme_instrument), size=1) +
      geom_text(data = temp_labels, aes(label= extreme_instrument), 
               hjust=0, show.legend = F) +
      facet_wrap(date~location, scales = "free") +
      scale_shape_manual(values = c(1:length(unique(dt_stops$instrument_id)))) +
      #don't show size in legend 
      guides(size = F) + 
      labs(y = "Standardized Value: (X-mean)/SD",
           title = "Plots of when high instrument readings occur"
           )
  }
  
```

**lowest (min) readings**

* there are more instances with min readings than max readings

* CO 190134 goes from very high to very low readings

```{r}
# pull out stops when we saw max values
extreme_low <- dt_stops2 %>%
  group_by(variable) %>%
  filter(value == min(value)) %>% 
  distinct(date, location, instrument_id) %>%
  group_by(date, location) %>%
  summarize(instrument_id = paste0(unique(instrument_id), collapse = "|"))
  
# extreme_low %>%
#   kable(caption = "stops where the min instrument value has been observed") %>%
#   kable_styling()
```

* nothing stands out as being unordinaryly low? 

* BC tends to have low readings 

```{r, fig.height=20}

df <- data.frame()

for (i in 1:nrow(extreme_low)) {
  #i=1
  
  temp <-  dt2_temp %>%
    filter(date == extreme_low$date[i] & location == extreme_low$location[i]) %>%
    mutate(
      #pt_size = ifelse(grepl(extreme_low$instrument_id[i], instrument_id), 1, 1),
      extreme_instrument = ifelse(grepl(extreme_low$instrument_id[i], instrument_id), instrument_id, NA),
           ) 
    df <- bind_rows(df, temp)
  
}
  
temp_labels <- df %>%
  ungroup() %>%
  group_by(date, location) %>%
  mutate(time = min(time),
         value = max(value)) %>%
  
  group_by(date, location, time, value) %>%
  drop_na(extreme_instrument) %>%
  summarize(extreme_instrument = paste0(unique(extreme_instrument), collapse = ", "))
  
  df %>%  {
    ggplot(., aes(x=time, y=value)) +
      geom_point(aes(shape=instrument_id, col=variable)) +
      #black points make it easier to ID questionnable data
      geom_point(data=subset(., instrument_id==extreme_instrument), size=1) +
      geom_text(data = temp_labels, aes(label= extreme_instrument), 
               hjust=0, show.legend = F) +
      facet_wrap(date~location, scales = "free") +
      scale_shape_manual(values = c(1:length(unique(dt_stops$instrument_id)))) +
      #don't show size in legend 
      guides(size = F) + 
      labs(y = "Standardized Value: (X-mean)/SD",
           title = "Plots of when low instrument readings occur"
           )
        
      }
```


## Update data

* drop co2_19 (backup instrument) readings for a day when these readings were much higher than other collocated instrument and other days 

```{r}

bad_index <- match("CO2_19", extreme_high$instrument_id)

dt_stops3 <- dt_stops2 %>%
  filter(
    #drop co2_19 (backup instrument) readings for a day when these readings were much higher than other collocated instrument and other days 
    !(date ==extreme_high$date[bad_index] & location== extreme_high$location[bad_index] & instrument_id==extreme_high$instrument_id[bad_index])
  )

```


# Collocated duplicate instruments

* No primary standard for UFPs unless instrument returned to TSI.  So need to address between-instrument data and how well they are reading the peaks.  

```{r}
#note: not including lab calibration data since adding calibration_data_2min causes issues b/c: a) there can be multiple diff refernce concentrations during any given time; and b) it's likely but not guaranteed that duplicate instruments were measuring the same reference concentrations at the same time


# #header names in both sources of data: stop and room data 
#common_names <- intersect(names(dt_stops3), names(calibration_data_2min))
common_names <- c("runname", "date", "time", "instrument_id", "value")

dt_stops3_colo <- dt_stops3 %>%
  #these are never collocated
  filter(!instrument_id %in% c("PMPTSCREEN_2", "PMPTSCREEN_93")) %>% 
  select(common_names) #%>% rbind(calibration_data_2min[common_names])   

dt_stops3_colo_wide <- dt_stops3_colo %>%  
  spread(instrument_id, value) %>%
  mutate(
    site = substr(runname, 12,14)
  )

 
# dt_stops3_colo %>% slice(21455, 21456, 21457, 21458, 21459)
# calibration_data_2min %>% filter(time == "2019-12-09 12:50:00", instrument_id=="CO_190134")
# calibration_data %>% filter(time >= "2019-12-09 12:50:00", time <= "2019-12-09 12:52:00",instrument_id=="CO_190134") %>% View()
# 
# calibration_data %>%
#   drop_na(notes) %>%
#   filter(!grepl("filter", notes)) %>%
#   distinct(instrument_id, notes) %>%
#   arrange(instrument_id) %>% View()


```

* most instrument collocations look okay, except for:      
  * CO2_19  - drop since it is never used on its own?     
  * NO2_1 tends to read high
  * PMSCAN_3 reads low/different at times but readings look OK for most of 2019 

* CO 1 (primary instrument) does not correlate well with either of the backup instruments. There is a lot of noise with CO 3, while CO 190134 seems to report very high readings    
  * CO 190134 seems to have values much higher than expected when compared to CO1 and CO3


```{r, fig.height=21}

p <- list()

p[[1]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "BC_0066", y.variable = "BC_0063", col.by = "site", 
            slope_digits = 2, alpha_value = 0.7) + 
  labs(col="Route")


p[[2]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "CO2_14", y.variable = "CO2_19", col.by = "site", alpha_value = 0.7
            ) + 
  labs(col="Route")

p[[3]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "NO2_2", y.variable = "NO2_1", col.by = "site", 
            slope_digits = 0, int_digits = 2, alpha_value = 0.7) + 
  labs(col="Route")

p[[4]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PM25_176", y.variable = "PM25_205", col.by = "site", int_digits = 6, rmse.digits = 6, alpha_value = 0.7) + 
  labs(col="Route")

p[[5]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMDISC_3", y.variable = "PMDISC_8", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

#that these are the only 2 non-screeend PTRAKS that were collocated 
p[[6]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMPT_94", y.variable = "PMPT_93", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

# that these are the only 2 screeend PTRAKS that were collocated 
p[[7]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMPTSCREEN_94", y.variable = "PMPTSCREEN_3", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

p[[8]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMSCAN_5", y.variable = "PMSCAN_3", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")


# CO
p[[9]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "CO_1", y.variable = "CO_3", col.by = "site", alpha_value = 0.7
            ) + 
  labs(col="Route")

p[[10]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "CO_1", y.variable = "CO_190134", col.by = "site", alpha_value = 0.7
            ) + 
  labs(col="Route")

p[[11]] <-  dt_stops3_colo_wide %>%
  colo.plot(x.variable = "CO_3", y.variable = "CO_190134", col.by = "site", alpha_value = 0.7
            ) + 
  labs(col="Route")

ggarrange(plotlist = p, ncol = 2, nrow = 6) %>%
  annotate_figure(top = "Instrument Collocations")

```

* NanoScan_3 correlates well with NanoScan_5 during 2019 but deviates around 2019-09, with the largest differences starting 2020-02 when the R2 drop a lot.   
* Nanoscan 3  returned from TSI after being serviced on 2/13/20
  * We'll only keep the data collected prior, which will allow us to use data collected with this instrument a few days before PMSCAN_5

* Note from Tim G: "...the variation between the NanoScans may be different in different size bins, so you might want to do a size-specific correction, depending on the data that you will use in subsequent analysis"

```{r, echo=T}
# ? right?
drop_ns3 <- ymd("2020-02-01")

```

```{r}

dt_stops3_colo_wide %>%
  drop_na(PMSCAN_5, PMSCAN_3) %>%  
  mutate(ym = substr(date, 1, 7)) %>%  
   
  ggplot(aes(x=PMSCAN_5, y=PMSCAN_3)) + 
  facet_wrap_equal(~ym, scales="free") + 
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.4) +
  geom_point(alpha=0.7, aes(col=runname)) +
  geom_smooth(se = F, method="lm", aes(col=runname)) +
  #note labels are weird when use facet_wrap, but look correct when not using facetwrap
  stat_poly_eq(aes(label =  paste(stat(eq.label),
                                  stat(rr.label), sep = "*\", \"*")),
               formula = y~x, parse=T) +
  
  labs(title = "Comparison of collocated NanoScans over time.",
       subtitle = "linear fit are for all the data on each panel"
  )

```

* nanoscan 3 was serviced Feb 13 2020, while nanoscan 5 was not serviced near that time (Feb 5, 2021).

* Collocated NanoScans vs Discminis.   
  - unclear if these plots clarify anything  

```{r}
dt_stops3_colo_wide %>%
  select(time, contains(c("PMSCAN", "PMDISC"))) %>% 
  #calculate one minute averages
  mutate(time = floor_date(time, "minutes")) %>%#  
  group_by(time) %>%
  summarize_at(str_subset(names(.), "PM"), mean, na.rm=T ) %>% 
  gather("PMDISC", "value_disc", contains("PMDISC")) %>%
  gather("PMSCAN", "value_scan", contains("SCAN")) %>%
  mutate(ym = substr(time, 1,7),
         year = year(time)
         ) %>%
  #filter(year>2019) %>%
    filter(time > ymd("2019-11-01")) %>%

  ggplot(aes(x=value_disc, y=value_scan, col=PMSCAN)) + 
  facet_grid_equal(PMDISC~ym, #scales="free"
                   ) + 
  theme(aspect.ratio = 1) +
  geom_abline(slope = 1, intercept = 0, linetype=2,alpha=0.5) +
  geom_point(alpha=0.1) + 
  geom_smooth(se=F, method="lm") + 
  labs(
    title = "1-min avg concentrations (pt/cm3) during collocations",
    x = "Discmini Conc",
    y = "NanoScan Conc"
  )


```

* Collocated NanoScans vs PTRAKS  
  * unclear whether PTRAKS were also drifting at the end of study...

```{r}
dt_stops3_colo_wide %>%
  select(time, contains(c("PMSCAN", "PMPT_"))) %>%  
  #calculate one minute averages
  mutate(time = floor_date(time, "minutes")) %>%#  
  group_by(time) %>%
  summarize_at(str_subset(names(.), "PM"), mean, na.rm=T ) %>% 
  gather("PMPT", "value_ptrak", contains("PMPT_")) %>%
  gather("PMSCAN", "value_scan", contains("SCAN")) %>%
  mutate(ym = substr(time, 1,7),
         year = year(time)
         ) %>%
  filter(time > ymd("2019-11-01")) %>% 
  
  ggplot(aes(x=value_ptrak, y=value_scan, col=PMSCAN)) + 
  facet_grid_equal(PMPT~ym, #scales="free"
                   ) + 
  theme(aspect.ratio = 1) +
  geom_abline(slope = 1, intercept = 0, linetype=2,alpha=0.5) +
  geom_point(alpha=0.1) + 
  geom_smooth(se=F, method="lm") + 
  labs(
    title = "1-min avg concentrations (pt/cm3) during collocations",
    x = "P-TRAK Conc",
    y = "NanoScan Conc"
  )



```


* NO2_1

```{r}

dt_stops3_colo_wide %>%
  drop_na(NO2_2, NO2_1) %>%
  mutate(ym = substr(date, 1, 7)) %>%  
   
  ggplot(aes(x=NO2_2, y=NO2_1, )) + 
  facet_wrap_equal(~ym,) + 
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.4) +
  geom_point(alpha=0.7, aes(col=runname)) +
  geom_smooth(se = F, method="lm", aes(col=runname)) +
  #note labels are weird when use facet_wrap, but look correct when not using facetwrap
  stat_poly_eq(aes(label =  paste(stat(eq.label),
                                  stat(rr.label), sep = "*\", \"*")),
               formula = y~x, parse=T) +
  labs(title = "Comparison of collocated NO2 over time")

```

* density plots ofPMPTSCREEN 2 and 93, the two screened p-traks that were mostly used in the campapign since they were never collocated   
  * things look fine - values are generally similar    

```{r}
dt_stops3 %>%
  filter(grepl("pnc_screen", variable)) %>%
  ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument)) + 
  geom_density() + 
  scale_x_log10() + 
  labs(title = "distribution of screened P-TRAK values")
  
```


```{r}
# ## Collocation instrument time trends
# 
# *see if duplicate instruments behave differenlty over time when in exact same place 
# 
# * still see some trends related to how long instruments are run (is this real?)   

```
 
 

# Calibrate Instruments based on duplicate collocations 

when were NanoScan3 and NO2_1 used on their own?

```{r}
#nanoscan
dt_stops3 %>%
  filter(grepl("ns_", variable)) %>%
  group_by(date) %>%
  summarize(backup_only = paste(unique(instrument_id), collapse = ", ") ==  "PMSCAN_3") %>%
  filter(backup_only==TRUE) %>%
  summarize(
    first_time = min(date),
    last_time = max(date)
  ) %>%
  kable(caption = "When NanoScan 3 was used on its own. (The next collocation happened end of March)") %>%
  kable_styling()

dt_stops3 %>%
  filter(grepl("no2", variable)) %>%
  group_by(date) %>%
  summarize(backup_only = paste(unique(instrument_id), collapse = ", ") ==  "NO2_1") %>%
  filter(backup_only==TRUE) %>%
  summarize(
    first_time = min(date),
    last_time = max(date)
  ) %>%
  kable(caption = "When NO2_1 was used on its own. The next collocation was a few days later") %>%
  kable_styling()

```

```{r}
#most recent collocation data is used for instrument calibrations  
drop_ns3_v2 <- ymd("2019-04-01") 
drop_no2_1 <- ymd("2019-04-01")   #ymd("2019-07-01")

```


* calibrations are done similar to before, with the reference instrument beign used as the independent variable (X). For the discminis, the mean of the two instruments is used as the independent variable.  

* no need to calibrate backup instruments to eachother if:      
  - there was a primary instrument used in the campaign the majority of the time and backup instruments produce similar readings during collocations    
    * unscreened P-TRAKs   
    * nephs   
    * BC     
  - other reasons:   
    * screened PTRAKs screened since only two backup instruments were collocated (3 and 94), and both were in good agreement    
    * CO instruments since none are in agreement with eachother    
    * CO2 since backup instrument will be dropped       

* if we don't have a primary instrument, however, readings should be calibrated to the average of both instruments based on collocations  
  * discminis 

* calibrate backup instruments when they deviate from the primary instrument using recent collocation data    
  - calibrate NanoScan (ns) 3 to ns 5 using data collected before `r drop_ns3_v2` since 3 tends to read low and ns 3 values after this will not be used anyways since nanoscan 5 readings exist
  - calibrate NO2_1 to NO2_2 using data collected before `r drop_no2_1`.     
    * Note that some NO2_1 values become negative 
  
* calibrate neph readings to estimate PM2.5         

**may update this ST neph calibration curve**

```{r, eval=F}
# last time backup instrument was used on route
dt_stops3_colo_wide %>%
  filter(
    grepl("R0", site),
    is.na(NO2_2) & !is.na(NO2_1)) %>%
  summarize(
    last_date = max(time)
    )

```

```{r}
# calibrate instruments using the most recent collocation data 

#NANOSCAN - using Scan 5 as the reference on 2019-03 since SCAN 3 was only used on its own on 2019-02

ns_lm <- dt_stops3 %>%
  filter(grepl("ns_total", variable),
         #use data when instruments were still in good agreement
         date < drop_ns3_v2
         ) %>%
  select(time, instrument_id, value) %>% 
  spread(instrument_id, value) %>% 
  drop_na() %>% 
  lm(PMSCAN_3~PMSCAN_5, data = .) %>%
  tidy() %>%
  select(term, estimate) %>%
  spread(term, estimate)
  
ns_5_int <- ns_lm$`(Intercept)`
ns_5_slope <- ns_lm$PMSCAN_5

###########################################################################################

# NO2 - using NO2_2 as reference. last time No2_1 was used on its own was 2019-06-13

no2_lm <- dt_stops3 %>%
  filter(grepl("no2", variable),
         date < drop_no2_1
         ) %>%
  select(time, instrument_id, value) %>% 
  spread(instrument_id, value) %>% 
  drop_na() %>%
  lm(NO2_1~NO2_2, data = .) %>%
  tidy() %>%
  select(term, estimate) %>%
  spread(term, estimate)
  
no2_2_int <- no2_lm$`(Intercept)`
no2_2_slope <- no2_lm$NO2_2

###########################################################################################

# DISCMINIS
disc_lm <- dt_stops3 %>%
  filter(grepl("pmdisc_", variable)) %>%
  select(time, instrument_id, value) %>% 
  spread(instrument_id, value) %>% 
  drop_na() %>%
  #calculate mean reading when collocations happened
  group_by(time) %>%
  mutate(mean =  mean(c(PMDISC_3, PMDISC_8)))

##disc 3
disc_3_lm <- disc_lm %>%
  lm(PMDISC_3~mean, data = .) %>%
  tidy() %>%
  select(term, estimate) %>%
  spread(term, estimate)
  
mean_int_disc_3 <- disc_3_lm$`(Intercept)`
mean_slope_disc_3 <- disc_3_lm$mean

##disc 8
disc_8_lm <- disc_lm %>%
  lm(PMDISC_8~mean, data = .) %>%
  tidy() %>%
  select(term, estimate) %>%
  spread(term, estimate)
  
mean_int_disc_8 <- disc_8_lm$`(Intercept)`
mean_slope_disc_8 <- disc_8_lm$mean

###########################################################################################

```

* estimates from Cooper's work. this might be updated

```{r, echo=T}
# NEPH   
neph_int <- 1.060539
## slope was for bscat x 10^-5, this is what it would be for bscat x 10^-4
neph_slope <- 2.509793 * 10

```

# Update Data 

 


```{r}
bscat_unit_conv <- 10^4

dt_stops4 <- dt_stops3 %>%
  #don't need instrument ranges anymore
  select(-c(Min, Max, Units)) %>%
  filter(
    #drop bad CO2 instrument data 
    !instrument_id %in% c("CO2_19"),
    #drop 2020 PMSCAN_3 data after 2019 since these are not in line w/ nanoscan 5. these data will be not used anyways b/c nanoscan 5 data exist
    !(instrument_id == "PMSCAN_3" & date >= drop_ns3),
         ) %>%
  # calibrate instruments (for a second time)
  rename(value_c1 = value) %>%  
  mutate(
    # convert bscat units to bscatx10^-4 (similar to DOE)
    value_c1 = ifelse(grepl("neph", variable), value_c1*bscat_unit_conv, value_c1),
    # adjust other readings
    value =
      # adjust earliest PMSCAN_3 readings based on PMSCAN_5
      ifelse(grepl("PMSCAN_3", instrument_id) & date < drop_ns3_v2, (value_c1-ns_5_int)/ns_5_slope,
             # adjust NO2_1 readings based on NO2_1
             ifelse(grepl("NO2_1", instrument_id) & date < drop_no2_1, (value_c1-no2_2_int)/no2_2_slope,
                    #adjust discmini readings to the mean
                    ifelse(grepl("PMDISC_3", instrument_id), (value_c1-mean_int_disc_3)/mean_slope_disc_3,
                           ifelse(grepl("PMDISC_8", instrument_id), (value_c1-mean_int_disc_8)/mean_slope_disc_8,
                                  #estimate PM2.5 from nephs
                                  ifelse(grepl("neph", variable), value_c1*neph_slope + neph_int,
                                         # value not calibrated a second time
                                         value_c1
                      )))))
  )


```



```{r, eval=F}
# * checking that only earlier NO2 values were changed. Looks good, though pre calibration values for 2019-02 were near 0 before and are < 0 now
 
# calibrations 
dt_stops4 %>%
  filter(instrument_id == "NO2_1") %>%
  mutate(ym = substr(date, 1,7),) %>%  
  ggplot(aes(x=value_c1, y=value,)) + 
  facet_wrap(~ym, ) + 
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  geom_point(alpha=0.3) + 
  labs(title = "NO2 values pre & post recalibration",
       x = "Pre Calibration Conc (ppb)",
       y = "Post Calibration Conc (ppb)"
       )

```


```{r, eval=F}
# ### --> issue: post 2nd calibration, instrument readings are further from the true concentration since they were already low and they are further reduced
# 
# * note: AQS plots below, however, show that the second calibration does get NO2 slightly closer to the readings from AQS sites


calibrations %>%
    filter(instrument_id == "NO2_1",
           date(time) == "2019-03-06"
           ) %>%
  #estimated what value would have been if we calibrate to the NO2 collocation that occurs soon after this
  mutate(value2 = (value - no2_2_int)/no2_2_slope) %>%
  rename(pre_instrument_calib = value, post_instrument_calib = value2, true_conc = conc) %>%
  
  gather("calibration", "value", contains("calib"), true_conc) %>%  
  
  ggplot(aes(x=time, y = value, col=calibration)) + 
  facet_wrap(~time) +
  geom_point()


```


# Instrument time trends (using all the all data)

* things look mostly fine except for CO?    
  * CO has some extreme values
  * e.g., don't see extreme NO2 trends 
  * nanoscan 3 still has slightly lower readings, though these might be from later times which will not be used later if nanoscan 5 was also on the platform
  
* using log10 y scale since there are some extreme values 

```{r, fig.height=12}

dt_stops4 %>%
  #have to group by runname (date and route) b/c some runs ended at midnight and the grouping gets messed up otherwise
  group_by(variable,  runname) %>%
    #calculate time since start of run
    mutate(first_stop = min(time),
           time = as.numeric(difftime(time, first_stop, units = "hours")),
           ) %>%  
  {
    ggplot(., aes(x=time, y=value, 
               col=instrument_id, 
               linetype=primary_instrument
               )) +
      facet_wrap(~variable, scales="free") +
      geom_smooth(se=F, alpha=0.8) +
      
                        #dont plot extreme points - easier visualization
      geom_point(#data=filter(., value < quantile(value, 0.99)),
                 alpha=0.05) +
      #easier to visualize overall trend w/ extreme values
      scale_y_log10() +
    
    labs(title = "instrument readings over time (all remaining data)",
         subtitle = "trend lines use all data",
         x= "hours since first stop",
         linetype="Instrument"
         )  
}

```

# Update data 

```{r}
# DELETE comment? 
# * dropped all CO data. PSCAA graphing tool indicates that hourly CO values are generally between 150-1000 ppb (0.150-1 ppm). These values may be too close to the instrument's meaurement range of 0 ppm (0.1 ppm = 100 ppb LOQ) 

```


* if multiple instruments were on the platform....    
  a) and a primary instrument was used, use readings from the primary instrument.     
  b) and both instruments were backups, use:   
    * PTRAK 94 (instead if PTRAK 3), since this is used unscreend & has good agreement with other instruments   
    * CO_190134 (instead of CO_3), since it has better agrement (R2) with CO_1      
    
```{r}
temp <- dt_stops4 %>%
  # #drop all CO data 
  # filter(variable != "co_ppm") %>%
  group_by(runname, date, time, location, stop_id, variable) %>%
  mutate(
    n = n(),
    has_primary = grepl("Primary", paste(unique(primary_instrument), collapse = ",")),
    ) %>%
  ungroup()

#only one instrument used
df1 <- temp %>%
    filter(n==1)

#duplicate instruments used, including a primary instrument
df2 <- temp %>%
    filter((n==2 & has_primary==TRUE),
      primary_instrument=="Primary")  

#duplicate instruments used, neither of which are primary
df3 <- temp %>%
    filter((n==2 & has_primary==FALSE),
      #select one "backup" instrument to use. PTRAK 94 is used unscreend & has good agreement with other instruments; CO_190134 has better agrement (R2) with CO_1 
      instrument_id %in% c("PMPTSCREEN_94", "CO_190134")
      )  
  
dt_stops5 <- rbind(df1, df2, df3) %>%
  select(-c(n, has_primary)) %>%
  arrange(time)

```


# AQS sites 

Looking at the ACT TRAP field protocol, it looks like BC, NO2 and neph (also neph PM2.5) values are collected every minute by DOE, though these are not officially validated like the hourly data are. We will use these instead of hourly data since we want to compare these readings to our 2-minute collocations.

Note that BH and 10W don’t have minute data. BH has rolling 1-hour averages every 6 minutes for PM2.5 (teoms), while 10W seems to report the same thing every minute for about an hour (~58 min). 

We will have to use a mix of semi-hourly PM2.5 data and hourly data for BH and 10W since data comes from various non-minute-level sources.


-see if our campaign measurements were similar to those from AQS sites. Compare AQS:
  - 2-min medians
  - 2-min annual avg estimate
  - true annual avg w/ full data

```{r}
 
#stop collocations
dt_stops3_colo_a <- dt_stops5 %>%
  filter(
    #pollutants also measured at AQS sites
    variable %in% c("ma200_ir_bc1", "neph_bscat", "no2", "co_ppm"),
    #only keep collocation sites
    grepl("MC", location),
    
    # #TEST: don't include backup COs
    # !instrument_id %in% c("CO_3", "CO_190134")
    
    ) %>%  
  #drop seconds from time stamp (for DOE merging). change default tz of UTC to LA
  mutate(time = ymd_hm(format(time, format = "%Y-%m-%d %H:%M"), tz=mytz)) %>%
  select(-c(instrument_id, primary_instrument, value_c1, value_raw)) %>%
  spread(variable, value) %>%  
  arrange(time)  %>%  
  #rename pollutants
  rename(BC = ma200_ir_bc1,
         PM2.5 = neph_bscat,
         NO2 = no2,
         CO = co_ppm
         )  %>%
  mutate(
    # put in same units as DOE
    ##change from ng/m3 to ug/m3
    BC = BC/10^3,
    ## change from ppm to ppb
    CO = CO*10^3
  )

## same as above for neph bscat readings (not PM2.5)
dt_stops3_colo_b <- dt_stops5 %>%
  filter(
    variable %in% c("neph_bscat"),
    #only keep collocation sites
    grepl("MC", location)
    ) %>% 
  #drop seconds from time stamp (for DOE merging). change default tz of UTC to LA
  mutate(time = ymd_hm(format(time, format = "%Y-%m-%d %H:%M"), tz=mytz)) %>%
  select(-c(instrument_id, primary_instrument, value, value_raw)) %>%
  #usine neph, not pm2.5 estimates here
  spread(variable, value_c1) %>% 
  arrange(time)  %>%  
  #rename pollutants
  rename(Neph = neph_bscat) %>%
  mutate(Neph = Neph)  

dt_stops3_colo <- left_join(dt_stops3_colo_a, dt_stops3_colo_b)

# data from DOE
doe_colo <- dt_stops3_colo %>%
  #also pull 2nd minute from DOE data
  mutate(time = time+60) %>%
  #add start time (1 min earlier)
  rbind(dt_stops3_colo,.) %>%
  #collocation times & places
  select(time, location) %>%
  arrange(time) %>% 
  # doe data 
  left_join(doe2) %>%  
  #MC0406 not visited on 2020-03-13 06:41:00	
  drop_na() %>% 
  #calculate 2-min median - same as avg for 2 numbers.
  group_by(date, location, location_doe, pollutant) %>%
  summarize(value=median(value)) %>% 
  #wide format
  pivot_wider(names_from = pollutant, values_from = value, names_prefix = "doe_")  
  
#merge our estimates and doe estimates
dt_doe <- left_join(dt_stops3_colo, doe_colo) 
  
```

```{r}
# repeat for NO2_1 only to compare values after the first and "second" calibration
 
no2_test <- dt_stops5 %>%
  filter(
    instrument_id=="NO2_1",
    #only keep collocation sites
    grepl("MC", location)
    ) %>% 
  #drop seconds from time stamp (for DOE merging). change default tz of UTC to LA
  mutate(time = ymd_hm(format(time, format = "%Y-%m-%d %H:%M"), tz=mytz)) %>% 
  select(-c(value_raw)) %>%
  rename(value_c2=value)
   
# data from DOE
no2_test_b <- no2_test %>%
  #also pull 2nd minute from DOE data
  mutate(time = time+60) %>%
  #add start time (1 min earlier)
  rbind(no2_test,.) %>%
  #collocation times & places
  select(time, location) %>%
  arrange(time) %>% 
  # doe data 
  inner_join(filter(doe2, pollutant=="NO2")) %>%  
  #MC0406 not visited on 2020-03-13 06:41:00	
  drop_na() %>% 
  #calculate 2-min median - same as avg for 2 numbers.
  group_by(date, location, location_doe, pollutant) %>%
  summarize(value=median(value)) %>%
  #wide format
  pivot_wider(names_from = pollutant, values_from = value, names_prefix = "doe_")  
  
#merge our estimates and doe estimates
no2_test_c <- inner_join(no2_test, no2_test_b) %>%
  gather(variable, value, contains("value"))
 
```




## Calibrate neph readings  

* here, we show that Copper's ST calibration curve is similar to the one we fit when we use DOE data for the study area and period alone.    

* fit a calibration curve to DOE PM2.5~Neph readings    
  * **dropping values < 1st and > 99th quantile**
    * this made the lm fit R2 go from ~ 0.05 to ~0.5    
  * note that neph readings are very minute, while PM2.5 readings are estimated every hour, so these are not totally comparable. But we want neph readings at the minute level, since these will be much noisier (like our mobile data) ?

```{r}
#lm fit
doe_neph_calib <- doe %>%
  filter(
    # hour/day estimates may be largely aggregates of minute data 
    freq == "minute",
    #locations with nephs and PM2.5 readings
    location_doe %in% c('AQSD', 'AQSK', 'AQSTUK'),
    pollutant %in% c("Neph", "PM2.5"),
    #don't inlcude neph-based pm2.5, which are calculated from the neph-PM2.5 calibration
    !variable %in% "doe_npm25"
    ) %>%  
  #if multiple PM2.5 readings at the same time and place, take the mean
  group_by(location_doe, location, time, pollutant) %>%
  summarize(value = mean(value)) %>%
  ungroup() %>%
  spread(pollutant, value) %>%
  #drop extreme values before fitting curve
  drop_na() %>%
  #don't plot extreme values
  filter(Neph > quantile(Neph,0.01) & Neph < quantile(Neph,0.99),
         PM2.5 > quantile(PM2.5,0.01) & PM2.5 < quantile(PM2.5,0.99),
         ) 

```



```{r, eval=F}
# * there is a lot of noise in the neph readings, but there is a generally positive trend

#plot takes long time to run
print("black line is best fit line; blue line is smooth fit")

doe_neph_calib %>%
  ggplot(aes(y=PM2.5, x=Neph)) + 
  #geom_point(alpha=0.1) + 
  geom_bin2d() +
  geom_smooth() +
  geom_smooth(method="lm", col="black") +
  
  labs(title = "Measured nephelometer and PM2.5 readings at AQS sites. \nnot plotting extreme values",
       x= "Minute Neph Scatter (bscat [x10^-4]/m)",
       y = "Hourly PM2.5 Conc (ug/m3)"
       )

```

* this slope is similar to Cooper's (int: `r neph_int`, slope: `r neph_slope`) when we adjust his calibration curve, which uses bscat x 10^-5, to to bscat x 10^-4.

```{r}

doe_lm <- doe_neph_calib %>%
  lm(PM2.5~Neph, data=.) 

doe_lm %>% summary()

# doe_int <- tidy(doe_lm)$estimate[tidy(doe_lm)$term=="(Intercept)"]
# doe_slope <- tidy(doe_lm)$estimate[tidy(doe_lm)$term=="Neph"]
   
```


## 2-min collocations (medians)

* There are various discrepancies between DOE and mobile monitoring observations    
  * locations - these are not exaclty the same. Also, the MM vehicle does not always park in the exact same location    
  * instrument differences    
    * all of our PM2.5 estimates come from neph estimates. DOE PM2.5 estimates compared here are from nephelometers when available (Duwamish, Kent, Tukwilla), otherwise from gravimetric and BAM (beta attenuation) methods (10W, BH)  
    * DOE uses site-specific neph-PM2.5 calibration curves. We will use a region calibraiton curve based on multiple AQS sites in the Spatiotemporal modeling region     
      * our MM calibration curve from the ST model excludes Duwamish, even though we use it to predict PM2.5 at Duwamish in this analysis     
    * DOE PM2.5 "minute" readings are actually for longer time periods. For example, rolling 1-hr estimates that are updated every 6 min.
    * the "2-min" median estimates have slightly different start and end times for the MM and DOE estimates. DOE estimates area alwyays exactly 2-min, starting at 00 seconds. MM median stops could have started after 00 seconds into the minute and could have been sligthly longer than 2 min (e.g., 3 min).    
    
* CO performs poorly. Performance was similarly poor when only CO_1 instrument data were compared to DOE readings.      

```{r}
dt_doe_l <- dt_doe %>%
  gather(pollutant, value, contains(c("BC", "Neph", "NO2", "PM2.5", "CO"))) %>% 
  mutate(
    source = ifelse(grepl("doe", pollutant), "DOE", "MM"),
    pollutant = gsub("doe_", "", pollutant)
  ) %>% 
  spread(source, value) %>%
  drop_na(DOE, MM)

```

```{r, fig.height=10}
print("2-min mobile monitoring collocations at AQS sites")

dt_doe_l %>%
  mutate(
    #AQS Units
    pollutant = recode_factor(factor(pollutant),
                               "BC" = "BC (ug/m3)",
                               "Neph" = "Neph (bscat [x 10^-4]/m)",
                               "NO2" = "NO2 (ppb)",
                               "PM2.5" = "PM2.5 (ug/m3)",
                              "CO" = "CO (ppb)"
                               )
  ) %>%
  ggplot(aes(x=DOE, y=MM, )) + 
  facet_wrap_equal(~pollutant, scales="free") +
  theme(aspect.ratio = 1) + 
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  geom_point(aes(col=location_doe)) + 
  geom_smooth(aes(x=DOE, y=MM), method = "lm",inherit.aes = F ) + 
   stat_poly_eq(aes(label =  paste(stat(eq.label),
                                  stat(rr.label), sep = "*\", \"*")),
               formula = y~x, parse=T, #coef.digits = 2
               ) +
  labs(
       y = "Mobile Monitoring",
       col = "AQS Site"
       )
   
```

```{r}
# table of fit

dt_doe_l %>%  
  group_by(pollutant) %>% 
  summarize(
    N_pairs = n(),
    R2_reg = cor(DOE, MM)^2,
    R2_mse = r2_mse_based(DOE, MM),
    RMSE = rmse(DOE, MM)
  ) %>%
  kable(caption = "Comparison of 2-min mobile monitoring stop medians to respective DOE readings", 
        digits = 2
        ) %>%
  kable_styling()
```


### NO2 before and after the second calibration based on collocated instruments

* showing that NO2_1 instrument readings are slightly better (more similar to AQS collocations) after adjusting No2_1 to NO2_2 readings, though low reading are below the 1-1 line. **We'll thus keep the negative NO2_1 readings since those are our best estimate of the true concentration.**

* this is only at two sites (N=1 AQS collocation per site)

```{r}
no2_test_c %>%
  ggplot(aes(x=doe_NO2, y=value, col=variable, shape=location_doe)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  labs(
    title = "NO2_1 values after calibrations. c1: gas ref calibration; c2: NO2_2 colo calibration",
    col = "Calibration #"
  )

```




## Annual Avg comparisons

```{r}
#calculate annual averages
true_annual <- doe2 %>%
  group_by(location_doe, location, pollutant) %>%
  summarize(
    true_avg = mean(value)
  )

short_term_annual <- dt_doe %>%
  #make long format
  gather("pollutant", "value", matches(c("bc", "neph", "no2", "PM2.5", "co"))) %>% 
  mutate(
    source = ifelse(grepl("doe", pollutant), "DOE", "Mobile Monitoring"),
    pollutant = gsub("doe_", "", pollutant)
  )  %>%
  #dorp places w/o data
  drop_na() %>% 
  group_by(location, location_doe, pollutant, source) %>%
  summarize(
    value = mean(value)
  )

#note that DOE PM2.5 has no MM equivalent (yet)
true_annual <- left_join(true_annual, short_term_annual) %>%
  drop_na(value) %>%
  ungroup()

```

* little data with a narrow range of values to compare annual average neph estimates  
* neph-based PM2.5    
  * looks OK except for duwamish for MM campaign    
  * looks better for DOE since they fit site-specific calibration curves   
  
* mobile monitoring annual average CO estimates were poor compared to the true annual avearge  


```{r, fig.height=10}

true_annual %>%
  mutate(
    #AQS Units
    pollutant = recode_factor(factor(pollutant),
                               "BC" = "BC (ug/m3)",
                               "Neph" = "Neph (bscat [x 10^-4]/m)",
                               "NO2" = "NO2 (ppb)",
                               "PM2.5" = "PM2.5 (ug/m3)",
                              "CO" = "CO (ppb)"
                               )
  ) %>%
  
  ggplot(aes(x=true_avg, y=value, col=source)) + 
  facet_wrap_equal(~pollutant, scales="free") +
  theme(aspect.ratio = 1) +
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  geom_line(stat="smooth", method = "lm", size = 0.5, alpha = 0.5) +
  geom_point(aes(shape=location_doe), size=3) + 
  
    labs(
         col = "2-min Est",
         shape = "Location",         
         x="True Annual Average",
         y = "Estimate"
         )

```

 
```{r}
true_annual %>%
  group_by(pollutant, source) %>%
  summarize(
    N_pairs = n(),
    R2_reg = cor(true_avg, value)^2,
    R2_mse = r2_mse_based(true_avg, value),
    RMSE = rmse(true_avg, value)
  ) %>%
  kable(caption = "site annual average estimates from 2-min readings relative to the true annual average", 
        digits = 2
        ) %>%
  kable_styling()
  
  
```



```{r, eval=F}
# Do the observed annual avearge neph readings have a narrow range?
# 
# * the distribution of minute-level neph data is much wider than the range of neph estimates we see for the annual averages above. This suggests that the "poor" performing neph readings are all close in range to one another and hence why the plot may be making these look bad even though they are not that different. 
#   * neph estimates for these sites (Duwamish, Kent, Tukwilla) are also on the high end relative to 


doe2 %>% 
  filter(pollutant == "Neph",
         #sites where we're using neph-based PM2.5 (vs e.g., FEM method)
         #location_doe %in% c("AQSD", "AQSK", "AQSTUK")
         ) %>%
  summarize(
    Min = min(value),
    Q05 = quantile(value, 0.05),
    Q25 = quantile(value, 0.25),
    Q50 = median(value),
    Q75 = quantile(value, 0.75),
    #IQR = IQR(value),
    Q95 = quantile(value, 0.95),
    Max= max(value)
  ) %>%
  kable(caption = "Distribution of neph readings (bscat x10^-4/m)") %>%
  kable_styling()
   
```


```{r, eval=F}
# When did we visit neph DOE sites? Checking that we conducted balanced sampling at these collocation sites.

neph_colos <- dt_doe %>%
  #sites with neph data
  filter(location_doe %in% c("AQSD", "AQSK", "AQSTUK")) %>%
  
  select(location_doe, date, time, Neph) %>%
  drop_na() %>%
  mutate(
    hour = factor(hour(time)),
    day = wday(time, label = T, week_start = 1),
  ) %>% 
  add_season(.date_var = "date") 

```


```{r, eval=F}
# * things look OK?  
#   * visits by season-day of week look okay. visits didn't occur on every combination, but weekday and weekend samples occurred    
#   * there are more gaps in season-hour combinations, though these are also intermittent for the most part   


#hour
neph_colos %>%
  ggplot(aes(x=hour, y=location_doe)) + 
  geom_bin2d() +
  facet_wrap(~season) + 
  #geom_bar(stat = "count", position = "dodge") 
  labs(title = "number of visits to DOE collocation sites with nephs")

#day
neph_colos %>%
  ggplot(aes(x=day, y=location_doe)) + #fill = location_doe
  geom_bin2d() +
  facet_wrap(~season) +
  #geom_bar(stat = "count", position = "dodge")
  labs(title = "number of visits to DOE collocation sites with nephs")

# #season
# neph_colos %>%
#   ggplot(aes(x=season, y=location_doe)) + #fill = location_doe
#     geom_bin2d()


```

## Overnight Collocations

- compare collocated instruments


# Update data

* drop all CO data b/c it does not correlate well with: 
  * other CO data from duplicate collocated instruments        
  * DOE data 

```{r}
dt_stops6 <- dt_stops5 %>%
  #drop all CO data
  filter(!grepl("co_", variable))

```


# Final Data 

 

## stop data dropped 

```{r}
original_data <- dt_stops
final_data <- dt_stops6

```


```{r}
#* what is a low-level concentration? for zero comparison plots above
  
stop_tb <- final_data %>%
  group_by(variable, instrument_id) %>%
  summarize(
    N = n(),
    Min = min(value),
    Q05 = quantile(value, 0.05),
    Q10 = quantile(value, 0.10),
    Median = median(value),
    Q90 = quantile(value, 0.90),
    Q95 = quantile(value, 0.95),
    Max = max(value)
  ) %>%
  ungroup()
  
#use in zero plots later
saveRDS(stop_tb, file = file.path("Data", "Output", "stops_distribution_table.rda"))

```




* the proportion of data dropped 

```{r}

# original_data %>%
#   summarize(
#     N_original = n()
#   ) %>%
#   cbind(summarize(final_data, N_final = n())) %>%
#   mutate(
#     N_dropped = N_original - N_final,
#     Proportion_dropped = N_dropped/N_original
#   ) %>%
#   kable(caption = "All Stop data dropped (). N = total median stops readings = 8 instruments x 309 sites x ~29 stops/site", digits = 2) %>%
#   kable_styling()

original_data %>%
  summarize(
    N_original = n()
  ) %>%
  cbind(summarize(dt_stops5, N_final = n())) %>%
  mutate(
    N_dropped = N_original - N_final,
    Proportion_dropped = N_dropped/N_original
  ) %>%
  kable(caption = "Stop data dropped (not including CO data). N = total median stops readings = 8 instruments x 309 sites x ~29 stops/site", digits = 2) %>%
  kable_styling()

```


```{r}
 
n_final <- final_data %>%
  group_by(variable) %>%
  summarize(N_final = n())

original_data %>%
  group_by(variable) %>%
  summarize(N_original = n()) %>%
  left_join(n_final) %>%
  mutate(
    N_final = ifelse(is.na(N_final), 0, N_final),
    N_dropped = N_original-N_final,
    #N_dropped = ifelse(is.na(N_dropped), )
    Proportion_dropped = N_dropped/N_original
  ) %>%
  variable_relabel() %>%
  kable(caption = "Data dropped. N = number of stops", digits = 3) %>%
  kable_styling()

```


## Save data 

```{r, eval=T}
# median stop data 
## units: nephs read bscatx10^-4 (similar to DOE). 
saveRDS(final_data, file.path("Data", "Output", "stop_data.rda"))

# #second data
## no need to save this if nothing changes really

# dt3 %>%
#   #left_join(locations) %>%
#   saveRDS(., file.path("Data", "Output", "second_data.rda"))

# variables
save(
  ap,
  #neph calibration curve to be used
  neph_int, neph_slope,
  file = file.path("Data", "Output", "common_vars.rda"))

```










# Appendix

## Calibrate to the mean of instrument readings

* we will not do this since there was generally good agreement between instruments and some backup instruments were less trustworthy (e.g., NO2_1, PMSCAN_3, PMDISC_8 was dropped at one point before being replaced).

* note that the screened PTRAKS were not all collocated, though we know that 93 and 94 were similar when they were unscreened, and that 94 and 3 were similar, thus we will assume that at least 3/4 of the instruments were in good agrement. We don't have any collocation information for PTRAK 2, which collected a substantial amount of data.

## Notes on Hank's CO Work

Amanda: "...Hank was seeing no correlation between measurements from mobile monitoring and at agency locations, and Jim was seeing really bad correlations between co-located monitors in the vehicle.  The calibration data was really different between the two instruments, but it seemed reproducible in the lab, but then in the vehicle there were a bunch of really strong temperature effects."

## Fieldnotes and instrument warnings 

* look at these notes throughout as we encounter issues. We can look at the field or lab notebook notes if things look wrong before deciding to drop data (e.g., how some PMPT lab data were dropped)         
* see QAQC.html doc that Amanda and Kaya have worked on    
  * most instrument error codes don't have "use" indicators to clarify whether that data should be used or not though (as of 5/11/21)      

```{r, eval=F}
fieldnotes2 <- fieldnotes %>%
  #only look at stop data w/ notes
  filter(
    str_detect (site_id, regex("^MS|MC", ignore_case = T)),
    !is.na(notes),  
    !notes %in% c("", ".", "\\")
  )


```


## ONA Correction

* The ONA correction averages based on change in attenuation (different than when you do a time avg). It accounts for change in sensitivity. You are saying that there is a non-linear change in noise and want to avg based on incremental changes in filter. Elena's code shifts every 5% change in attenuation.

* We don’t want to do the ONA correction b/c this would include time points across more space when using the moving data. For consistency, we thus will not do this.


## Resources

**email from Jill Schulte** regarding instrument QC: 

* "I think you'll find much of the information you're looking for on our Information For Air Monitoring Professionals webpage: https://ecology.wa.gov/Regulations-Permits/Guidance-technical-assistance/Information-for-air-monitoring-professionals. If you open the "Other Procedures" tree, the document "Air Monitoring Documentation, Data Review, and Validation Procedure" has general information about how we validate data, apply completeness criteria, and use 1-minute averages for validation. You'll also find more specific information in each of our standard operating procedures for specific pollutants. You can open the SOP trees for gases and particulate matter instruments on the same page, and most have a "Data Validation and Quality Assurance" section. 
* We also rely heavily on EPA's Quality Assurance Handbook for Air Pollution Measurement Systems Volume II: https://www.epa.gov/sites/production/files/2020-10/documents/final_handbook_document_1_17.pdf. "

*ideas    
  * comapre instrument readings to nearby stops (i.e., stations in the same air shed)     
    * this may be misleading in our case b/c we can smooth out 2-min/instant readings, which may correctly show very different readings?   

