---
title: "Instrument Quality Control (QC)"
author: "Magali Blanco"
date: ' `r Sys.time()` '
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    number_sections: true
    toc_float: true
    collapsed: false
    smooth_scroll: false
editor_options: 
  chunk_output_type: console
---
# Overview 

**Purpose**:   

* This script documents the quality control procedures taken to ensure data quality after the ACT TRAP mobile monitoring campaign. 

**Approach**  

* We check that calibrations and particle instrument zeroing occurred     
* Instrument error checks: _____    
* Unit of analysis: Data are cleaned up at the stop level (2-minute median concentration) rather than the second level since that is our unit of analysis. Finer, second data can be a lot more variable and have a lot more extreme values. If your interest is in using these data at the second level, additional analyses may be necessary.    
* At the stop level, we investigate the times when we observed extreme pollutant values (i.e., the lowest and highest readings). We do so by comparing these against other instruments to check that concentration patterns were similar.      
* To assess instrument precision, we compare collocated instrument readings in the field and in the lab   
* Based on the stop-level analyses, we drop bad readings from the dataset  
* We check that there are no instrument time trends (i.e., that there is no concentration "drift" as a function of an instrument's operating time)    
* To assess stop concentration accuracy, we compare the readings from the mobile monitoring campaign against readings from AQS site collocations   

**Results** 

### --> resuls summary 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F, 
                      tidy.opts=list(width.cutoff=60), tidy=TRUE,
                      #fig.height = 5, fig.width = 8
                      fig.height = 6, fig.width = 10
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
      detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(knitr, kableExtra, 
               ggpubr, tidyverse,
               ggrepel, #geom_label_repel
               #mapping...adding scales, N arrows
               ggmap, sf, ggspatial, 
               units, #convert between e.g., m to km
               #time series data 
               lubridate,
               # modeling
               broom ##tidy()
               )    
 

set.seed(1)

```


# Upload data

common variables & functions

```{r}
#functions
source("0_Functions.R")

mytz <- "America/Los_Angeles"

start_date <- "2019-02-22"
end_date <- "2020-03-17" #or: "2020-03-18" to capture all of 3/17 ?

pt_pollutants <- c("PM", "BC")

#air pollutants of immediate interest
ap <- c(
  #CO2
  "co2_umol_mol",
  #aethalometer
  "ma200_ir_bc1",
  #neph
  "neph_bscat", #"neph_ccoef",
  #no2
  "no2",
  #nanoscan
  "ns_total_conc",
  # discmini
  'pmdisc_number',  
  #ptraks
  "pnc_noscreen", "pnc_screen"
  )

unique_routes <- paste0("R0", c(1:9))

#drop instruments w/ few readings (only want a primary and backup instrument if possbile)
drop_instruments <-  c(paste0("PMPT_", c(1,2,4)),
                               paste0("PMPTSCREEN_", c(1,4))
)
```

## helper tables

```{r}
#imports, fieldnotes, and location tables
load(file.path("Data", "Original", "imports_fieldnotes_location_tables.rda"))

calibrations <- readRDS(file.path("Data", "Original", "lab_calibrations_in_fixed_rooms.rda")) %>%
  mutate(date = ymd(substr(time, 1, 10))) %>%
  filter(
    #not sure why tim measured these  
    !analyte %in% c("no, nox"),
    # only keep instrument readings we're interested in
    variable %in% ap,
    #drop instruments w/ few readings (only want a primary and backup instrument)
    !instrument_id %in% drop_instruments
  ) %>%
  
  #drop analyte since this is sometimes wrong (Amanda); other variables make duplicate rows - due to merging issue earlier?
  select(-c(analyte, id, import_id, level)) %>%
  distinct() %>%
  drop_na(instrument_id, variable, value)

```

## stop data

```{r}
#all stop data
dt0 <- readRDS(file.path("Data", "Original", "stop_data_2019-02-22_2020-03-17.rda")) %>%
  mutate(date = ymd(substr(time, 1, 10))) %>%
  filter(
    # only keep instrument readings we're interested in
    variable %in% ap,
    #drop instruments w/ few readings (only want a primary and backup instrument)
    !instrument_id %in% drop_instruments,
    #drop Roosevelt Garage
    #location != "MS0000"
    #this location was only visited once
    location != "MS0398"
  ) %>%
  #drop empty data
  drop_na(value) %>%
  # drop ~ 700 duplicate rows - why are these here?? MS000?
  distinct()
  

#ID primary instruments
primary_instruments <- dt0 %>%
  group_by(variable, instrument_id) %>%
  summarize(
    n = n()
  ) %>% 
  group_by(variable) %>%
  filter(n == max(n)) %>%
  ungroup() %>%
  pull(instrument_id)
 
dt0 <- dt0 %>%
  mutate(primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup" ),
         primary_instrument = factor(primary_instrument, levels = c("Primary", "Backup"))
         )


# DUPLICATE ROWS??
# There ~ 15k rows w/ duplicate information other than the value column. Looks like co_14 (the primary CO2 instrument) was primarily responsible for this - it reported duplicate readings at any given time. Duplicate readings appear to be very similar. Was this related to an instrument logging issues?? We'll average over these.

#dt0 %>% select_at(vars(-value)) %>% distinct() %>% nrow() #6283650
  
# dt0 %>% 
#   select(location, time, instrument_id, value) %>% 
#   
#   slice(2974420, 2974421, 4056517, 4056518, 4056523, 4056524, 4056529, 4056530, 4056535, 4056536, 4056541, 4056542, 4056549, 4056550 , 4056555, 4056556, 4056561, 4056562, 4056567, 4056568, 4056573, 4056574, 4056579, 4056580, 4056585, 4056586, 4056591, 4056592, 4056597, 4056598, 4056603, 4056604, 4056611, 4056612, 4056617, 4056618, 4056623, 4056624, 4056629, 4056630, 4056635, 4056636
# , 4056641, 4056642
# , 4056647, 4056648
# , 4056653, 4056654
# , 4056659, 4056660
# , 4056665, 4056666
# , 4056673, 4056674
# , 4056679, 4056680
# , 4056685, 4056686
# , 4056691, 4056692
# , 4056697, 4056698
# , 4056703, 4056704
# , 4056709, 4056710
# , 4056715, 4056716
# , 4056721, 4056722
# , 4056727, 4056728
# , 4056735, 4056736
# , 4056741, 4056742
# , 4056747, 4056748
# , 4056753, 4056754
# , 4056759, 4056760
# , 4056765, 4056766
# , 4056771, 4056772
# , 4056777, 4056778
# , 4056783, 4056784
# , 4056789, 4056790
# , 4056797, #4056
#         ) %>% View()
  
   
dt0 <- dt0 %>% 
  #take avg of duplicate CO2_14 rows w/ slightly different values
  group_by_at(vars(-value)) %>% 
  summarize(value = mean(value)) %>% 
  ungroup()

dt0_w <- dt0 %>%
  select(-c(variable, primary_instrument)) %>%
  spread(instrument_id, value) %>%
  arrange(time)
  
```

```{r}
#types of instruments
instruments <- unique(dt0$instrument_id) %>%
  str_extract(., "[^_]+") %>%
  unique() %>%
  sort()

```

 
```{r, eval=F}
# don't add stop_route_no since results in sites w/ missing visited number?

#sequence of stop order based on a run. Using runs after the campaign had run for a while, since some changes happened early on. The resulting numbering should be mostly accurate for the rest of the campaign.

first_run <- dt0 %>%
  #don't consider initial runs
  filter(date > ymd("2019-04-01"),
         location != "MS0000"
         ) %>%  
  
  group_by(site) %>%
  filter(time == min(time)) %>% 
  ungroup() %>%
  distinct(runname) %>% pull()
  
run_stop_order <- dt0 %>%
  #only keep first run of each route
  filter(runname %in% first_run,
         location != "MS0000"
         ) %>%
  
  group_by(runname, location) %>%
  mutate(arrival_time = min(time)) %>% 
  ungroup() %>%
  distinct(runname, location, arrival_time) %>% 
  group_by(runname) %>%
  #number stop order within each day
  mutate(run_stop_no = row_number(),
         run_stop_no = str_pad(run_stop_no, width = 2, side = "left", pad = "0")
         ) %>%
  ungroup() %>%
  select(location, run_stop_no)

#unique(dt0$run_stop_no)

#dim(left_join(dt0, run_stop_order))

# add stop order within a route
dt0 <- left_join(dt0, run_stop_order)

# # how many sites have NAs (location not visited within that runname)
# dt0 %>%
#   filter(is.na(run_stop_no)) %>%
#   distinct(location) %>%
#   nrow()

```

## Additional room collocation data 

* calculate 2-min medians so instrument time stamps always line up

*  dropped PMPT_94 when time >= "2019-04-18 11:48:00" & time < "2019-04-18 12:12:00" in LABC. Paper lab notebook states that this PTRAK had alcohol wick issues. Instrument 94 also reads higher than 93.

```{r}
room_colo <- readRDS(file.path("Data", "Original", "room_colo.rda")) %>%
  ungroup() %>%
  filter(
    # only keep instrument readings we're interested in
    variable %in% ap,
    # don't look at these instruments
    !instrument_id %in% c(
      #we did not conduct MM with this instrment 
      "PMSCAN_2",
      #these were not used in the campaign as non-screened
      paste0("PMPT_", c(2:4))
      ),
    # none of the screened ptraks were collocated. #c(2,94))
    !grepl("PMPTSCREEN", instrument_id),
    
    # paper field notebook says that PMPT_94 had a wick issue around this time. the colloation data also shows that its readings were higher than PMPT93 for 6 readings (readings occurred every 2 sec)
    !(instrument_id == "PMPT_94" & time >= "2019-04-18 11:48:00" & time < "2019-04-18 12:12:00"),
    
        ) %>%
  #calculate 2-min medians
  mutate(time = floor_date(time, "2 mins")) %>% 
  group_by_at(vars(-value)) %>%
  summarize(value = median(value)) %>%
  ungroup()


# test <- room_colo %>%
#   spread(instrument_id, value) 


```



```{r, eval=F}
# what room data do we have?
room_colo %>%
  distinct(date, variable, instrument_id) %>%  
  ggplot(aes(x=date, y=instrument_id, col=variable)) + 
  geom_point() + 
  geom_vline(xintercept = ymd(end_date))
  
```


## DOE 

```{r, eval=F}
# #tell R time is actually 8 hrs behind GMT (always in PST), then convert to include PDT. Otherwise, R thinks the times are all in PDT & PST. DO THIS FIRST - before calculating other temporal variables.

daylight_start <- ymd("2019-03-10")
daylight_end <- ymd("2019-11-03")

#LA time
summer <- ymd_hm("2019-03-11 15:00", tz = mytz)
fall <-ymd_hm("2019-11-04 15:00", tz = mytz)

# reset time to actually be in PST, then adjust to LA time
with_tz(ymd_hms(summer, tz = "Etc/GMT+8"),
        tzone = mytz
        )

with_tz(ymd_hms(fall, tz = "Etc/GMT+8"),
        tzone = mytz
        )

```

* **note** DOE data is fixed so that it is read as PST (correct) and then converted to LA time (includes daylight savings) like the rest of our mobile monitoring data. If Dave S fixes this in the dataset, this initial setting should be removed/adjusted.

```{r}
doe0 <- readRDS(file.path("Data", "Original", "doe_dt.rda"))

drop_doe_vars <- c(
  # # have no2 at the same sites 
  "doe_trace_noy", "doe_trace_no",     "doe_trace_noy-no", "doe_nox", "doe_no"
  )

doe <- doe0 %>%
  rename(location_doe=site) %>%
  
  filter(
    #don't need Tacoma site - no collocation here
    location_doe != "AQSTAC",
    #don't need these variables
    !variable %in% drop_doe_vars,
         ) %>%
  mutate(
    #NO NEED TO DO THIS BELOW ANYMORE b/c database was fixed
    #tell R time is actually 8 hrs behind GMT (always in PST, no dayllight savings), then convert to America/Los_Angeles to include PDT like the rest of our data. Otherwise, R thinks the times are all in UTC OR PDT & PST. DO THIS FIRST - before calculating other temporal variables.
    #time = with_tz(ymd_hms(time, tz = "Etc/GMT+8"), tzone = mytz),
    #time = ymd_hms(time, tz = mytz),  
    
    date = ymd(substr(time, 1, 10)),
    #crosswalk for our location IDs
    location = recode_factor(factor(location_doe),
                             #10th & Weller
                             "AQS10W" = "MC0120",
                             #Beacon Hill
                             "AQSBH" = "MC0003",
                             #Kent-Central & James
                             "AQSK" = "MC0406",
                             #Duwamish
                             "AQSD" = "MC0126",
                             #Tukwila Allentown
                             "AQSTUK" = "MC0002"
                             ),
    #sampling frequency
    freq = recode_factor(factor(freq),
                          "DOE_H" = "hour",
                          "DOE_M" = "minute",
                          #"DOE_D" = "day",
                          ),
    freq = factor(freq, levels = c("minute", "hour"#, "day"
                                   )),
    
    pollutant = ifelse(grepl("_no", variable), "NO2", 
                            ifelse(grepl("pm25", variable), "PM2.5",
                                   ifelse(grepl("neph", variable), "Neph",
                                          ifelse(grepl("_bc_", variable), "BC", NA)
                                          )
                            )
                            ),
    pollutant = factor(pollutant, levels = c("BC", "NO2", "Neph", "PM2.5"))
    
  )   

```

* note, hourly and daily readings should be "validated" by the DOE since the final files were requested > 3 months after the data were collected (how long it takes DOE to validate data.)

```{r}

doe %>%
  # make code run faster
  group_by(location_doe, freq, variable, pollutant) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  
  #filter(!variable %in% drop_doe_vars) %>%
  mutate(location_doe = substr(location_doe, 4, nchar(location_doe))) %>%
  
  ggplot(aes(y=variable, x=location_doe)) + 
  facet_grid(pollutant~freq, scales="free", space = "free_y") + 
  geom_bin2d(aes(fill=count)) + 
  
  labs(title =  "Available regulatory data")

```

* check that minute and hour data are similar since only hourly data are validated by DOE

```{r}
#compare minute and hour data
df <- doe %>%
  select(-import_id) %>%
  #filter(freq %in% c("minute", "hour")) %>%
  #remove minute time stamp from minute data
  mutate(time = ymd_h(format(time, format = "%Y-%m-%d %H")))  
  
df1 <- df %>%
  filter(freq=="hour") %>%
  rename(hour_val = value) 

df2 <- df %>%
  filter(freq=="minute") %>%
  select(location_doe, time, variable, value) #%>% View()

df <- left_join(df1, df2) %>%
  select(-freq) %>%
  drop_na()
  
```

```{r}
df %>%
  mutate(diff = value - hour_val) %>%
  group_by(pollutant, variable) %>%
  summarize(
    min = min(diff),
    median = median(diff),
    max = max(diff)
  ) %>%
  kable(caption = "differnece between minute and hourly readings (minute-hour value)") %>%
  kable_styling()
  
  
```

* there is a lot more variability in the minute data compared to the hourly data. This could be real, or it could be noise.    
* [not shown in plot] smooth line is directly over the 1-1 line, indicating that minute data are in agreement with hourly data    

```{r}

## #takes forever to run
# df %>%
#   #takes forever to run
#   filter(pollutant %in% c("BC", "NO2", "Neph")) %>%
#   
#   ggplot(aes(x=hour_val, y=value, col=location_doe)) + 
#   facet_wrap(~pollutant, scales="free") + 
#   geom_point(alpha=0.05) +
#   geom_smooth() +
#   geom_abline(slope = 1, intercept = 0) + 
#   #coord_fixed() +
#   
#   labs(x="Validated hourly value",
#        y = "Unvalidated minute value"
#        )

df %>%
  #pollutants with true minute values
  filter(pollutant %in% c("BC", "NO2", "Neph")) %>%
  
  ggplot(aes(x=hour_val, y=value)) + 
  #facet_grid (location_doe~pollutant, scales="free") + 
  facet_wrap_equal(pollutant~location_doe, scales="free") + 
  geom_bin2d() +
  geom_abline(slope = 1, intercept = 0, linetype="dashed", aes(col="1-1")) + 
  #geom_smooth(aes(col="loess")) +
 
  labs(x="Validated hourly value",
       y = "Unvalidated minute value"
       ) 

```


observations    
* 10W and BH had PM2.5 from different sources over the course of the year 

```{r}
#check that there is sufficient data in the minute data 
doe %>%
  filter(freq == "minute") %>%
  distinct(location_doe, variable, pollutant, date) %>%
  
  filter(!variable %in% drop_doe_vars) %>%
  
  ggplot(aes(x=date, y=variable)) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) + 
  
  labs(title = "Available minute data during the study period")

  
```

* note, most are FEM methods (not nephs; also, not all bam methods are FEM - e.g., doe_bam_pm25?)    
* there is some overlap in instrument readings. This is probably done on purpose by the PSCAA in order to calibrate readings from different instruments. We will drop instrument reading as soon as another FEM method starts sampling.    
* BC readings at 10W are missing for ~4 months from around May-Aug 2019



```{r, eval=F}
# don't need to show this. next fig is similar but updated 

doe %>%
  filter(freq == "minute") %>%
  distinct(location_doe, variable, pollutant, date) %>%
   
  filter(
    #these sites already have full neph data 
         !(location_doe %in% c('AQSD', 'AQSK', 'AQSTUK') & variable %in% c('doe_bam_pm25', 'doe_pm25_fem', 'doe_pm25_fem_bam', 'doe_pm25_fem_teo') )
         
    
  ) %>%
  
  ggplot(aes(x=date, y=variable)) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) + 
  
  labs(title = "Minute data to be used to calculate anual averages")

```

```{r, eval=F}
# when does sampling start/stop or sites with multiple instruments measuring PM2.5?
doe %>%
  filter(freq == "minute",
         location_doe %in% c('AQS10W', 'AQSBH'),
         pollutant == "PM2.5"
         ) %>%
  group_by(location_doe, variable) %>%
  summarize(
    start_date = min(date),
    end_date = max(date)
  ) %>%
  arrange(location_doe, start_date)
 
```

```{r}
doe2 <- doe %>%
  filter(
    # hour/day estimates may be largely aggregates of minute data 
    freq == "minute",
    
    #these sites already have full neph data
    !(location_doe %in% c('AQSD', 'AQSK', 'AQSTUK') & variable %in% c('doe_bam_pm25', 'doe_pm25_fem', 'doe_pm25_fem_bam', 'doe_pm25_fem_teo') ),
    
    # a diff FEM method starts sampling at both 10W and BH at this time
    !(variable == "doe_pm25_fem" & date > ymd("2019-04-01") ),
    # a diff ?FEM method starts sampling at BH 
    !(variable == "doe_pm25_fem_teo" & date > ymd("2019-11-25") & location_doe == 'AQSBH'),
    
  ) 

```

* dropped overlapping data at 10W and BH: 

```{r}
#check that things look right # looks right

## plot
doe2 %>%
  distinct(location_doe, variable, pollutant, date) %>%
  
  ggplot(aes(x=date, y=variable
             #y=pollutant
             )) + 
  facet_grid(pollutant~location_doe, scales="free", space = "free_y") + 
  geom_point(alpha=0.1) +
  
  labs(title = "Data used to estimate annual averages at AQS sites")


# # no need to show this
# ## table
# doe2 %>%
#   filter(freq == "minute",
#          location_doe %in% c('AQS10W', 'AQSBH'),
#          pollutant == "PM2.5"
#          ) %>%
#   group_by(location_doe, variable) %>%
#   summarize(
#     start_date = min(date),
#     end_date = max(date)
#   ) %>%
#   arrange(location_doe, start_date) %>%
#   kable(caption = "PM2.5 readings used to calculate an annual average at 10W and BH since multiple instruments were used over the course of the ACT TRAP study period") %>%
#   kable_styling()

```

```{r}
doe2 <- doe2 %>%
  #rename each pollutant regardless of exact measurement instrument used
  group_by(location_doe, location, date, time, pollutant) %>%
  summarize(value = mean(value, na.rm = T)) %>% 
  ungroup()

```

observations per instrument 

* looks good. We have ~1 reading/minute at each site

```{r}
doe2 %>%
  group_by(pollutant, location_doe) %>%
  summarize(
    total_samples = n(),
    samples_per_hour = total_samples/as.numeric(difftime(max(date), min(date), units="hour"))
  ) %>%
  kable(caption = "Number of samples per site and pollutant used to estimate annual averages", digits = 0) %>%
  kable_styling()
  
```





# Instrumentation

Instrument notes

* extreme instrument readings outside the range. Individual spikes may indicate poor instrument performance, but sustained spikes may be true though they may have reduced accuracy. Taking median stop concentrations and dropping median concentrations outside the range of the instrument readings should address these ideas?

  - from Tim G: "Not only the magnitude of the reading matters, but also the duration of the extreme value readings.  Instrument readings that are outside the manufacturers' rated range are best omitted from the data set, especially if they are one-time spikes.   A sustained extreme reading suggests that a high level of the pollutant has been detected but the quantity may not be very reliable.  You might consider replacing the numeric levels that exceed the manufacturers' rated maximum with that maximum number for sustained exceedances, on the premise that the device isn't made to measure higher than the stated level."

* The legacy NO2 CAPS monitor used at the start of the study does not have automatic baseline adjustment so would be reset on a weekly basis instead of occurring a few times during the monitoring day as with the newer fast-response unit.

* There were no clock issues with either NanoScan.  The clock issues were primarily with the P-Traks, but the resetting of internal clock before every drive kept the clock drift to a minimum.  After ~6 hrs, the clock for the instrument might be off by 2 to 3 sec. 

```{r}
instrument_range <- dt0 %>%
  distinct(variable) %>%
  mutate(
    Min = c(0, 0, 0, 10^3, 0, 10^2, 0, 0),
    #no max for neph
    Max = c(5e3, 2e3, 5e5, 10^6,NA, 10^6, 10^6, 5e5),
    Units = c("ppm", "ppb", "pt/cm3", "pt/cm3", "bscat/m", "pt/cm3", "ng/m3", "pt/cm3")
    
  )
  
```

Expected instrument values

```{r}
instrument_range %>%
  kable(caption = "Instrumet measurement ranges. Note, nephs don't have a max.") %>%
  kable_styling()

```

```{r}
"from Tim Gould, based on the highway-oriented measurements in the ACT-TRAP study. Some higher concentration levels from vehicle sources or other elevated pollution events are possible.  The ranges are a good indicator but any outlier should not automatically be discarded.

Expected range of measurements by instrument
BC:   0 - 15,000 ng/m^3
CO2:  410 - 600 ppm
NO2:  2 - 100 ppb
neph.:  0.1 e-5 to 7.5 e-5 /m
NanoScan particle total:  1500 to 80,000 pt/cm^3
P-Trak particle count:  500 to 70,000 pt/cm^3"

```

data availability plot

```{r}
#plot 
dt0 %>%
  distinct(date, variable, instrument_id) %>%  
  
  ggplot(aes(x=date, y=instrument_id, col=variable)) + 
  geom_point()

```

```{r}
#tables
dt0 %>%
  distinct(date, variable, #instrument_id
           ) %>%
  group_by(variable, #instrument_id
           ) %>%
  summarize(
      sampling_days = n(),
      start_date = min(date),
      end_date = max(date)
    ) %>%
  add_row(variable="OVERALL MEAN", sampling_days=mean(.$sampling_days)) %>%
  kable(caption = "Total instrument sampling", 
        digits = 0
        ) %>%
  kable_styling()

```

# Quality Assurance 

## Calibrations

* I think the "level" column is what the instrument read, while the conc column is the true concentration during a calibration procedure. 
* pm25 is nephelometer readings 

* **NOTE**: don't use the "analyte" column, which is wrong for some instruments (Amanda confirmed this). Use instrument_id column.

```{r}
# NOTE: don't use "analyte" column, which is wrong for some instruments??

calibrations %>%
  #one record per calibration attempt
  distinct(date, instrument_id) %>%  
  #number of times each instrument was calibrated
  group_by(instrument_id) %>%
  summarize(
    number_of_calibrations = n(),
    dates = paste(date, collapse = ", ")
  ) %>%
  kable(caption = "Number of gas instrument calibrations (incluing nephelometers [PM25]) and particle instrument zero checks during the study period") %>%
  kable_styling()
  
```


## Zeroing for particle intruments
notes   
* notes field does not clarify exact time when the particle filter was placed/removed from the instruments. You can typically see instrument readigns dip for a period of time before going back up though (e.g., 2019-10-22).

observations      

* there are no notes in the calibration table for 5/6 to explain why PMPT_94 did not read anywhere near 0 if a zeroing activity occurred. The filter could have not been placed on the inlet?

* The filter may not have been placed correctly/on time on 2019-05-06?

```{r}

# NOTE: don't use "analyte" column, which is wrong for some instruments??

calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|"))),
                    !str_detect(instrument_id, "PM25")
                    ) %>% 
  
  ggplot(aes(x=time, y=value, col=instrument_id, shape=instrument_id)) + 
  scale_shape_manual(values = c(1:length(unique(calibrations$instrument_id)))) +
  facet_wrap(~date, scales="free") + 
  geom_point(alpha=0.5) + 
  geom_hline(yintercept = 0, linetype="dashed")

  
  
# dates
calibrations %>%
  # look at particle instruments
  filter(str_detect(instrument_id, paste0("^", paste(pt_pollutants, collapse = "|"))),
                    !str_detect(instrument_id, "PM25")
                    ) %>%  
  group_by(date, #instrument_id
           ) %>%
  summarize(
    instruments = paste(unique(instrument_id), collapse = ","),
    #notes = paste(unique(notes), collapse = "," )
  ) %>%
  kable(caption = "zero check dates and instruments. The notes field does not indicate the exact filter placement/removal time from instruments (when instruments should have read '0')") %>%
  kable_styling()
  
  
  
    # #some instruments had duplicate files in one day
  # distinct(instrument_id, date) %>% 
  # group_by(instrument_id) %>%
  # summarize(
  #   number_of_zeros = n(),
  #   dates = paste0(unique(date), collapse = ", ")
  #   
  # ) %>%
  # kable(caption = "when zero checks for particle instruments occurred") %>%
  # kable_styling()

```








```{r}
# Instrument Time Trends w/ All Data

```

```{r}
### --> delete here and only do at  2-min level?
### --> see manuals?
```


notes   
* see how values change as a sampling day goes on (e.g., see NO2 isues?)
* plotting hours as a continous variable   
* note: this is only using stop data 

observations       
* see some instrument sensitivity over time (e.g., CO2 sensitiviy early on)?
* changes over time could occur b/c: a) different sites; b) changing AP levels; c) instrument issues   
* backup instruments typically have a lot less data (so more variable)


```{r, fig.height=12, eval=F}

p <- list()

#give each facet its own instrument_id legend. make plot in loop b/c various things are done to df before plotting. 

for(i in seq_along(ap)) {
  #i=1
  
  p[[i]] <- dt0 %>%
    
    filter(variable == ap[i],
           #location != "MS0000"
           ) %>%
    
    group_by(instrument_id, runname) %>%
    #calculate time since start of run
    mutate(time = as.numeric(difftime(time, min(time), units = "hours"))) %>% #View()
    
    #drop roosevelt after use it ot calculate start time
    filter(location != "MS0000") %>%
    
    ggplot(aes(x=time, y=value, col=instrument_id, 
               linetype=primary_instrument
               )) +
    geom_smooth() +
    facet_wrap(~variable, scales="free") +
    
    # theme(legend.justification=c(0,0), legend.position=c(0,0),
    #       #legend.key = element_blank(), #legend.key.size = 1
    #       ) +
     
    labs(x= "hours since departing Roosevelt garage",
         col="",
         linetype = ""
         )  
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)

```

same as above but by stop order number (note, some routes may take a while to get to first stop, e.g., route 8)



```{r, fig.height=12, eval=F}
### --> error: plots r empty 

 
#plot with all the data
p0 <- dt0 %>%
    ggplot(aes(x=location, y=value, col=instrument_id, 
               linetype=primary_instrument
               )) +
   geom_boxplot() + 
  #geom_smooth() +
    facet_wrap(~variable, scales="free") +
    
    #theme(legend.justification=c(0,0), legend.position=c(0,0)) +
    
    labs(x= "stop order number within a run",
         col="",
         linetype = ""
         )

p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1
  
  #only plot a subset of the data
  p[[i]] <- p0 %+% subset(dt0, variable == ap[i])

  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)


```


 


# Fieldnotes and instrument warnings 

### --> see if instrument errors are associated w/ poor instrument collocations

### -> have Kaya code these and ID things that need attention? how to do this efficiently?

notes    
* see QAQC.html doc that Amanda and Kaya have worked on 
  * most instrument error codes don't have "use" indicators to clarify whether that data should be used or not though (as of 5/11/21)     

*e.g., alcohol wick issues 

```{r}
fieldnotes2 <- fieldnotes %>%
  #only look at stop data w/ notes
  filter(
    str_detect (site_id, regex("^MS|MC", ignore_case = T)),
    !is.na(notes),  
    !notes %in% c("", ".", "\\")
  )


```



# Summary of all raw data 

### --> drop instrument error messages? 

```{r}

```


```{r}
# note: doing this at the stop level. otherwise PMPTSCREEN_94 has some low values that don't correlate wel w/ PMPTSCREEN_3

low_pt_val <- 300

dt <- dt0 #%>%
  # filter(
  #   #drop particle readings of 0 since these are often associated w/ alcohol wick issues (not inlcuding neph)
  #   !(grepl("PM", instrument_id) & !grepl("PM25", instrument_id)  & value <= low_pt_val)
  #  )  
  
```

```{r, eval=F}
print("proportion of data dropped")

(nrow(dt0) - nrow(dt))/nrow(dt0) # ~0.04%

```

```{r}
print("total raw, second-level obsevations")

nrow(dt)

```

 
```{r,}
#table 

dt %>%
  #drop Roosevelt Garage
  filter(location != "MS0000") %>%
  
  group_by(instrument_id) %>%
  summarize(
    no_readings = n(),
    Min = min(value),
    # q01 = quantile(value, 0.01),
    Q02.5 = quantile(value, 0.025),
    Q25 = quantile(value, 0.25),
    Q50 = quantile(value, 0.50),
    Q75 = quantile(value, 0.75),
    Q97.5 = quantile(value, 0.975),
    # q99 = quantile(value, 0.99),
    Max = max(value),
  ) %>%
  # scientific notation w/ few sigfigs 
  mutate_at( vars(-no_readings), ~format(., scientific=T, digits = 3) ) %>%
  
  kable(caption="raw instrument readings",
        ) %>%
  kable_styling()

```

* note, some instruments have very high, abnormal readings
  - e.g., neph 176

```{r, fig.height=12}
#density plots 

p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  df <- dt0 %>%
    #drop Roosevelt Garage
    filter(location != "MS0000",
           variable== ap[i]) #%>% 
  ##example - look at a subset of data. for initial plotting
    #filter(sample(c(TRUE, FALSE), size = nrow(.), prob = c(0.1, 0.9), replace = T))
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    #guides(linetype = FALSE) + 
    labs(title = "raw instrument readings with min/max annotated",
         linetype = "Instrument",
         col = "ID"
         )
   
    
  
  #p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)

```


# Calculate Site medians (for each instrument)

 
- don't include lab data here? 

```{r}
#note: on rare instantces when duplicate instruments were on board, all the data are used to calculate a median
dt_stops <- dt %>%
  filter(location != "MS0000") %>%
  #calculate arrival time at any stop
  group_by(runname, date, location) %>%
  mutate(time = min(time)) %>%  
  
   group_by(runname, date, time, location, site, instrument_id, variable, primary_instrument) %>%
  summarize(
    #mean_value = mean(value),
    value = median(value)
  ) %>%
  ungroup()

```


# Summary of stop-level data 

notes    
* boxplots are not traditinal. they show specific quantiles 

 
```{r, fig.height=12, eval=F}
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  p[[i]] <- dt_stops %>%
  
  filter(
    ##drop Roosevelt Garage
    #location != "MS0000",
         variable== ap[i]
         ) %>%
    
    #calc stats for alternative boxplots
    group_by(instrument_id, variable, primary_instrument) %>%
    alt_boxplot(var = "value") %>%  
    
    ggplot(aes(x=instrument_id, #col=instrument_id, 
               linetype=primary_instrument),) + 
    geom_boxplot(aes(ymin=Qmin, lower=Q25, middle=Q50, upper=Q75, ymax=Qmax), stat = "identity") + 
    
    facet_wrap(~variable, scales="free") +
    
    labs(linetype="Instrument", col="ID")
  
  #p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)


```


* labels are for the min/max reading of each instrument 

```{r, fig.height=12, eval=T}
 
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=1

  #only plot a subset of the data
  df <- dt_stops %>%
            ##drop Roosevelt Garage
    filter(#location != "MS0000",
           variable== ap[i])  
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    guides(linetype = FALSE)
   
    # don't do since we have values <=0 
    #scale_x_log10()
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4)


```



amount of data outside of expectd instrument ranges
   
* out of range date is more likely to be below the minimum instrument range. Removing all values outside the instrument range could thus bias overall median slightly up (this was more true when we ran this at the raw second level, where ~16% of BC data was below the min)

```{r}
dt_stops %>%
  #observations per pollutant
  group_by(variable, #instrument_id
           ) %>%
  mutate(n_original = n()) %>%
  ungroup() %>%
  
  left_join(instrument_range) %>%
  #filter(value >= Min, value <= Max) %>% #View()
  mutate(below_min = value < Min,
         above_max = value > Max
         ) %>%
  
  group_by(variable, #instrument_id
           ) %>%
  summarize(
    n_original = unique(n_original),
    n_below_min = sum(below_min),
    n_above_max = sum(above_max),
    
    prop_below_min = mean(below_min),
    prop_above_max = mean(above_max),
    total_prop_out_of_range = sum(prop_below_min, prop_above_max, na.rm = T)
  ) %>%
  kable(caption = "proportion of data outside the instrument range. N = number of stops (~309 stops x ~29 visits/stop)", digits = 2) %>%
  kable_styling()


```

stop estimates when values outside the range of data are dropped

```{r, fig.height=12}
p <- list()

#give each facet its own instrument_id legend
for(i in seq_along(ap)) {
  #i=3

  #only plot a subset of the data
  df <- dt_stops %>%
    left_join(instrument_range) %>% 
    
    #neph has no max. need a number to avoid an error
    mutate(Max = ifelse(is.na(Max), 9999999, Max)) %>% #View()
    
    filter( 
           variable== ap[i],
           #drop values outside the expected instrument range
           value >= Min, value <= Max 
           ) 
    
  #summary table for labels
  df_sum <- df %>%
    # add min/max labels
    group_by(primary_instrument, instrument_id) %>%
    summarize(
      min = min(value),
      max=max(value)
      )  %>%
    #make long format to avoid overlapping labels later
    gather(kind, value, min,max)
    
  p[[i]] <- df %>% 
    
    ggplot(aes(x=value, col=instrument_id, linetype=primary_instrument),) + 
    geom_density() + 
    facet_wrap(~variable, scales="free") +
    
    geom_text_repel(data=df_sum, aes(x=value, y=0, label=value), show.legend = F ) +
     
    #theme(legend.justification=c(1,1), legend.position=c(1,1)) + 
    #don't show linetype in legend 
    guides(linetype = FALSE)
   
    # don't do since we have values <=0 
    #scale_x_log10()
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4) %>%
  annotate_figure(top = "Stop medians when stops with values outside an instrument range are dropped")

```

## Update data

* **drop median estimates outside the range of each instrument's reporting range**    
* drop NanoScan and non-screened PTRAK values < 300 pt/cm3   
* drop screened PTRAK values < 100 pt/cm3. there is one value of 2.0. All other readings are > 100 pt/cm3.   

### --> ? not doing currently: drop BC readings equal to 0 ng/m3 (~7 estimates). This doesn't happen in real life, but other instruments could have also had "0" readings

 

```{r}
min_tot_ufp <- 300
# PMPTSCREEN_94 has 1 reading of 2. All other readings are > 100
min_screen_ufp <- 100

dt_stops2 <- dt_stops %>%
    left_join(instrument_range) %>% 
    #neph has no max. need a number to avoid an error
    mutate(Max = ifelse(is.na(Max), 9999999, Max)) %>% #View()
    filter(
      #drop values outside the expected instrument range
      value >= Min, value <= Max 
           )  %>%
  #drops 3 UFP values < 300
  filter(
    !(variable %in% c("ns_total_conc", "pnc_noscreen") & value < min_tot_ufp ),
    !(variable %in% c("pnc_screen") & value < min_screen_ufp ),
    # #drop BC readings = 0
    # !(variable %in% c("ma200_ir_bc1") & value == 0),
    )
  

#keep approved stop data, but see raw values
dt2 <- dt_stops2 %>%
  distinct(date, location, instrument_id) %>%
  left_join(dt)

```


# Time series of when the lowest and highest values were observed

* as an example, we're just looking at the lowest and highest readings here

* pulling out the second data used to calculate site medians to better inspect the instrument patterns for possible errors

```{r}

#create standardized values & relabel UFP instruments
dt2_temp <- dt2 %>%  
    #scale values using all the data 
    group_by(variable) %>%
    mutate(value = scale(value)) %>%
  ungroup() %>%
  mutate(
    #simplify plots
    variable = ifelse(variable %in% c("ns_total_conc", "pmdisc_number", "pnc_noscreen", "pnc_screen"), "UFP", variable)
    )

```

**highest (max) readings**

```{r}
# pull out stops when we saw max values
extreme_high <- dt_stops2 %>%
  group_by(variable) %>%
  mutate(median_overall_value = median(value)) %>%
  filter(value == max(value)) %>% 
  #ungroup() %>%
  distinct(date, location, instrument_id, value, median_overall_value) %>%
  group_by(date, location) %>%
  summarize(instrument_id = paste0(unique(instrument_id), collapse = "|"),
            median_stop_value = paste0(unique(value), collapse = "|"),
            median_overall_stop_value = paste0(unique(median_overall_value), collapse = "|")
            )
  
extreme_high %>%
  kable(caption = "stops where the max instrument value has been observed") %>%
  kable_styling()
```

* high instrument observations tend to occur when other particle readings are also high, particularly for particle instruments

* CO2_19 (backup) has much higher readings than co2_14 (primary) on 6/13 and higher readings than other CO2 readings.     
  * **drop all co2_19 readings?** co2_19 also recorded the highest co2 readings in the dataset on the same day at different stops. CO2_19 was never used on its own.

* BC on 2/10 stands out here, but other particle instruments are also increasing though less so
* **keep NO2_2 on 5/11 since otehr instruments are also high?**


```{r, fig.height=10}
df <- data.frame()

for (i in 1:nrow(extreme_high)) {
  #i=1
  
  temp <-  dt2_temp %>%
    filter(date == extreme_high$date[i] & location == extreme_high$location[i]) %>%
    mutate(
      #pt_size = ifelse(grepl(extreme_high$instrument_id[i], instrument_id), 1, 1),
      extreme_instrument = ifelse(grepl(extreme_high$instrument_id[i], instrument_id), instrument_id, NA),
           ) 
    df <- bind_rows(df, temp)
  
}
  
temp_labels <- df %>%
  drop_na() %>%
  
  group_by(date, location) %>%

  mutate(time = min(time),
         value = max(value)
         ) %>%
  ungroup() %>%
  
  group_by(date, location, time, value) %>%
  summarize(
    extreme_instrument = paste0(unique(extreme_instrument), collapse = ", ")
  )
  
  df %>%  {
    
    ggplot(., aes(x=time, y=value)) + 
    
      #black points make it easier to ID questionnable data
      geom_point(data=subset(., instrument_id==extreme_instrument), size=2) +
      
      geom_point(aes(shape=instrument_id, col=variable)) +
      geom_text(data = temp_labels, aes(label= extreme_instrument), 
               hjust=0, show.legend = F) +
    
      facet_wrap(date~location, scales = "free") +
      scale_shape_manual(values = c(1:length(unique(dt_stops$instrument_id)))) +
      
      #don't show size in legend 
      guides(size = F) + 
      
      labs(y = "Standardized Value: (X-mean)/SD",
           title = "Plots of when high instrument readings occur"
           )
  }
  
```

**lowest (min) readings**

* there are more instances with min readings than max readings

```{r}
# pull out stops when we saw max values
extreme_low <- dt_stops2 %>%
  group_by(variable) %>%
  filter(value == min(value) #| value == min(value),
         ) %>% 
  distinct(date, location, instrument_id) %>%
  group_by(date, location) %>%
  summarize(instrument_id = paste0(unique(instrument_id), collapse = "|"))
  
extreme_low %>%
  kable(caption = "stops where the min instrument value has been observed") %>%
  kable_styling()
```

* nothing stands out as being unordinaryly low? 

* BC tends to have low readings 

```{r, fig.height=20}

df <- data.frame()

for (i in 1:nrow(extreme_low)) {
  #i=1
  
  temp <-  dt2_temp %>%
    filter(date == extreme_low$date[i] & location == extreme_low$location[i]) %>%
    mutate(
      #pt_size = ifelse(grepl(extreme_low$instrument_id[i], instrument_id), 1, 1),
      extreme_instrument = ifelse(grepl(extreme_low$instrument_id[i], instrument_id), instrument_id, NA),
           ) 
    df <- bind_rows(df, temp)
  
}
  
temp_labels <- df %>%
  drop_na() %>%
  ungroup() %>%
  group_by(date, location) %>%

  mutate(time = min(time),
         value = max(value)
         ) %>%
  
  group_by(date, location, time, value) %>%
  summarize(extreme_instrument = paste0(unique(extreme_instrument), collapse = ", "))
  
  df %>%  {
    
    ggplot(., aes(x=time, y=value)) + 
    
      #black points make it easier to ID questionnable data
      geom_point(data=subset(., instrument_id==extreme_instrument), size=2) +
      
      geom_point(aes(shape=instrument_id, col=variable)) +
      geom_text(data = temp_labels, aes(label= extreme_instrument), 
               hjust=0, show.legend = F) +
    
      facet_wrap(date~location, scales = "free") +
      scale_shape_manual(values = c(1:length(unique(dt_stops$instrument_id)))) +
      
      #don't show size in legend 
      guides(size = F) + 
      
      labs(y = "Standardized Value: (X-mean)/SD",
           title = "Plots of when low instrument readings occur"
           )
        
      }
```


## Update data

* drop co2_19 (backup instrument) readings for a day when these readings were much higher than other collocated instrument and other days 

```{r}

dt_stops3 <- dt_stops2 %>%
  filter(
    #drop co2_19 (backup instrument) readings for a day when these readings were much higher than other collocated instrument and other days 
    !(date ==extreme_high$date[3] & location== extreme_high$location[3] & instrument_id==extreme_high$instrument_id[3])
  )

```


# Collocated duplicate instruments

```{r}
#header names in both sources of data: stop and room data 
common_names <- intersect(names(dt_stops3), names(room_colo))

dt_stops3_colo <- dt_stops3 %>%
  #these are never collocated
  filter(!instrument_id %in% c("PMPTSCREEN_2", "PMPTSCREEN_93")) %>%
  select(common_names) %>%
  rbind(room_colo[common_names]) 

dt_stops3_colo_wide <- dt_stops3_colo %>%  
  #select(-primary_instrument) %>%
  spread(instrument_id, value)

# dt_wide <- dt %>%
#   spread(instrument_id, value)
  
```

* note, have to modify code if we ever have > 2 collocated duplicate instruments (e.g. if we keep all the PMPTSCREEN data above)

```{r}
df <- data.frame()

#only keep stop data w/ collocations
colo3 <- dt_stops3_colo %>%
  group_by(time, site, variable) %>%
  filter(length(unique(instrument_id)) >1 ) %>% 
  ungroup() %>%
  #make sure this is arranged by time
  arrange(time)
  #select(-primary_instrument)

for(i in seq_along(ap)) {
  #i=1
  
  temp <- colo3 %>%
    filter(variable == ap[i]) %>%
    group_by(variable) %>% #View()
    summarize(
      n_pairs = n()/2,
      
      R2_reg = cor(
        .$value[.$instrument_id == unique(.$instrument_id)[1] ],
        .$value[.$instrument_id == unique(.$instrument_id)[2] ]
                   )^2,
      R2_mse = r2_mse_based(
        .$value[.$instrument_id == unique(.$instrument_id)[1] ],
        .$value[.$instrument_id == unique(.$instrument_id)[2] ]
        ),
      RMSE =rmse(
        .$value[.$instrument_id == unique(.$instrument_id)[1] ],
        .$value[.$instrument_id == unique(.$instrument_id)[2] ]
        )
      ) %>%
    mutate_at(vars(contains("R2")), ~round(., 2)) %>%
    #note: neph gets 0 b/c values are small
    mutate(RMSE = round(RMSE, 0)
        
    )  
  
  df <- rbind(df, temp)
  
}

df %>%
  kable(caption = "Agreement between collocated duplicate instruments. Based on site medians", 
        #digits = 2
          ) %>%
  kable_styling()

```



* BC looks good 
* **CO2 has a poor fit w/ the backup instrument**, which tends to report high readings

  * drop co2_19, especially since it is never used on its own?
* NO2 looks good 
* nephs look good
* discminis look good

* PTRAKS look good
* screened P-TRAKS look good    
  * only these 2 screened PTRAKS were collocated.

* **PMSCAN 3 has lower readings in 2020, but readings look OK for 2019**

* PMPT 94 seems to have higher readings than PMPT 93 for 6 seconds. 94 readings are higher than surrounding readings, so this instrument is a) wrong, or b) placed near a source for a few seconds. There's no documentation for this?

```{r, eval=F}
# dt_stops3_colo_wide %>%
#   drop_na(PMPT_93, PMPT_94) %>%
#   filter(grepl("lab", site, ignore.case = T),
#          runname == "2019-04-18_LABC",
#          time < "2019-04-18 13:00:00"
#          
#          # PMPT_93 > 2000 & PMPT_93 < 5000,
#          # PMPT_94 > 4000 & PMPT_94 <10000
#          ) %>% 
#   select(runname:variable, contains("PMPT_"),) %>% View()

```


```{r, fig.height=18}

p <- list()

p[[1]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "BC_0066", y.variable = "BC_0063", col.by = "site", 
            slope_digits = 2, alpha_value = 0.7) + 
  labs(col="Route")


p[[2]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "CO2_14", y.variable = "CO2_19", col.by = "site", alpha_value = 0.7
            ) + 
  labs(col="Route")

p[[3]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "NO2_2", y.variable = "NO2_1", col.by = "site", 
            slope_digits = 0, int_digits = 2, alpha_value = 0.7) + 
  labs(col="Route")

p[[4]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PM25_176", y.variable = "PM25_205", col.by = "site", int_digits = 6, rmse.digits = 6, alpha_value = 0.7) + 
  labs(col="Route")

p[[5]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMDISC_3", y.variable = "PMDISC_8", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

#that these are the only 2 non-screeend PTRAKS that were collocated 
p[[6]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMPT_94", y.variable = "PMPT_93", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

# that these are the only 2 screeend PTRAKS that were collocated 
p[[7]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMPTSCREEN_94", y.variable = "PMPTSCREEN_3", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

p[[8]] <- dt_stops3_colo_wide %>%
  colo.plot(x.variable = "PMSCAN_5", y.variable = "PMSCAN_3", col.by = "site", alpha_value = 0.7) + 
  labs(col="Route")

ggarrange(plotlist = p, ncol = 2, nrow = 4) %>%
  annotate_figure(top = "Instrument Collocations")

```


* PMSCAN_3 looks fine before 2020. We'll only keep the data collected during 2019, which will allow us to use data collected with this instrument a few days before PMSCAN_5

```{r}
dt_stops3_colo_wide %>%
  filter(year(date) < "2020"
         #date < "2019-04-01"
         ) %>%
  colo.plot(x.variable = "PMSCAN_5", y.variable = "PMSCAN_3", col.by = "runname", alpha_value = 0.7, mytitle = "Collocated NanoScan data during 2019 (excludes 2020)") 
   
```

## Collocation instrument time trends

*see if duplicate instruments behave differenlty over time when in exact same place 

* still see some trends related to how long instruments are run (is this real?)   
 
```{r}

colo <- data.frame()

for (i in seq_along(instruments)) {
  #i=1
  
  colo0 <- dt_stops3_colo_wide %>%
    # #these were never collocated w/ the others
    # select(-PMPTSCREEN_2, -PMPTSCREEN_93) %>% 
    
    select(date, time, #location, 
           site, variable, contains(paste0(instruments[i], "_" ))) %>%
    drop_na() %>%
    gather("instrument_id", "value", contains(instruments[i])) 
  
  if(i==1) {
    colo <- colo0
    } else
      colo <- rbind(colo, colo0)  
  
}
  
colo <- colo %>%
    mutate(
      primary_instrument = ifelse(instrument_id %in% primary_instruments, "Primary", "Backup"),
      primary_instrument = factor(primary_instrument, levels = c("Primary", "Backup"))
    )

#unique(colo$instrument_id)

```

* **things look good? would expect concentrations to increase as the day goes on if you depart early on, and the opposite to happen if you start in the afternoon**

* pnc_screen only show a trend line for a backup instrument on 2019-03-22, though these were two backup instruments (PMPTSCREEN_3 and PMPTSCREEN_94)

```{r, fig.height=12}
p <- list()

#give each facet its own instrument_id legend. make plot in loop b/c various things are done to df before plotting. 

for(i in seq_along(ap)) {
  #i=2
  
  p[[i]] <- colo %>%
    
    filter(variable == ap[i],
           #only use rout data
           grepl("R0", site)
           ) %>% #View()
    
    group_by(variable, #instrument_id, 
             date) %>%
    #calculate time since start of run
    mutate(first_stop = min(time),
           time = as.numeric(difftime(time, first_stop, units = "hours")),
           
           ) %>%  # View()
    
    ggplot(aes(x=time, y=value, 
               #col=factor(date),
               #col=instrument_id, 
               linetype=primary_instrument)) +
    facet_wrap(~variable, scales="free") +
    geom_smooth(aes(col=factor(first_stop),), se=F,
                alpha=0.8
                ) +
    geom_smooth(col="black") +
    
    # geom_point(aes(col=factor(date)), alpha=0.8) + geom_line(aes(col=factor(date)), alpha=0.8) +
    # geom_point() + geom_line() +
    
    labs(#title = "collocated instrument readings",
         x= "hours since first stop",
         col="1st Stop Time",
         linetype=""
         )  
  
  p[[i]]
  
  }

ggarrange(plotlist =  p, ncol = 2, nrow = 4) %>%
  annotate_figure(top = "collocated instrument readings. black = overall trend")

```


# Update Data 

**Stop level data**

* dropped stop medians outside each instrument's reporting range b/c these readings are less trustworthy/accurate (kept readings at the second level since we will mainly be dealing w/ stop medians)

* Dropped all co2_19 readings b/c    
  * this instrument tends to report high values
  * its values were higher than CO2_14 when they were collocated
  * it was a backup instrument that was never used on its own

* dropped SCAN 3 after 2019 when collocation readings looked different than SCAN 5
  * we collected a few extra days of data with this instrument at the begining of the campiagn   
  * SCAN 3 (Edmund's) was an older instrument    

* if duplicate instruments were on the platform, calculate their median value

**Second-level data**

* see above for inclusion criteria   
* note: extremes are still included in these data if stop medians were within an instrument's range

```{r}
dt_stops4 <- dt_stops3 %>%
  #don't need instrument ranges anymore
  select(-c(Min, Max, Units)) %>%
  filter(
    #drop all CO2_19 data
    !instrument_id %in% c("CO2_19"),
    #drop 2020 PMSCAN_3 data after 2019
    !(instrument_id == "PMSCAN_3" & year(date) > "2019"),
         ) %>%
  # #if multiple instruments on platform, use the median of the two medians
   group_by_at(vars(-c(instrument_id, primary_instrument, value))) %>%
   summarize(value = median(value)) %>%
  ungroup()
  

# note, this dataset could still have some extreme values (this hasn't been vetted as much as stop data)
dt3 <- dt2 %>%
  filter(
    #drop all CO2_19 data
    !instrument_id %in% c("CO2_19"),
    #drop 2020 PMSCAN_3 data after 2019
    !(instrument_id == "PMSCAN_3" & year(date) > "2019"),
         )
  
```


# Instrument time trends (all data)

* things look fine?    
  * e.g., don't see extreme NO2 trends      
* using log10 y scale since there are some extreme values 


```{r, fig.height=12}

dt_stops4 %>%
  #have to group by runname (date and route) b/c some runs ended at midnight and the grouping gets messed up otherwise
  group_by(variable,  runname) %>%
    #calculate time since start of run
    mutate(first_stop = min(time),
           time = as.numeric(difftime(time, first_stop, units = "hours")),
           ) %>%  
  {
    ggplot(., aes(x=time, y=value, 
               #col=instrument_id, 
               #linetype=primary_instrument
               )) +
      facet_wrap(~variable, scales="free") +
      geom_smooth(se=F, alpha=0.8) +
      
                        #dont plot extreme points - easier visualization
      geom_point(#data=filter(., value < quantile(value, 0.99)),
                 alpha=0.05) +
      #easier to visualize overall trend w/ extreme values
      scale_y_log10() +
    
    labs(title = "instrument readings over time (all remaining data)",
         subtitle = "trend lines use all data",
         x= "hours since first stop",
         linetype="Instrument"
         )  
}


```


# AQS sites 

Looking at the ACT TRAP field protocol, it looks like BC, NO2 and neph (also neph PM2.5) values are collected every minute, though these are not officially validated like the hourly data are. We will use these instead of hourly data since we want to compare these readings to our 2-minute collocations.

Note that BH and 10W don’t have minute data. BH has rolling 1-hour averages every 6 minutes for PM2.5 (teoms), while 10W seems to report the same thing every minute for about an hour (~58 min). 

We will have to use a mix of semi-hourly PM2.5 data and hourly data for BH and 10W since data comes from various non-minute-level sources.


-see if our campaign measurements were similar to those from AQS sites. Compare AQS:
  - 2-min medians
  - 2-min annual avg estimate
  - true annual avg w/ full data

```{r}
 
#stop collocations
dt_stops3_colo <- dt_stops4 %>%
  filter(
    #pollutants also measured at AQS sites
    variable %in% c("ma200_ir_bc1", "neph_bscat", "no2"),
    #only keep collocation sites
    grepl("MC", location)
    ) %>% 

  #drop seconds from time stamp (for DOE merging). change default tz of UTC to LA
  mutate(time = ymd_hm(format(time, format = "%Y-%m-%d %H:%M"), tz=mytz),  #tz=mytz
         #time = force_tz(time, tzone = mytz),
         ) %>%  #View()
  #if duplicate instruments, take mean value
  group_by(date, time, location, variable) %>%
  summarize(value = mean(value)) %>% 
  ungroup() %>%
  
  spread(variable, value) %>%
  arrange(time)  %>%
  
  #rename pollutants
  rename(BC = ma200_ir_bc1,
         Neph = neph_bscat,
         NO2 = no2
         )  %>%
  mutate(
    # put in same units as DOE
    ##change from ng/m3 to ug/m3
    BC = BC/10^3,
    ##change from bscat/m to (bscatx10^-4)/m
    Neph = Neph*10^4
  )

# data from DOE
doe_colo <- dt_stops3_colo %>%
  #also pull 2nd minute from DOE data
  mutate(time = time+60) %>%
  #add start time (1 min earlier)
  rbind(dt_stops3_colo,.) %>%
  #collocation times & places
  select(time, location) %>%
  arrange(time) %>% 
  # doe data 
  left_join(doe2) %>%  
  #MC0406 not visited on 2020-03-13 06:41:00	
  drop_na() %>%# View()
  #calculate 2-min median - same as avg for 2 numbers.
  group_by(date, location, location_doe, pollutant) %>%
  summarize(value=median(value)) %>% 
  #wide format
  pivot_wider(names_from = pollutant, values_from = value, names_prefix = "doe_") #%>%
  #don't have mobile monitoring equivalent
  #select(-doe_PM2.5) 
  
#merge our estimates and doe estimates
dt_doe <- left_join(dt_stops3_colo, doe_colo) 
  

```

## Calibrate neph readings using DOE neph and PM2.5 observations

* e.g., PM2.5 will be used for Kaya's disparities paper 

- if our neph readings are similar to the readings from AQS sites (near 1-1 line), we'll use Cooper's calibration curve to estimate PM2.5 mass concentration from bscat. Alternatively, we can create a new calibration based on the PM2.5 readings at the AQS sites we visited during our campaign (?).

* see what how the DOE calibrates nephelometer readings to estimate PM2.5. Note that they do this at the site level.

```{r}
# #doe2 #could use this, but have to filter specific sites that used npm25 for PM2.5 estimates
# #
# doe %>%
#   select(-c(import_id, pollutant)) %>%
#   filter(variable %in% c("doe_neph", "doe_npm25"),
#          freq=="minute"
#          ) %>% 
#   spread(variable, value) %>% 
#   
#   colo.plot(x.variable = "doe_neph", y.variable = "doe_npm25", col.by="location_doe")

```

* fitting a calibration curve to PM2.5~Neph readings    
  * **dropping values < 1st and > 99th quantile**
    * this made the lm fit R2 go from ~ 0.05 to ~0.5    
  * note that neph readings are very minute, while PM2.5 readings are estimated every hour, so these are not totally comparable. But we want neph readings at the minute level, since these will be much noisier (like our mobile data) ?

```{r}
#lm fit
doe_neph_calib <- doe %>%
  # select(-c(import_id, pollutant)) %>%
  # filter(variable %in% c("doe_neph", "doe_npm25"),
  #        freq=="minute"
  #        ) %>% 
  filter(
    # hour/day estimates may be largely aggregates of minute data 
    freq == "minute",
    #locations with nephs and PM2.5 readings
    location_doe %in% c('AQSD', 'AQSK', 'AQSTUK'),
    pollutant %in% c("Neph", "PM2.5"),
    #don't inlcude neph-based pm2.5, which are calculated from the neph-PM2.5 calibration
    !variable %in% "doe_npm25"
    ) %>% #View()
  #if multiple PM2.5 readings at the same time and place, take the mean
  group_by(location_doe, location, time, pollutant) %>%
  summarize(value = mean(value)) %>%
  ungroup() %>%
  spread(pollutant, value) %>%
  #drop extreme values before fitting curve
  drop_na() %>%
  #don't plot extreme values
  filter(Neph > quantile(Neph,0.01) & Neph < quantile(Neph,0.99),
         PM2.5 > quantile(PM2.5,0.01) & PM2.5 < quantile(PM2.5,0.99),
         ) 

```

* there is a lot of noise in the neph readings, but there is a generally positive trend

```{r}
#plot takes long time to run
print("black line is best fit line; blue line is smooth fit")

doe_neph_calib %>%
  ggplot(aes(y=PM2.5, x=Neph)) + 
  #geom_point(alpha=0.1) + 
  geom_bin2d() +
  geom_smooth() +
  geom_smooth(method="lm", col="black") +
  
  labs(title = "Measured nephelometer and PM2.5 readings at AQS sites. \nnot plotting extreme values",
       x= "Minute Neph Scatter (bscat [x10^-4]/m)",
       y = "Hourly PM2.5 Conc (ug/m3)"
       )

```

* this slope is similar to Cooper's when we adjust his calibration curve, which uses bscat x 10^-5, to to bscat x 10^-4.

```{r}

doe_lm <- doe_neph_calib %>%
  #changing the units changes the slope estimate, but not the intercept
  #mutate(Neph = Neph*10) %>% 
  lm(PM2.5~Neph, data=.) 

doe_lm %>% summary()

doe_int <- tidy(doe_lm)$estimate[tidy(doe_lm)$term=="(Intercept)"]
doe_slope <- tidy(doe_lm)$estimate[tidy(doe_lm)$term=="Neph"]
   
```


### --> if we use need to update calibration curve when Dave updates this.

* using doe neph-pm2.5 calibration at three AQS sites since    
  * these are at the minute level
  * there are a lot of pairs?

```{r, echo=T}
#convert bscat ___/m to PM2.5 ug/m3 

#estimates from Cooper's work. this will be updated based on what Dave Slager finds.
int <- 1.060539
# slope was for bscat x 10^-5, whereas here bscat is x 10^-4
slope <- 2.509793 *10

# estimates from DOE data (see above)  
# int <- doe_int
# slope <- doe_slope

```

```{r}
dt_doe <- dt_doe %>%
  #estimate PM2.5 from neph readings
  mutate(PM2.5 = Neph*slope + int)

```


## 2-min collocations (medians)

* There are various discrepancies between DOE and mobile monitoring observations    
  * locations - these are not exaclty the same. Also, the MM vehicle does not always park in the exact same location    
  * instrument differences    
    * all of our PM2.5 estimates come from neph estimates. DOE PM2.5 estimates compared here are both from nephelometer and other measurement methods.     
    * DOE uses site-specific neph-PM2.5 calibration curves. We will use a region calibraiton curve based on multiple AQS sites in the Spatiotemporal modeling region     
      * our MM calibration curve from the ST model excludes Duwamish, even though we use it to predict PM2.5 at Duwamish in this analysis   
    * DOE PM2.5 "minute" readings are actually for longer time periods. For example, rolling 1-hr estimates that are updated every 6 min.
    * the "2-min" median estimates have slightly different start and end times for the MM and DOE estimates. DOE estimates area alwyays exactly 2-min, starting at 00 seconds. MM median stops could have started after 00 seconds into the minute and could have been sligthly longer than 2 min (e.g., 3 min).

```{r}
dt_doe_l <- dt_doe %>%
  gather(pollutant, value, contains(c("BC", "Neph", "NO2", "PM2.5"))) %>% 
  mutate(
    source = ifelse(grepl("doe", pollutant), "DOE", "MM"),
    pollutant = gsub("doe_", "", pollutant)
  ) %>% 
  spread(source, value) %>%
  drop_na(DOE, MM)

```

```{r, fig.height=10}
print("2-min mobile monitoring collocations at AQS sites")


dt_doe_l %>%
  mutate(
    #AQS Units
    pollutant = recode_factor(factor(pollutant),
                               "BC" = "BC (ug/m3)",
                               "Neph" = "Neph (bscat [x 10^-4]/m)",
                               "NO2" = "NO2 (ppb)",
                               "PM2.5" = "PM2.5 (ug/m3)"
                               )
  ) %>%
  
  ggplot(aes(x=DOE, y=MM, )) + 
  facet_wrap_equal(~pollutant, scales="free") + 
  geom_point(aes(col=location_doe)) + 
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  geom_smooth(aes(x=DOE, y=MM), method = "lm",inherit.aes = F ) + 
  #geom_smooth(method = "lm", se=F) + 
  theme(aspect.ratio = 1) + 
  labs(
       y = "Mobile Monitoring",
       col = "AQS Site"
       )
   
```

```{r, fig.height=10, eval=F}
# this has fit estimates on plots, but uses diff legends


p <- list()

#BC
p[[1]] <- dt_doe %>%
  colo.plot(x.variable = "doe_BC", y.variable = "BC", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "BC (ug/m3)",
       y = "Mobile Monitoring",
       x = "DOE"
       )

#Neph
p[[2]] <- dt_doe %>%
  colo.plot(x.variable = "doe_Neph", y.variable = "Neph", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "Nephelometer (bscat [x 10^-4]/m)",
       y = "Mobile Monitoring",
       x = "DOE"
       )

#PM2.5
p[[3]] <- dt_doe %>%
  colo.plot(x.variable = "doe_PM2.5", y.variable = "PM2.5", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "PM2.5 (ug/m3)",
       y = "Mobile Monitoring (neph estimate)",
       x = "DOE"
       )

#NO2
p[[4]] <- dt_doe %>%
  colo.plot(x.variable = "doe_NO2", y.variable = "NO2", col.by = "location_doe", alpha_value = 0.8) +
  geom_smooth(method = "lm", aes(group=location_doe, col=location_doe), se = F) +
  labs(fill= "Overall Fit",
       title = "NO2 (ppb)",
       y = "Mobile Monitoring",
       x = "DOE"
       )
  
ggarrange(plotlist = p) %>%
  annotate_figure(top = "2-min collocations (medians)")

```

```{r}
# table of fit

dt_doe_l %>%  
  group_by(pollutant) %>% 
  summarize(
    N_pairs = n(),
    R2_reg = cor(DOE, MM)^2,
    R2_mse = r2_mse_based(DOE, MM),
    RMSE = rmse(DOE, MM)
  ) %>%
  kable(caption = "Comparison of 2-min mobile monitoring stop medians to respective DOE readings", 
        digits = 2
        ) %>%
  kable_styling()
```



## Annual Avg comparisons

```{r}
#calculate annual averages
true_annual <- doe2 %>%
  group_by(location_doe, location, pollutant) %>%
  summarize(
    true_avg = mean(value)
  )

short_term_annual <- dt_doe %>%
  #make long format
  gather("pollutant", "value", matches(c("bc", "neph", "no2", "PM2.5"))) %>%   
  mutate(
    source = ifelse(grepl("doe", pollutant), "DOE", "Mobile Monitoring"),
    pollutant = gsub("doe_", "", pollutant)
  )  %>%
  #dorp places w/o data
  drop_na() %>% 
  group_by(location, location_doe, pollutant, source) %>%
  summarize(
    value = mean(value)
  )

#note that DOE PM2.5 has no MM equivalent (yet)
true_annual <- left_join(true_annual, short_term_annual) %>%
  drop_na(value) %>%
  ungroup()

```

* annual average neph estimates are not great    
  * though we only had 3 sites with neph readings, and the range was narrow (?)       
* neph-based PM2.5    
  * looks OK except for duwamish for MM campaign    
  * looks better for DOE since they fit site-specific calibration curves   


```{r, fig.height=10}

true_annual %>%
  mutate(
    #AQS Units
    pollutant = recode_factor(factor(pollutant),
                               "BC" = "BC (ug/m3)",
                               "Neph" = "Neph (bscat [x 10^-4]/m)",
                               "NO2" = "NO2 (ppb)",
                               "PM2.5" = "PM2.5 (ug/m3)"
                               )
  ) %>%
  
  ggplot(aes(x=true_avg, y=value, col=source)) + 
  geom_point(aes(shape=location_doe)) + 
  geom_smooth(method = "lm" ,se=F, aes()) + 
  geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
  facet_wrap_equal(~pollutant, scales="free") +
  theme(aspect.ratio = 1) +
    labs(
         col = "2-min Est",
         shape = "Location",         
         x="True Annual Average",
         y = "Estimate"
         )

```

 
```{r}
true_annual %>%
  group_by(pollutant, source) %>%
  summarize(
    N_pairs = n(),
    R2_reg = cor(true_avg, value)^2,
    R2_mse = r2_mse_based(true_avg, value),
    RMSE = rmse(true_avg, value)
  ) %>%
  kable(caption = "site annual average estimates from 2-min readings relative to the true annual average", 
        digits = 2
        ) %>%
  kable_styling()
  
  
```


Do the observed annual avearge neph readings have a narrow range?

* the distribution of minute-level neph data is much wider than the range of neph estimates we see for the annual averages above. This suggests that the "poor" performing neph readings are all close in range to one another and hence why the plot may be making these look bad even though they are not that different. 
  * neph estimates for these sites (Duwamish, Kent, Tukwilla) are also on the high end relative to 

```{r}
doe2 %>% 
  filter(pollutant == "Neph",
         #sites where we're using neph-based PM2.5 (vs e.g., FEM method)
         #location_doe %in% c("AQSD", "AQSK", "AQSTUK")
         ) %>%
  summarize(
    Min = min(value),
    Q05 = quantile(value, 0.05),
    Q25 = quantile(value, 0.25),
    Q50 = median(value),
    Q75 = quantile(value, 0.75),
    #IQR = IQR(value),
    Q95 = quantile(value, 0.95),
    Max= max(value)
  ) %>%
  kable(caption = "Distribution of neph readings (bscat x10^-4/m)") %>%
  kable_styling()
   
```



When did we visit neph DOE sites? Checking that we conducted balanced sampling at these collocation sites.

```{r}
neph_colos <- dt_doe %>%
  #sites with neph data
  filter(location_doe %in% c("AQSD", "AQSK", "AQSTUK")) %>%
  
  select(location_doe, date, time, Neph) %>%
  drop_na() %>%
  mutate(
    hour = factor(hour(time)),
    day = wday(time, label = T, week_start = 1),
  ) %>% 
  add_season(.date_var = "date") 

```

* things look OK?  
  * visits by season-day of week look okay. visits didn't occur on every combination, but weekday and weekend samples occurred    
  * there are more gaps in season-hour combinations, though these are also intermittent for the most part   

```{r}
#hour
neph_colos %>%
  ggplot(aes(x=hour, y=location_doe)) + 
  geom_bin2d() +
  facet_wrap(~season) + 
  #geom_bar(stat = "count", position = "dodge") 
  labs(title = "number of visits to DOE collocation sites with nephs")

#day
neph_colos %>%
  ggplot(aes(x=day, y=location_doe)) + #fill = location_doe
  geom_bin2d() +
  facet_wrap(~season) +
  #geom_bar(stat = "count", position = "dodge")
  labs(title = "number of visits to DOE collocation sites with nephs")

# #season
# neph_colos %>%
#   ggplot(aes(x=season, y=location_doe)) + #fill = location_doe
#     geom_bin2d()


```



# Appendix

## Calibrate to the mean of instrument readings

* we will not do this since there was generally good agreement between instruments and some backup instruments were less trustworthy (e.g., NO2_1, PMSCAN_3, PMDISC_8 was dropped at one point before being replaced).

* note that the screened PTRAKS were not all collocated, though we know that 93 and 94 were similar when they were unscreened, and that 94 and 3 were similar, thus we will assume that at least 3/4 of the instruments were in good agrement. We don't have any collocation information for PTRAK 2, which collected a substantial amount of data.

## Resources

**email from Jill Schulte** regarding instrument QC: 

* "I think you'll find much of the information you're looking for on our Information For Air Monitoring Professionals webpage: https://ecology.wa.gov/Regulations-Permits/Guidance-technical-assistance/Information-for-air-monitoring-professionals. If you open the "Other Procedures" tree, the document "Air Monitoring Documentation, Data Review, and Validation Procedure" has general information about how we validate data, apply completeness criteria, and use 1-minute averages for validation. You'll also find more specific information in each of our standard operating procedures for specific pollutants. You can open the SOP trees for gases and particulate matter instruments on the same page, and most have a "Data Validation and Quality Assurance" section. 
* We also rely heavily on EPA's Quality Assurance Handbook for Air Pollution Measurement Systems Volume II: https://www.epa.gov/sites/production/files/2020-10/documents/final_handbook_document_1_17.pdf. "

*ideas    
  * comapre instrument readings to nearby stops (i.e., stations in the same air shed)     
    * this may be misleading in our case b/c we can smooth out 2-min/instant readings, which may correctly show very different readings?   



# Final Data 


## stop data dropped 

```{r}
original_data <- dt_stops
final_data <- dt_stops4
```

```{r}
 original_data %>%
  summarize(
    N_original = n()
  ) %>%
  cbind(summarize(final_data, N_final = n())) %>%
  mutate(
    N_dropped = N_original - N_final,
    Proportion_dropped = N_dropped/N_original
  ) %>%
  kable(caption = "Stop data dropped. N = total median stops readings = 8 instruments x 309 sites x ~29 stops/site", digits = 2) %>%
  kable_styling()

```


```{r}
 
n_final <- final_data %>%
  group_by(variable) %>%
  summarize(N_final = n())

original_data %>%
  group_by(variable) %>%
  summarize(N_original = n()) %>%
  left_join(n_final) %>%
  mutate(
    N_dropped = N_original-N_final,
    Proportion_dropped = N_dropped/N_original
  ) %>%
  variable_relabel() %>%
  kable(caption = "Data dropped. N = number of stops", digits = 3) %>%
  kable_styling()




```


## Save data 

```{r, eval=T}
# median stop data 
dt_stops4 %>%
  #left_join(locations) %>%
  saveRDS(., file.path("Data", "Output", "stop_data.rda"))

# #second data
## no need to save this if nothing changes really

dt3 %>%
  #left_join(locations) %>%
  saveRDS(., file.path("Data", "Output", "second_data.rda"))

# variables
save(ap,
     #neph calibration curve to be used  
     int, slope,
     file = file.path("Data", "Output", "common_vars.rda"))

```

